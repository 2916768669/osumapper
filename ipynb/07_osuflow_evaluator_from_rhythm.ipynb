{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2018/8/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://ar3.moe/files/sophie.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "GAN is, kind of, hard to train; and I personally felt the pain when I downloaded 10+ github repos where no one worked. While all of those code are based on MNIST, they either do not train any image that looks good, or simply fail to run.\n",
    "\n",
    "As a consequence, I coded this notebook using tf.contrib.eager and tf.keras - took a while to find out how to modify that loss function. The biggest obstacle was that all the tensors are one dimension higher than the data, which was the batch (first dimension).\n",
    "\n",
    "tf.contrib.eager is also said to be a pretty new feature in Tensorflow. ~~Idk how to write the other style of code using sessions without getting error anyways,~~ so make sure Tensorflow has the right version. My env is tensorflow v1.9.0. On Win10, python3.5, no cuda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "3LXMVuV0VhDr"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# this line below can only run once in the notebook! otherwise it will cause errors\n",
    "tf.enable_eager_execution();\n",
    "tfe = tf.contrib.eager;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + 5: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // 4;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 3, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "timestamps_after = timestamps_plus_1 - timestamps;\n",
    "timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "\n",
    "note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some line graph plotting functions found on the web.\n",
    "\n",
    "Unfortunately I forgot where it was found, so let's assume stackoverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib.text as mtext\n",
    "\n",
    "\n",
    "class MyLine(lines.Line2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # we'll update the position when the line data is set\n",
    "        self.text = mtext.Text(0, 0, '')\n",
    "        lines.Line2D.__init__(self, *args, **kwargs)\n",
    "\n",
    "        # we can't access the label attr until *after* the line is\n",
    "        # inited\n",
    "        self.text.set_text(self.get_label())\n",
    "\n",
    "    def set_figure(self, figure):\n",
    "        self.text.set_figure(figure)\n",
    "        lines.Line2D.set_figure(self, figure)\n",
    "\n",
    "    def set_axes(self, axes):\n",
    "        self.text.set_axes(axes)\n",
    "        lines.Line2D.set_axes(self, axes)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        # 2 pixel offset\n",
    "        texttrans = transform + mtransforms.Affine2D().translate(2, 2)\n",
    "        self.text.set_transform(texttrans)\n",
    "        lines.Line2D.set_transform(self, transform)\n",
    "\n",
    "    def set_data(self, x, y):\n",
    "        if len(x):\n",
    "            self.text.set_position((x[-1], y[-1]))\n",
    "\n",
    "        lines.Line2D.set_data(self, x, y)\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        # draw my label at the end of the line with 2 pixel offset\n",
    "        lines.Line2D.draw(self, renderer)\n",
    "        self.text.draw(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import numpy as np;\n",
    "\n",
    "root = \".\\\\mapdata\";\n",
    "\n",
    "chunk_size = 10;\n",
    "step_size = 5;\n",
    "divisor = 4;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define some sort of plotting functions to show the generator and discriminator losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Limitless]')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# actual_train_data = np.concatenate((special_train_data[0:1], special_false_data[0:1]), axis=0);\n",
    "# actual_train_labels = np.concatenate((special_train_labels[0:1], special_false_labels[0:1]), axis=0);\n",
    "\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0.2)) * tf.square(0.3 - vg);\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 0.8)) * tf.square(vg - 0.7);\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0));\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 1));\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "# This function is kind of unused now - should be!\n",
    "# TODO: delete this.\n",
    "def construct_map(var_tensor):\n",
    "    out = [];\n",
    "    cp = tf.constant([0.5, 0.5]);\n",
    "    l = 0.3;\n",
    "    cos_list = l* tf.cos(var_tensor * 6.283);\n",
    "    sin_list = l* tf.sin(var_tensor * 6.283);\n",
    "    for k, _ in enumerate(cos_list):\n",
    "        cp = tf.add(cp, tf.stack([cos_list[k], sin_list[k]]));\n",
    "        out.append(cp);\n",
    "    return tf.stack(out, axis=0);\n",
    "\n",
    "# This function is kind of unused now - I didn't delete it for it could be a backup.\n",
    "# TODO: delete this.\n",
    "def construct_map_without_sliders(var_tensor, extvar={}):\n",
    "    \n",
    "    var_tensor = tf.to_float(var_tensor);\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_tensor.shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    batch_size = var_tensor.shape[0];\n",
    "    note_distances_now = length_multiplier * np.tile(np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "    note_angles_now = np.tile(np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "#     ntensor_list = ((var_tensor - 0.5)) * 2 * 6.283;\n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "#     cos_list2 = tf.cos(var_tensor * 6.283 + phase);\n",
    "#     sin_list2 = tf.sin(var_tensor * 6.283 + phase);\n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.to_float(tf.greater(l, y_max / 2));\n",
    "    not_rerand = tf.to_float(tf.less_equal(l, y_max / 2));\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _px, _py = extvar[\"start_pos\"];\n",
    "    else:\n",
    "        _px = 256;\n",
    "        _py = 192;\n",
    "    _pa = 0;\n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = 256;\n",
    "    _y = 192;\n",
    "    for k in range(half_tensor):\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        wall_value_l = tf.to_float(tf.less(_px, wall_l[:, k]));\n",
    "        wall_value_r = tf.to_float(tf.greater(_px, wall_r[:, k]));\n",
    "        wall_value_xmid = tf.to_float(tf.greater(_px, wall_l[:, k])) * tf.to_float(tf.less(_px, wall_r[:, k]));\n",
    "        wall_value_t = tf.to_float(tf.less(_py, wall_t[:, k]));\n",
    "        wall_value_b = tf.to_float(tf.greater(_py, wall_b[:, k]));\n",
    "        wall_value_ymid = tf.to_float(tf.greater(_py, wall_t[:, k])) * tf.to_float(tf.less(_py, wall_b[:, k]));\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "        _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "        cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b]));\n",
    "        out.append(cp);\n",
    "        _px = _x;\n",
    "        _py = _y;\n",
    "        _pa = tf.mod(_pa + 0, 6.283);\n",
    "    return tf.transpose(tf.stack(out, axis=0), [1, 0, 2]);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "\n",
    "    var_tensor = tf.to_float(var_tensor);\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_tensor.shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    batch_size = var_tensor.shape[0];\n",
    "    note_distances_now = length_multiplier * np.tile(np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "    note_angles_now = np.tile(np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "#     ntensor_list = ((var_tensor - 0.5)) * 2 * 6.283;\n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "#     cos_list2 = tf.cos(var_tensor * 6.283 + phase);\n",
    "#     sin_list2 = tf.sin(var_tensor * 6.283 + phase);\n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.to_float(tf.greater(l, y_max / 2));\n",
    "    not_rerand = tf.to_float(tf.less_equal(l, y_max / 2));\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _px, _py = extvar[\"start_pos\"];\n",
    "    else:\n",
    "        _px = 256;\n",
    "        _py = 192;\n",
    "    _pa = 0;\n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = 256;\n",
    "    _y = 192;\n",
    "    for k in range(half_tensor):\n",
    "        note_index = begin_offset + k;\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l = tf.to_float(tf.less(_px, wall_l[:, k]));\n",
    "        wall_value_r = tf.to_float(tf.greater(_px, wall_r[:, k]));\n",
    "        wall_value_xmid = tf.to_float(tf.greater(_px, wall_l[:, k])) * tf.to_float(tf.less(_px, wall_r[:, k]));\n",
    "        wall_value_t = tf.to_float(tf.less(_py, wall_t[:, k]));\n",
    "        wall_value_b = tf.to_float(tf.greater(_py, wall_b[:, k]));\n",
    "        wall_value_ymid = tf.to_float(tf.greater(_py, wall_t[:, k])) * tf.to_float(tf.less(_py, wall_b[:, k]));\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        if is_slider[note_index]:\n",
    "            sln = slider_lengths[note_index];\n",
    "            slider_type = slider_types[note_index];\n",
    "            scos = slider_cos[slider_type];\n",
    "            ssin = slider_sin[slider_type];\n",
    "            _a = cos_list[:, k + half_tensor];\n",
    "            _b = sin_list[:, k + half_tensor];\n",
    "            # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "            # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "            _oa = _a * scos - _b * ssin;\n",
    "            _ob = _a * ssin + _b * scos;\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x + _a * sln;\n",
    "            _py = _y + _b * sln;\n",
    "        else:\n",
    "            _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "            _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x;\n",
    "            _py = _y;\n",
    "        _pa = tf.mod(_pa + 0, 6.283);\n",
    "    return tf.transpose(tf.stack(out, axis=0), [1, 0, 2]);\n",
    "\n",
    "def stack_loss(tensor):\n",
    "    complex_list = tf.complex(tensor[:, :, 0] * 512, tensor[:, :, 1] * 384);\n",
    "    stack_limit = 30;\n",
    "    precise_limit = 1;\n",
    "    a = [];\n",
    "    for k in range(tensor.shape[1]):\n",
    "        w = tf.tile(tf.expand_dims(complex_list[:, k], axis=1), [1, tensor.shape[1]]);\n",
    "        r = tf.abs(w - complex_list);\n",
    "        rless = tf.to_float(tf.less(r, stack_limit)) * tf.to_float(tf.greater(r, precise_limit));\n",
    "        rmean = tf.reduce_mean(rless * (stack_limit - r) / stack_limit);\n",
    "        a.append(rmean);\n",
    "    b = tf.reduce_sum(a);\n",
    "#         print(tf.tile(w, [1, tensor.shape[1]]));\n",
    "#         print(complex_list);\n",
    "    return b;\n",
    "\n",
    "# This polygon loss was an attempt to make the map less likely to overlap each other.\n",
    "# The idea is: calculate the area of polygon formed from the note positions;\n",
    "# If it is big, then it is good - they form a convex shape, no overlap.\n",
    "# ... of course it totally doesn't work like that.\n",
    "def polygon_loss(tensor):\n",
    "    tensor_this = tensor[:, :, 0:2];\n",
    "    tensor_next = tf.concat([tensor[:, 1:, 0:2], tensor[:, 0:1, 0:2]], axis=1);\n",
    "    sa = (tensor_this[:, :, 0] + tensor_next[:, :, 0]) * (tensor_next[:, :, 1] - tensor_this[:, :, 0]);\n",
    "    surface = tf.abs(tf.reduce_sum(sa, axis=1))/2;\n",
    "    return surface;\n",
    "\n",
    "def construct_map_and_calc_loss(var_tensor, extvar):\n",
    "    # first make a map from the outputs of generator, then ask the classifier (discriminator) to classify it\n",
    "    classifier_model = extvar[\"classifier_model\"]\n",
    "    out = construct_map_with_sliders(var_tensor, extvar=extvar);\n",
    "    cm = classifier_model(out);\n",
    "    predmean = 1 - tf.reduce_mean(cm, axis=1);\n",
    "#    regulator = tf.reduce_mean(tf.reduce_mean(- 0.1 * tf.square(out[:, :, 1:2] - 0.5), axis=2), axis=1); # * tf.square(out[:, :, 1:2] - 2)\n",
    "    box_loss = inblock_loss(out[:, :, 0:2]);\n",
    "    box_loss2 = inblock_loss(out[:, :, 4:6]);\n",
    "#     polygon = polygon_loss(out);\n",
    "    # print(out.shape); shape is (10, X, 4)\n",
    "    #return predmean + box_loss*100;\n",
    "    return predmean + box_loss + box_loss2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "TODO: add more info here\n",
    "\n",
    "TODO: strip variables group_note_count, good_maps_each_epoch, bad_maps_each_epoch, epoch_g, epoch_c, epoch_both_good, epoch_both_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "XdfkR223D9dW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0, Epoch 1: G loss: 0.27958315185138155 vs. C loss: 0.1292490524550279\n",
      "Group 0, Epoch 2: G loss: 0.3746268285172326 vs. C loss: 0.20244843595557746\n",
      "Group 0, Epoch 3: G loss: 0.10400864992822918 vs. C loss: 0.18398718370331657\n",
      "Group 0, Epoch 4: G loss: 0.22418356708117895 vs. C loss: 0.18450145588980782\n",
      "Group 0, Epoch 5: G loss: 0.30156519817454475 vs. C loss: 0.15179098976982963\n",
      "Group 0, Epoch 6: G loss: 0.3019875896828515 vs. C loss: 0.21671335895856222\n",
      "Group 1, Epoch 1: G loss: 0.3243919619492122 vs. C loss: 0.18815342916382685\n",
      "Group 1, Epoch 2: G loss: 0.312267028433936 vs. C loss: 0.15512682580285603\n",
      "Group 1, Epoch 3: G loss: 0.4788761164460863 vs. C loss: 0.14936687548955283\n",
      "Group 1, Epoch 4: G loss: 0.24844115759645188 vs. C loss: 0.23109460208151075\n",
      "Group 1, Epoch 5: G loss: 0.017349985986948015 vs. C loss: 0.20790315833356646\n",
      "Group 1, Epoch 6: G loss: 0.012608666558350834 vs. C loss: 0.2049229939778646\n",
      "Group 2, Epoch 1: G loss: 0.5207503395421165 vs. C loss: 0.23599143988556334\n",
      "Group 2, Epoch 2: G loss: 0.36108110334192 vs. C loss: 0.18464418997367224\n",
      "Group 2, Epoch 3: G loss: 0.17268841330494203 vs. C loss: 0.17022148768107095\n",
      "Group 2, Epoch 4: G loss: 0.270246650491442 vs. C loss: 0.1842367930544747\n",
      "Group 2, Epoch 5: G loss: 0.1615940864597048 vs. C loss: 0.19657266967826417\n",
      "Group 2, Epoch 6: G loss: 0.19608701680387772 vs. C loss: 0.15402725173367393\n",
      "Group 3, Epoch 1: G loss: 0.4194209218025207 vs. C loss: 0.19053353369235992\n",
      "Group 3, Epoch 2: G loss: 0.3091102808713913 vs. C loss: 0.16284005675050947\n",
      "Group 3, Epoch 3: G loss: 0.5261357733181545 vs. C loss: 0.10287762102153565\n",
      "Group 3, Epoch 4: G loss: 0.6164985707827977 vs. C loss: 0.040682556107640266\n",
      "Group 3, Epoch 5: G loss: 0.9062716501099723 vs. C loss: 0.010774855113898715\n",
      "Group 3, Epoch 6: G loss: 0.4138871618679592 vs. C loss: 0.12567858687705463\n",
      "Group 4, Epoch 1: G loss: 0.4362770242350442 vs. C loss: 0.2517873230907652\n",
      "Group 4, Epoch 2: G loss: 0.316576350586755 vs. C loss: 0.17172345519065857\n",
      "Group 4, Epoch 3: G loss: 0.2689616739749908 vs. C loss: 0.15581135120656756\n",
      "Group 4, Epoch 4: G loss: 0.3673462923083986 vs. C loss: 0.20037526554531518\n",
      "Group 4, Epoch 5: G loss: 0.09671897845608846 vs. C loss: 0.16332315074072945\n",
      "Group 4, Epoch 6: G loss: 0.3433093386037009 vs. C loss: 0.0943795558479097\n",
      "Group 5, Epoch 1: G loss: 0.354587186234338 vs. C loss: 0.15587546345260408\n",
      "Group 5, Epoch 2: G loss: 0.3604116703782763 vs. C loss: 0.14768445491790771\n",
      "Group 5, Epoch 3: G loss: 0.4408509850502014 vs. C loss: 0.08464602298206753\n",
      "Group 5, Epoch 4: G loss: 0.4550006525857109 vs. C loss: 0.12109134222070374\n",
      "Group 5, Epoch 5: G loss: 0.5813016210283551 vs. C loss: 0.10233684670594002\n",
      "Group 5, Epoch 6: G loss: 0.331383409670421 vs. C loss: 0.11429000894228618\n",
      "Group 5, Epoch 7: G loss: 0.7011422693729401 vs. C loss: 0.1288817102710406\n",
      "Group 5, Epoch 8: G loss: 0.28318655469587867 vs. C loss: 0.2589267922772302\n",
      "Group 6, Epoch 1: G loss: 0.41476866432598664 vs. C loss: 0.19597557352648842\n",
      "Group 6, Epoch 2: G loss: 0.2640608293669564 vs. C loss: 0.13300294842984942\n",
      "Group 6, Epoch 3: G loss: 0.4576628931931087 vs. C loss: 0.1026503915588061\n",
      "Group 6, Epoch 4: G loss: 0.30994599035808024 vs. C loss: 0.21998678478929734\n",
      "Group 6, Epoch 5: G loss: 0.02337213610964162 vs. C loss: 0.20698164569007027\n",
      "Group 6, Epoch 6: G loss: 0.018721992070121422 vs. C loss: 0.20252601636780632\n",
      "Group 7, Epoch 1: G loss: 0.6673172048160009 vs. C loss: 0.2978539458579487\n",
      "Group 7, Epoch 2: G loss: 0.5087708226272039 vs. C loss: 0.08586527821090485\n",
      "Group 7, Epoch 3: G loss: 0.8012592111315046 vs. C loss: 0.04454271443602111\n",
      "Group 7, Epoch 4: G loss: 0.8144288778305053 vs. C loss: 0.0700062858975596\n",
      "Group 7, Epoch 5: G loss: 0.946520061152322 vs. C loss: 0.03683872727884187\n",
      "Group 7, Epoch 6: G loss: 0.17757558945034232 vs. C loss: 0.2079442242781321\n",
      "Group 8, Epoch 1: G loss: 0.3335098568882261 vs. C loss: 0.15393821563985613\n",
      "Group 8, Epoch 2: G loss: 0.32283902551446647 vs. C loss: 0.14528805100255543\n",
      "Group 8, Epoch 3: G loss: 0.3273103041308267 vs. C loss: 0.14434464110268488\n",
      "Group 8, Epoch 4: G loss: 0.662790150301797 vs. C loss: 0.1261427683962716\n",
      "Group 8, Epoch 5: G loss: 0.680735446725573 vs. C loss: 0.06009781567586794\n",
      "Group 8, Epoch 6: G loss: 0.29994347723467 vs. C loss: 0.21460176176495024\n",
      "Group 9, Epoch 1: G loss: 0.21302407171045032 vs. C loss: 0.19269506384929022\n",
      "Group 9, Epoch 2: G loss: 0.38618376765932355 vs. C loss: 0.11081438553002147\n",
      "Group 9, Epoch 3: G loss: 0.62427231328828 vs. C loss: 0.07129338300890393\n",
      "Group 9, Epoch 4: G loss: 0.48810849743229995 vs. C loss: 0.148548757036527\n",
      "Group 9, Epoch 5: G loss: 0.30882390258567677 vs. C loss: 0.2367675420310762\n",
      "Group 9, Epoch 6: G loss: 0.03687882604343551 vs. C loss: 0.20795648958947924\n",
      "Group 10, Epoch 1: G loss: 0.5000294165951865 vs. C loss: 0.2603895018498103\n",
      "Group 10, Epoch 2: G loss: 0.31074300536087585 vs. C loss: 0.180546465019385\n",
      "Group 10, Epoch 3: G loss: 0.15129368603229523 vs. C loss: 0.1809622844060262\n",
      "Group 10, Epoch 4: G loss: 0.20310552035059248 vs. C loss: 0.17451179358694288\n",
      "Group 10, Epoch 5: G loss: 0.1440541073679924 vs. C loss: 0.20504812730683222\n",
      "Group 10, Epoch 6: G loss: 0.061405274697712495 vs. C loss: 0.19890084200435212\n",
      "Group 11, Epoch 1: G loss: 0.30033638562474935 vs. C loss: 0.17998766567971972\n",
      "Group 11, Epoch 2: G loss: 0.3198233553341457 vs. C loss: 0.12783681435717478\n",
      "Group 11, Epoch 3: G loss: 0.5656710948262895 vs. C loss: 0.1090754506488641\n",
      "Group 11, Epoch 4: G loss: 0.736502959047045 vs. C loss: 0.04798196670081881\n",
      "Group 11, Epoch 5: G loss: 0.38131666215402743 vs. C loss: 0.20749954879283905\n",
      "Group 11, Epoch 6: G loss: 0.03215128742158413 vs. C loss: 0.2076661934455236\n",
      "Group 12, Epoch 1: G loss: 0.4931825637817383 vs. C loss: 0.22726424617899788\n",
      "Group 12, Epoch 2: G loss: 0.3310806666101728 vs. C loss: 0.1810529761844211\n",
      "Group 12, Epoch 3: G loss: 0.14065267103058948 vs. C loss: 0.16923164079586664\n",
      "Group 12, Epoch 4: G loss: 0.26619058081081937 vs. C loss: 0.1845054899652799\n",
      "Group 12, Epoch 5: G loss: 0.25518386917454855 vs. C loss: 0.13623182889488009\n",
      "Group 12, Epoch 6: G loss: 0.4000711560249329 vs. C loss: 0.12091269012954499\n",
      "Group 12, Epoch 7: G loss: 0.3358002062354769 vs. C loss: 0.21063096821308136\n",
      "Group 13, Epoch 1: G loss: 0.43817956703049793 vs. C loss: 0.20844070282247332\n",
      "Group 13, Epoch 2: G loss: 0.3713509112596511 vs. C loss: 0.1930454158120685\n",
      "Group 13, Epoch 3: G loss: 0.0967362669961793 vs. C loss: 0.1795288109117084\n",
      "Group 13, Epoch 4: G loss: 0.19872684968369342 vs. C loss: 0.16679554929335913\n",
      "Group 13, Epoch 5: G loss: 0.3509298873799187 vs. C loss: 0.14141951666937932\n",
      "Group 13, Epoch 6: G loss: 0.3447204027857099 vs. C loss: 0.10160332255893284\n",
      "Group 14, Epoch 1: G loss: 0.44811714036124084 vs. C loss: 0.2030082510577308\n",
      "Group 14, Epoch 2: G loss: 0.20439324804714748 vs. C loss: 0.16260224332412085\n",
      "Group 14, Epoch 3: G loss: 0.4177151492663792 vs. C loss: 0.09159038960933685\n",
      "Group 14, Epoch 4: G loss: 0.2444008072572095 vs. C loss: 0.20546111630068886\n",
      "Group 14, Epoch 5: G loss: 0.028269501881940027 vs. C loss: 0.19926588237285614\n",
      "Group 14, Epoch 6: G loss: 0.05076957217284612 vs. C loss: 0.18064181506633759\n",
      "Group 15, Epoch 1: G loss: 0.33031924877847946 vs. C loss: 0.22629009187221524\n",
      "Group 15, Epoch 2: G loss: 0.26757393777370453 vs. C loss: 0.1830109589629703\n",
      "Group 15, Epoch 3: G loss: 0.1540300292628152 vs. C loss: 0.15025238692760468\n",
      "Group 15, Epoch 4: G loss: 0.41427981768335614 vs. C loss: 0.09328399391637908\n",
      "Group 15, Epoch 5: G loss: 0.5480062161173139 vs. C loss: 0.11514368901650111\n",
      "Group 15, Epoch 6: G loss: 0.20178438478282515 vs. C loss: 0.2257611569431093\n",
      "Group 16, Epoch 1: G loss: 0.16664358760629386 vs. C loss: 0.1678787784443961\n",
      "Group 16, Epoch 2: G loss: 0.30593859468187606 vs. C loss: 0.17288042936060163\n",
      "Group 16, Epoch 3: G loss: 0.26675730305058615 vs. C loss: 0.12650488110052213\n",
      "Group 16, Epoch 4: G loss: 0.5637721470424106 vs. C loss: 0.11389452053440942\n",
      "Group 16, Epoch 5: G loss: 0.2522413339998041 vs. C loss: 0.21061769790119597\n",
      "Group 16, Epoch 6: G loss: 0.04025251460926873 vs. C loss: 0.20672192176183066\n",
      "Group 17, Epoch 1: G loss: 0.11048263971294676 vs. C loss: 0.18467371662457785\n",
      "Group 17, Epoch 2: G loss: 0.17357853800058365 vs. C loss: 0.12693489756849077\n",
      "Group 17, Epoch 3: G loss: 0.2601289003023079 vs. C loss: 0.18855167925357819\n",
      "Group 17, Epoch 4: G loss: 0.2754487224987575 vs. C loss: 0.11081102490425111\n",
      "Group 17, Epoch 5: G loss: 0.5735760586602346 vs. C loss: 0.10983465280797745\n",
      "Group 17, Epoch 6: G loss: 0.18915291428565978 vs. C loss: 0.2093942960103353\n",
      "Group 18, Epoch 1: G loss: 0.15344242325850896 vs. C loss: 0.15104036447074679\n",
      "Group 18, Epoch 2: G loss: 0.2939024925231934 vs. C loss: 0.1310005465315448\n",
      "Group 18, Epoch 3: G loss: 0.695528883593423 vs. C loss: 0.0565645427753528\n",
      "Group 18, Epoch 4: G loss: 0.7816067320959909 vs. C loss: 0.024682092842542466\n",
      "Group 18, Epoch 5: G loss: 0.7443191162177494 vs. C loss: 0.02143538086157706\n",
      "Group 18, Epoch 6: G loss: 0.9852774466787065 vs. C loss: 0.018258989225917805\n",
      "Group 19, Epoch 1: G loss: 0.3110778953347887 vs. C loss: 0.1597777158021927\n",
      "Group 19, Epoch 2: G loss: 0.3156953322035925 vs. C loss: 0.18971136212348938\n",
      "Group 19, Epoch 3: G loss: 0.14846327411276955 vs. C loss: 0.15453616032997766\n",
      "Group 19, Epoch 4: G loss: 0.3744186648300715 vs. C loss: 0.08950268063280319\n",
      "Group 19, Epoch 5: G loss: 0.6164391023772104 vs. C loss: 0.05862595223718219\n",
      "Group 19, Epoch 6: G loss: 0.22910715767315454 vs. C loss: 0.16746204884515867\n",
      "Group 20, Epoch 1: G loss: 0.3042927214077541 vs. C loss: 0.2113114876879586\n",
      "Group 20, Epoch 2: G loss: 0.2746610977819988 vs. C loss: 0.11664656177163124\n",
      "Group 20, Epoch 3: G loss: 0.5167055530207498 vs. C loss: 0.10086958731214206\n",
      "Group 20, Epoch 4: G loss: 0.5259983841861998 vs. C loss: 0.11945778876543046\n",
      "Group 20, Epoch 5: G loss: 0.586995843904359 vs. C loss: 0.21141097942988077\n",
      "Group 20, Epoch 6: G loss: 0.09764907072697368 vs. C loss: 0.20619224674171868\n",
      "Group 21, Epoch 1: G loss: 0.20863235337393626 vs. C loss: 0.1754513813389672\n",
      "Group 21, Epoch 2: G loss: 0.378919836453029 vs. C loss: 0.1364090997311804\n",
      "Group 21, Epoch 3: G loss: 0.5493954896926879 vs. C loss: 0.0885253676937686\n",
      "Group 21, Epoch 4: G loss: 0.5954944934163775 vs. C loss: 0.05695623738898172\n",
      "Group 21, Epoch 5: G loss: 0.3653365731771503 vs. C loss: 0.2201138138771057\n",
      "Group 21, Epoch 6: G loss: 0.015314460465950626 vs. C loss: 0.20843204193645057\n",
      "Group 22, Epoch 1: G loss: 0.3406831920146942 vs. C loss: 0.18051190177599588\n",
      "Group 22, Epoch 2: G loss: 0.36438839350427904 vs. C loss: 0.15698941962586507\n",
      "Group 22, Epoch 3: G loss: 0.35664092217172894 vs. C loss: 0.10339273181226517\n",
      "Group 22, Epoch 4: G loss: 0.7120615805898394 vs. C loss: 0.05358578388889631\n",
      "Group 22, Epoch 5: G loss: 0.8319584812436785 vs. C loss: 0.02607801804939906\n",
      "Group 22, Epoch 6: G loss: 0.8123804024287632 vs. C loss: 0.05426473387827476\n",
      "Group 23, Epoch 1: G loss: 0.5518175380570548 vs. C loss: 0.24983531402217016\n",
      "Group 23, Epoch 2: G loss: 0.39379849689347407 vs. C loss: 0.13424708487259016\n",
      "Group 23, Epoch 3: G loss: 0.3326639422348568 vs. C loss: 0.18203475740220812\n",
      "Group 23, Epoch 4: G loss: 0.3102923503943852 vs. C loss: 0.14287770787874857\n",
      "Group 23, Epoch 5: G loss: 0.41492483403001507 vs. C loss: 0.14775603926844066\n",
      "Group 23, Epoch 6: G loss: 0.34161155484616756 vs. C loss: 0.2329273538457023\n",
      "Group 24, Epoch 1: G loss: 0.4451262652873993 vs. C loss: 0.2093678977754381\n",
      "Group 24, Epoch 2: G loss: 0.3460068353584834 vs. C loss: 0.10367145554886924\n",
      "Group 24, Epoch 3: G loss: 0.4092322255883899 vs. C loss: 0.12267742844091521\n",
      "Group 24, Epoch 4: G loss: 0.729663380554744 vs. C loss: 0.03839182181076871\n",
      "Group 24, Epoch 5: G loss: 0.4247617218111242 vs. C loss: 0.2001425971587499\n",
      "Group 24, Epoch 6: G loss: 0.05836351716092654 vs. C loss: 0.1938743276728524\n",
      "Group 25, Epoch 1: G loss: 0.3928533732891083 vs. C loss: 0.16624446875519225\n",
      "Group 25, Epoch 2: G loss: 0.43261582595961434 vs. C loss: 0.08764915416638057\n",
      "Group 25, Epoch 3: G loss: 0.6749839697565351 vs. C loss: 0.036361567883027926\n",
      "Group 25, Epoch 4: G loss: 0.5237598304237638 vs. C loss: 0.20379728741115996\n",
      "Group 25, Epoch 5: G loss: 0.07207605019211769 vs. C loss: 0.2095071772734324\n",
      "Group 25, Epoch 6: G loss: 0.011568491919232265 vs. C loss: 0.20905711087915632\n",
      "Group 26, Epoch 1: G loss: 0.24628976796354562 vs. C loss: 0.18827777107556662\n",
      "Group 26, Epoch 2: G loss: 0.3710148189749036 vs. C loss: 0.12596581379572552\n",
      "Group 26, Epoch 3: G loss: 0.2411779373884201 vs. C loss: 0.18881940013832518\n",
      "Group 26, Epoch 4: G loss: 0.1475055537053517 vs. C loss: 0.1629233525858985\n",
      "Group 26, Epoch 5: G loss: 0.43052190712520055 vs. C loss: 0.09719650447368622\n",
      "Group 26, Epoch 6: G loss: 0.42470589961324423 vs. C loss: 0.10370404231879447\n",
      "Group 27, Epoch 1: G loss: 0.2406491207224982 vs. C loss: 0.16388218104839325\n",
      "Group 27, Epoch 2: G loss: 0.30419251237596784 vs. C loss: 0.12183689574400584\n",
      "Group 27, Epoch 3: G loss: 0.5613773431096758 vs. C loss: 0.057091515718234905\n",
      "Group 27, Epoch 4: G loss: 0.6681505961077555 vs. C loss: 0.05253862424029244\n",
      "Group 27, Epoch 5: G loss: 0.7924392512866428 vs. C loss: 0.03316966547734208\n",
      "Group 27, Epoch 6: G loss: 0.3847531931740897 vs. C loss: 0.2081939031680425\n",
      "Group 27, Epoch 7: G loss: 0.04186430371233395 vs. C loss: 0.20751155416170755\n",
      "Group 28, Epoch 1: G loss: 0.3980751744338444 vs. C loss: 0.17258787403504053\n",
      "Group 28, Epoch 2: G loss: 0.3309664462293897 vs. C loss: 0.10721832803554004\n",
      "Group 28, Epoch 3: G loss: 0.5927997035639627 vs. C loss: 0.0749028647939364\n",
      "Group 28, Epoch 4: G loss: 0.802045100075858 vs. C loss: 0.03500394843932655\n",
      "Group 28, Epoch 5: G loss: 0.5255146582211767 vs. C loss: 0.2008912306692865\n",
      "Group 28, Epoch 6: G loss: 0.04812212150011743 vs. C loss: 0.20988157060411242\n",
      "Group 29, Epoch 1: G loss: 0.36642585907663616 vs. C loss: 0.25203311691681546\n",
      "Group 29, Epoch 2: G loss: 0.32969193884304593 vs. C loss: 0.09530025472243626\n",
      "Group 29, Epoch 3: G loss: 0.5797976204327175 vs. C loss: 0.10218348022964265\n",
      "Group 29, Epoch 4: G loss: 0.4079611186470304 vs. C loss: 0.14038116981585821\n",
      "Group 29, Epoch 5: G loss: 0.4936407076460974 vs. C loss: 0.15255102846357557\n",
      "Group 29, Epoch 6: G loss: 0.2623866166387286 vs. C loss: 0.2195140884982215\n",
      "Group 30, Epoch 1: G loss: 0.32085933046681536 vs. C loss: 0.19351433714230856\n",
      "Group 30, Epoch 2: G loss: 0.3099278475557055 vs. C loss: 0.13745477216111288\n",
      "Group 30, Epoch 3: G loss: 0.5752726827348981 vs. C loss: 0.09238422082530129\n",
      "Group 30, Epoch 4: G loss: 0.429718434384891 vs. C loss: 0.18017041352060106\n",
      "Group 30, Epoch 5: G loss: 0.1643485690866198 vs. C loss: 0.15653512626886368\n",
      "Group 30, Epoch 6: G loss: 0.39025849699974063 vs. C loss: 0.12987140234973696\n",
      "Group 30, Epoch 7: G loss: 0.4936460469450269 vs. C loss: 0.08151620419489013\n",
      "Group 31, Epoch 1: G loss: 0.5133827686309814 vs. C loss: 0.21661706682708526\n",
      "Group 31, Epoch 2: G loss: 0.30277365744113915 vs. C loss: 0.14837280495299235\n",
      "Group 31, Epoch 3: G loss: 0.3528440262590135 vs. C loss: 0.09830509954028659\n",
      "Group 31, Epoch 4: G loss: 0.5501112214156559 vs. C loss: 0.17305804871850541\n",
      "Group 31, Epoch 5: G loss: 0.19048711870397841 vs. C loss: 0.1636939984228876\n",
      "Group 31, Epoch 6: G loss: 0.354629055091313 vs. C loss: 0.09716524183750153\n",
      "Group 31, Epoch 7: G loss: 0.40306222438812256 vs. C loss: 0.13792784346474543\n",
      "Group 32, Epoch 1: G loss: 0.4574862446103776 vs. C loss: 0.2309674024581909\n",
      "Group 32, Epoch 2: G loss: 0.38024608152253286 vs. C loss: 0.1443231105804443\n",
      "Group 32, Epoch 3: G loss: 0.2887903273105622 vs. C loss: 0.20919033553865218\n",
      "Group 32, Epoch 4: G loss: 0.10907216114657266 vs. C loss: 0.19154575301541224\n",
      "Group 32, Epoch 5: G loss: 0.11700797634465353 vs. C loss: 0.1714027341869142\n",
      "Group 32, Epoch 6: G loss: 0.24808577810015 vs. C loss: 0.21423070298300848\n",
      "Group 33, Epoch 1: G loss: 0.28058452350752694 vs. C loss: 0.18306329184108314\n",
      "Group 33, Epoch 2: G loss: 0.2419598409107753 vs. C loss: 0.13984673470258713\n",
      "Group 33, Epoch 3: G loss: 0.4998766439301627 vs. C loss: 0.13777889725234774\n",
      "Group 33, Epoch 4: G loss: 0.5124203085899353 vs. C loss: 0.06291257838408153\n",
      "Group 33, Epoch 5: G loss: 0.7904078926358904 vs. C loss: 0.03313528104788727\n",
      "Group 33, Epoch 6: G loss: 0.6737054854631425 vs. C loss: 0.0656165113258693\n",
      "Group 34, Epoch 1: G loss: 0.26889252534934455 vs. C loss: 0.16146531866656408\n",
      "Group 34, Epoch 2: G loss: 0.31910781264305116 vs. C loss: 0.11844261445932917\n",
      "Group 34, Epoch 3: G loss: 0.6088352560997009 vs. C loss: 0.04156444832268689\n",
      "Group 34, Epoch 4: G loss: 0.4608017044407981 vs. C loss: 0.07509573921561241\n",
      "Group 34, Epoch 5: G loss: 0.8712211489677429 vs. C loss: 0.019214472526477445\n",
      "Group 34, Epoch 6: G loss: 0.6161408100809369 vs. C loss: 0.1821791016393238\n",
      "Group 34, Epoch 7: G loss: 0.17032846978732516 vs. C loss: 0.2116762896378835\n",
      "Group 35, Epoch 1: G loss: 0.4462072678974697 vs. C loss: 0.18382819659180114\n",
      "Group 35, Epoch 2: G loss: 0.3050388838563647 vs. C loss: 0.11489173107677036\n",
      "Group 35, Epoch 3: G loss: 0.3863033486264093 vs. C loss: 0.10847812684045897\n",
      "Group 35, Epoch 4: G loss: 0.29462832169873376 vs. C loss: 0.21159266763263282\n",
      "Group 35, Epoch 5: G loss: 0.02066800392099789 vs. C loss: 0.20677648981412253\n",
      "Group 35, Epoch 6: G loss: 0.01380679729793753 vs. C loss: 0.20711208548810747\n",
      "Group 36, Epoch 1: G loss: 0.32622427088873723 vs. C loss: 0.13697638776567247\n",
      "Group 36, Epoch 2: G loss: 0.29792895019054416 vs. C loss: 0.13034426007005903\n",
      "Group 36, Epoch 3: G loss: 0.5223891360419136 vs. C loss: 0.16233671373791164\n",
      "Group 36, Epoch 4: G loss: 0.32378734302307877 vs. C loss: 0.22987437744935355\n",
      "Group 36, Epoch 5: G loss: 0.005987433197775058 vs. C loss: 0.20759182452052805\n",
      "Group 36, Epoch 6: G loss: 0.0022690942444439444 vs. C loss: 0.20767396522892845\n",
      "Group 37, Epoch 1: G loss: 0.28365133362157 vs. C loss: 0.18783200283845267\n",
      "Group 37, Epoch 2: G loss: 0.23398809369121282 vs. C loss: 0.18053158455424842\n",
      "Group 37, Epoch 3: G loss: 0.23129753640719816 vs. C loss: 0.1659082074960073\n",
      "Group 37, Epoch 4: G loss: 0.3028946489095688 vs. C loss: 0.1384904732306798\n",
      "Group 37, Epoch 5: G loss: 0.41412801401955746 vs. C loss: 0.16790877199835244\n",
      "Group 37, Epoch 6: G loss: 0.24877698229891917 vs. C loss: 0.23219647092951667\n",
      "Group 38, Epoch 1: G loss: 0.5353925721985953 vs. C loss: 0.26986362122827107\n",
      "Group 38, Epoch 2: G loss: 0.41764775429453166 vs. C loss: 0.13016718957159254\n",
      "Group 38, Epoch 3: G loss: 0.34979831320898874 vs. C loss: 0.14730380723873773\n",
      "Group 38, Epoch 4: G loss: 0.5893903987748282 vs. C loss: 0.11370879080560471\n",
      "Group 38, Epoch 5: G loss: 0.5149172859532494 vs. C loss: 0.10039802889029185\n",
      "Group 38, Epoch 6: G loss: 0.6187399123396192 vs. C loss: 0.09200065914127563\n",
      "Group 38, Epoch 7: G loss: 0.4737340865390641 vs. C loss: 0.2141433192623986\n",
      "Group 38, Epoch 8: G loss: 0.05525782842721259 vs. C loss: 0.20418318609396616\n",
      "Group 38, Epoch 9: G loss: 0.04826059554304396 vs. C loss: 0.20150557657082876\n",
      "Group 38, Epoch 10: G loss: 0.05750162995287351 vs. C loss: 0.18470441963937548\n",
      "Group 38, Epoch 11: G loss: 0.24526928961277009 vs. C loss: 0.1433407606350051\n",
      "Group 38, Epoch 12: G loss: 0.3545137924807412 vs. C loss: 0.15739090657896468\n",
      "Group 38, Epoch 13: G loss: 0.32859568510736736 vs. C loss: 0.1538433349794812\n",
      "Group 39, Epoch 1: G loss: 0.5280898417745318 vs. C loss: 0.2001293872793516\n",
      "Group 39, Epoch 2: G loss: 0.3704295252050672 vs. C loss: 0.17481059994962478\n",
      "Group 39, Epoch 3: G loss: 0.23733044479574475 vs. C loss: 0.14431045121616787\n",
      "Group 39, Epoch 4: G loss: 0.4608948307377952 vs. C loss: 0.10754065298371845\n",
      "Group 39, Epoch 5: G loss: 0.43910338580608366 vs. C loss: 0.22524377703666687\n",
      "Group 39, Epoch 6: G loss: 0.02721855416893959 vs. C loss: 0.2070893810855018\n",
      "Group 40, Epoch 1: G loss: 0.40803741557257517 vs. C loss: 0.20445754627386728\n",
      "Group 40, Epoch 2: G loss: 0.36441335763250077 vs. C loss: 0.108887513478597\n",
      "Group 40, Epoch 3: G loss: 0.4316813000610896 vs. C loss: 0.1375669530696339\n",
      "Group 40, Epoch 4: G loss: 0.5813332634312766 vs. C loss: 0.0779204134725862\n",
      "Group 40, Epoch 5: G loss: 0.349481937289238 vs. C loss: 0.1379224095079634\n",
      "Group 40, Epoch 6: G loss: 0.547984191775322 vs. C loss: 0.15399004518985748\n",
      "Group 41, Epoch 1: G loss: 0.32023572581154963 vs. C loss: 0.18585061033566794\n",
      "Group 41, Epoch 2: G loss: 0.2629729445491518 vs. C loss: 0.17730572157435945\n",
      "Group 41, Epoch 3: G loss: 0.15791550746985844 vs. C loss: 0.19801807403564453\n",
      "Group 41, Epoch 4: G loss: 0.0941835282104356 vs. C loss: 0.19252395961019728\n",
      "Group 41, Epoch 5: G loss: 0.11495209549154554 vs. C loss: 0.1444673885901769\n",
      "Group 41, Epoch 6: G loss: 0.3148292945963996 vs. C loss: 0.1877715927031305\n",
      "Group 42, Epoch 1: G loss: 0.4044622250965663 vs. C loss: 0.20189990765518614\n",
      "Group 42, Epoch 2: G loss: 0.16081269638878956 vs. C loss: 0.15684009591738382\n",
      "Group 42, Epoch 3: G loss: 0.26399974312101093 vs. C loss: 0.1483051511976454\n",
      "Group 42, Epoch 4: G loss: 0.551739741223199 vs. C loss: 0.10434669090641868\n",
      "Group 42, Epoch 5: G loss: 0.29345234526055197 vs. C loss: 0.12127870611018604\n",
      "Group 42, Epoch 6: G loss: 0.8066853608403887 vs. C loss: 0.0481376846631368\n",
      "Group 43, Epoch 1: G loss: 0.23529432160513744 vs. C loss: 0.16305921806229484\n",
      "Group 43, Epoch 2: G loss: 0.3756509167807443 vs. C loss: 0.11000675459702809\n",
      "Group 43, Epoch 3: G loss: 0.6724074312618802 vs. C loss: 0.05906257633533743\n",
      "Group 43, Epoch 4: G loss: 0.44030177422932215 vs. C loss: 0.14813919530974493\n",
      "Group 43, Epoch 5: G loss: 0.7534350974219187 vs. C loss: 0.036426830364184246\n",
      "Group 43, Epoch 6: G loss: 0.9617405329431807 vs. C loss: 0.002904327579825703\n",
      "Group 44, Epoch 1: G loss: 0.2069940230676106 vs. C loss: 0.1899284174044927\n",
      "Group 44, Epoch 2: G loss: 0.211397722363472 vs. C loss: 0.17553867234124076\n",
      "Group 44, Epoch 3: G loss: 0.3777018751416888 vs. C loss: 0.15822593371073404\n",
      "Group 44, Epoch 4: G loss: 0.28789054964269906 vs. C loss: 0.1393619163168801\n",
      "Group 44, Epoch 5: G loss: 0.4210546059267862 vs. C loss: 0.12901829100317427\n",
      "Group 44, Epoch 6: G loss: 0.3507236008133207 vs. C loss: 0.18937816884782577\n",
      "Group 45, Epoch 1: G loss: 0.2403962471655437 vs. C loss: 0.15951285014549893\n",
      "Group 45, Epoch 2: G loss: 0.4552800519125802 vs. C loss: 0.11105864577823216\n",
      "Group 45, Epoch 3: G loss: 0.512506224002157 vs. C loss: 0.1242227748864227\n",
      "Group 45, Epoch 4: G loss: 0.6131725694452014 vs. C loss: 0.1599237190352546\n",
      "Group 45, Epoch 5: G loss: 0.39305401444435123 vs. C loss: 0.24299604859617022\n",
      "Group 45, Epoch 6: G loss: 0.04178803536508764 vs. C loss: 0.20550919075806937\n",
      "Group 46, Epoch 1: G loss: 0.44362730298723496 vs. C loss: 0.32169271343284184\n",
      "Group 46, Epoch 2: G loss: 0.43605658582278656 vs. C loss: 0.13422050409846836\n",
      "Group 46, Epoch 3: G loss: 0.4279611472572599 vs. C loss: 0.1667704482873281\n",
      "Group 46, Epoch 4: G loss: 0.3628677632127489 vs. C loss: 0.21012001732985178\n",
      "Group 46, Epoch 5: G loss: 0.11231828459671565 vs. C loss: 0.19077084296279487\n",
      "Group 46, Epoch 6: G loss: 0.1363843728389059 vs. C loss: 0.12091886003812154\n",
      "Group 46, Epoch 7: G loss: 0.7358757989747184 vs. C loss: 0.08150817164116435\n",
      "Group 46, Epoch 8: G loss: 0.5551329712782588 vs. C loss: 0.1880290624168184\n",
      "Group 46, Epoch 9: G loss: 0.19443767070770263 vs. C loss: 0.1452978973587354\n",
      "Group 46, Epoch 10: G loss: 0.7305433537278857 vs. C loss: 0.14028067307339773\n",
      "Group 47, Epoch 1: G loss: 0.4635729798248836 vs. C loss: 0.21738414466381073\n",
      "Group 47, Epoch 2: G loss: 0.3423956428255353 vs. C loss: 0.1706565436389711\n",
      "Group 47, Epoch 3: G loss: 0.4632659758840289 vs. C loss: 0.12635333091020587\n",
      "Group 47, Epoch 4: G loss: 0.336872099339962 vs. C loss: 0.16297494371732077\n",
      "Group 47, Epoch 5: G loss: 0.5070897281169892 vs. C loss: 0.11787045747041702\n",
      "Group 47, Epoch 6: G loss: 0.44973190511975963 vs. C loss: 0.14993856474757195\n",
      "Group 48, Epoch 1: G loss: 0.2079036431653159 vs. C loss: 0.18214269230763117\n",
      "Group 48, Epoch 2: G loss: 0.328060564824513 vs. C loss: 0.09490740671753883\n",
      "Group 48, Epoch 3: G loss: 0.6606953075953893 vs. C loss: 0.04282397642317745\n",
      "Group 48, Epoch 4: G loss: 0.4475687286683491 vs. C loss: 0.1426769784755177\n",
      "Group 48, Epoch 5: G loss: 0.5429182371922902 vs. C loss: 0.1945894236365954\n",
      "Group 48, Epoch 6: G loss: 0.5007299887282508 vs. C loss: 0.18225973803136086\n",
      "Group 49, Epoch 1: G loss: 0.30009065781320843 vs. C loss: 0.20192954109774694\n",
      "Group 49, Epoch 2: G loss: 0.22681733454976766 vs. C loss: 0.19553331202930876\n",
      "Group 49, Epoch 3: G loss: 0.16087700575590133 vs. C loss: 0.1579462712009748\n",
      "Group 49, Epoch 4: G loss: 0.3877422639301845 vs. C loss: 0.09978227068980534\n",
      "Group 49, Epoch 5: G loss: 0.3418368450232915 vs. C loss: 0.20177225768566132\n",
      "Group 49, Epoch 6: G loss: 0.031178344360419687 vs. C loss: 0.20772883296012878\n",
      "Group 50, Epoch 1: G loss: 0.5112973902906691 vs. C loss: 0.20483433620797262\n",
      "Group 50, Epoch 2: G loss: 0.36690846255847387 vs. C loss: 0.17162511601216265\n",
      "Group 50, Epoch 3: G loss: 0.11510175785848073 vs. C loss: 0.18914263115988839\n",
      "Group 50, Epoch 4: G loss: 0.14900382310152052 vs. C loss: 0.19307710064782035\n",
      "Group 50, Epoch 5: G loss: 0.18347565191132678 vs. C loss: 0.18339027712742487\n",
      "Group 50, Epoch 6: G loss: 0.25173115708998267 vs. C loss: 0.16702358093526629\n",
      "Group 51, Epoch 1: G loss: 0.4353658046041216 vs. C loss: 0.18978910893201828\n",
      "Group 51, Epoch 2: G loss: 0.2734999426773616 vs. C loss: 0.17168292320436904\n",
      "Group 51, Epoch 3: G loss: 0.17359951798404966 vs. C loss: 0.20123106241226196\n",
      "Group 51, Epoch 4: G loss: 0.11218013146093915 vs. C loss: 0.18594415568643144\n",
      "Group 51, Epoch 5: G loss: 0.1901077900614057 vs. C loss: 0.16921088182263902\n",
      "Group 51, Epoch 6: G loss: 0.24639649646622794 vs. C loss: 0.1342632778816753\n",
      "Group 52, Epoch 1: G loss: 0.36407423274857653 vs. C loss: 0.20123699886931312\n",
      "Group 52, Epoch 2: G loss: 0.2561838167054313 vs. C loss: 0.17589830855528513\n",
      "Group 52, Epoch 3: G loss: 0.1712775481598718 vs. C loss: 0.17290284732977548\n",
      "Group 52, Epoch 4: G loss: 0.31193850721631733 vs. C loss: 0.10602555010053848\n",
      "Group 52, Epoch 5: G loss: 0.4908044116837638 vs. C loss: 0.10438707139756943\n",
      "Group 52, Epoch 6: G loss: 0.46175389119556975 vs. C loss: 0.12490787439876132\n",
      "Group 53, Epoch 1: G loss: 0.5577950869287763 vs. C loss: 0.2798907127645281\n",
      "Group 53, Epoch 2: G loss: 0.45109939660344806 vs. C loss: 0.1243405607011583\n",
      "Group 53, Epoch 3: G loss: 0.38373835682868956 vs. C loss: 0.08870106438795726\n",
      "Group 53, Epoch 4: G loss: 0.7749494467462812 vs. C loss: 0.03320825813959042\n",
      "Group 53, Epoch 5: G loss: 0.8271599820681982 vs. C loss: 0.007081292802467942\n",
      "Group 53, Epoch 6: G loss: 0.8761614084243775 vs. C loss: 0.025165858833740156\n",
      "Group 54, Epoch 1: G loss: 0.3767849070685251 vs. C loss: 0.20065184185902277\n",
      "Group 54, Epoch 2: G loss: 0.3705910384654999 vs. C loss: 0.11410997807979585\n",
      "Group 54, Epoch 3: G loss: 0.441421194587435 vs. C loss: 0.07926208360327615\n",
      "Group 54, Epoch 4: G loss: 0.765847657408033 vs. C loss: 0.020315349567681548\n",
      "Group 54, Epoch 5: G loss: 0.6210766964725085 vs. C loss: 0.09020074136141275\n",
      "Group 54, Epoch 6: G loss: 0.70185575570379 vs. C loss: 0.14639016882412964\n",
      "Group 55, Epoch 1: G loss: 0.29045233045305524 vs. C loss: 0.19334242078993055\n",
      "Group 55, Epoch 2: G loss: 0.23926810451916283 vs. C loss: 0.1382029718822903\n",
      "Group 55, Epoch 3: G loss: 0.5656757133347649 vs. C loss: 0.07748654319180383\n",
      "Group 55, Epoch 4: G loss: 0.5442469656467438 vs. C loss: 0.0767983343038294\n",
      "Group 55, Epoch 5: G loss: 0.2684929311275482 vs. C loss: 0.14912732276651594\n",
      "Group 55, Epoch 6: G loss: 0.43086829696382795 vs. C loss: 0.14661205725537407\n",
      "Group 55, Epoch 7: G loss: 0.40220602963651925 vs. C loss: 0.1895387868086497\n",
      "Group 55, Epoch 8: G loss: 0.44302756296736856 vs. C loss: 0.22021199266115823\n",
      "Group 55, Epoch 9: G loss: 0.05290268063545227 vs. C loss: 0.19939285144209862\n",
      "Group 56, Epoch 1: G loss: 0.43146483557564874 vs. C loss: 0.2175203677680757\n",
      "Group 56, Epoch 2: G loss: 0.29284446792943136 vs. C loss: 0.16461675696902806\n",
      "Group 56, Epoch 3: G loss: 0.33316310729299276 vs. C loss: 0.11310722761683994\n",
      "Group 56, Epoch 4: G loss: 0.3422094472817013 vs. C loss: 0.15127132005161711\n",
      "Group 56, Epoch 5: G loss: 0.40729149580001833 vs. C loss: 0.1636620081133313\n",
      "Group 56, Epoch 6: G loss: 0.19134563228913717 vs. C loss: 0.2117423415184021\n",
      "Group 57, Epoch 1: G loss: 0.47021513240677976 vs. C loss: 0.21722583058807585\n",
      "Group 57, Epoch 2: G loss: 0.19896719689880102 vs. C loss: 0.1798748191859987\n",
      "Group 57, Epoch 3: G loss: 0.22786482061658586 vs. C loss: 0.17154273390769958\n",
      "Group 57, Epoch 4: G loss: 0.3422957318169729 vs. C loss: 0.11470870011382633\n",
      "Group 57, Epoch 5: G loss: 0.5518584770815713 vs. C loss: 0.07674968325429493\n",
      "Group 57, Epoch 6: G loss: 0.5226557152611869 vs. C loss: 0.1580243938499027\n",
      "Group 57, Epoch 7: G loss: 0.4208542547055653 vs. C loss: 0.22311979863378736\n",
      "Group 57, Epoch 8: G loss: 0.06983674988150597 vs. C loss: 0.20429816345373789\n",
      "Group 57, Epoch 9: G loss: 0.04637097993067333 vs. C loss: 0.20034003257751465\n",
      "Group 57, Epoch 10: G loss: 0.054890920008931844 vs. C loss: 0.19029257280959025\n",
      "Group 57, Epoch 11: G loss: 0.2256157381193978 vs. C loss: 0.07693995949294832\n",
      "Group 57, Epoch 12: G loss: 0.9519281949315753 vs. C loss: 0.04430453375809723\n",
      "Group 57, Epoch 13: G loss: 0.8908776794161115 vs. C loss: 0.00880970853096288\n",
      "Group 57, Epoch 14: G loss: 0.5318180365221841 vs. C loss: 0.18873145679632822\n",
      "Group 57, Epoch 15: G loss: 0.6977991155215671 vs. C loss: 0.031027360612319574\n",
      "Group 57, Epoch 16: G loss: 0.8690300103809152 vs. C loss: 0.2344061533610026\n",
      "Group 57, Epoch 17: G loss: 0.05352644356233733 vs. C loss: 0.20526698231697083\n",
      "Group 58, Epoch 1: G loss: 0.21381120383739471 vs. C loss: 0.15853544986910292\n",
      "Group 58, Epoch 2: G loss: 0.27609973422118594 vs. C loss: 0.125463604927063\n",
      "Group 58, Epoch 3: G loss: 0.7141286747796195 vs. C loss: 0.04996127593848441\n",
      "Group 58, Epoch 4: G loss: 0.5686534242970602 vs. C loss: 0.19560828556617102\n",
      "Group 58, Epoch 5: G loss: 0.0721528739801475 vs. C loss: 0.20958870980474684\n",
      "Group 58, Epoch 6: G loss: 0.0713463662990502 vs. C loss: 0.19039291267593703\n",
      "Group 59, Epoch 1: G loss: 0.37168704356466026 vs. C loss: 0.18111537396907806\n",
      "Group 59, Epoch 2: G loss: 0.330469639812197 vs. C loss: 0.13711406621668074\n",
      "Group 59, Epoch 3: G loss: 0.6159264275005886 vs. C loss: 0.062181137088272304\n",
      "Group 59, Epoch 4: G loss: 0.8293956654412405 vs. C loss: 0.018146801946891677\n",
      "Group 59, Epoch 5: G loss: 0.6812242427042552 vs. C loss: 0.05895497960348924\n",
      "Group 59, Epoch 6: G loss: 0.9503449014254979 vs. C loss: 0.03466441585785813\n",
      "Group 60, Epoch 1: G loss: 0.2683875360659191 vs. C loss: 0.18104364143477544\n",
      "Group 60, Epoch 2: G loss: 0.43943726079804557 vs. C loss: 0.09878919687536027\n",
      "Group 60, Epoch 3: G loss: 0.6713836005755833 vs. C loss: 0.05900557328843408\n",
      "Group 60, Epoch 4: G loss: 0.5404853250299182 vs. C loss: 0.0351408664137125\n",
      "Group 60, Epoch 5: G loss: 0.8758080363273619 vs. C loss: 0.04370055078632302\n",
      "Group 60, Epoch 6: G loss: 0.8649763447897774 vs. C loss: 0.008915532225122055\n",
      "Group 61, Epoch 1: G loss: 0.3077185179506029 vs. C loss: 0.16898705396387312\n",
      "Group 61, Epoch 2: G loss: 0.29515485039779116 vs. C loss: 0.12905271227161091\n",
      "Group 61, Epoch 3: G loss: 0.5760421463421412 vs. C loss: 0.043496673719750516\n",
      "Group 61, Epoch 4: G loss: 0.6581319306577954 vs. C loss: 0.054136051485935845\n",
      "Group 61, Epoch 5: G loss: 0.5217441456658499 vs. C loss: 0.18232777383592394\n",
      "Group 61, Epoch 6: G loss: 0.36311080498354775 vs. C loss: 0.2221905498041047\n",
      "Group 61, Epoch 7: G loss: 0.06617417899625642 vs. C loss: 0.20619908471902212\n",
      "Group 61, Epoch 8: G loss: 0.04736678089414324 vs. C loss: 0.20674581660164726\n",
      "Group 62, Epoch 1: G loss: 0.28903462673936575 vs. C loss: 0.16617553763919404\n",
      "Group 62, Epoch 2: G loss: 0.3029190706355231 vs. C loss: 0.15449229793416128\n",
      "Group 62, Epoch 3: G loss: 0.4609905294009617 vs. C loss: 0.1155105506380399\n",
      "Group 62, Epoch 4: G loss: 0.4963753138269697 vs. C loss: 0.1698338571521971\n",
      "Group 62, Epoch 5: G loss: 0.266659568463053 vs. C loss: 0.14561685133311483\n",
      "Group 62, Epoch 6: G loss: 0.7294991374015808 vs. C loss: 0.05574318394064903\n",
      "Group 63, Epoch 1: G loss: 0.2252764071737017 vs. C loss: 0.20615494747956595\n",
      "Group 63, Epoch 2: G loss: 0.3269284478255681 vs. C loss: 0.10814037505123351\n",
      "Group 63, Epoch 3: G loss: 0.6768331357410976 vs. C loss: 0.0446352321240637\n",
      "Group 63, Epoch 4: G loss: 0.6706085971423558 vs. C loss: 0.03443662325541179\n",
      "Group 63, Epoch 5: G loss: 0.5595577532691615 vs. C loss: 0.21445851027965546\n",
      "Group 63, Epoch 6: G loss: 0.01836660858243704 vs. C loss: 0.2080142953329616\n",
      "Group 64, Epoch 1: G loss: 0.3197530891214098 vs. C loss: 0.1565929568476147\n",
      "Group 64, Epoch 2: G loss: 0.3297221686158861 vs. C loss: 0.0975639166103469\n",
      "Group 64, Epoch 3: G loss: 0.7269673994609287 vs. C loss: 0.033962227041936584\n",
      "Group 64, Epoch 4: G loss: 0.536360688507557 vs. C loss: 0.20265567137135398\n",
      "Group 64, Epoch 5: G loss: 0.020552283951214383 vs. C loss: 0.20370588699976602\n",
      "Group 64, Epoch 6: G loss: 0.024707183827246938 vs. C loss: 0.19779016739792296\n",
      "Group 65, Epoch 1: G loss: 0.5020488074847631 vs. C loss: 0.1952721600731214\n",
      "Group 65, Epoch 2: G loss: 0.35190691224166326 vs. C loss: 0.14301398230923545\n",
      "Group 65, Epoch 3: G loss: 0.2809430177722659 vs. C loss: 0.2043182634645038\n",
      "Group 65, Epoch 4: G loss: 0.11517032171998706 vs. C loss: 0.18197623557514617\n",
      "Group 65, Epoch 5: G loss: 0.26923690267971584 vs. C loss: 0.11693655451138814\n",
      "Group 65, Epoch 6: G loss: 0.39334080048969816 vs. C loss: 0.11841031950381069\n",
      "Group 66, Epoch 1: G loss: 0.3182509473391942 vs. C loss: 0.16739186396201453\n",
      "Group 66, Epoch 2: G loss: 0.3252735376358032 vs. C loss: 0.11488493780295052\n",
      "Group 66, Epoch 3: G loss: 0.36896377120699203 vs. C loss: 0.11435770202014182\n",
      "Group 66, Epoch 4: G loss: 0.5189230459077018 vs. C loss: 0.050115142845445215\n",
      "Group 66, Epoch 5: G loss: 0.7224325844219753 vs. C loss: 0.041468133942948446\n",
      "Group 66, Epoch 6: G loss: 0.39550017544201443 vs. C loss: 0.15134564538796744\n",
      "Group 67, Epoch 1: G loss: 0.32524937902178086 vs. C loss: 0.16977844883998236\n",
      "Group 67, Epoch 2: G loss: 0.294692799448967 vs. C loss: 0.14381000896294913\n",
      "Group 67, Epoch 3: G loss: 0.2791736464415278 vs. C loss: 0.12904459817541972\n",
      "Group 67, Epoch 4: G loss: 0.50551307286535 vs. C loss: 0.08595166313979359\n",
      "Group 67, Epoch 5: G loss: 0.6775824121066503 vs. C loss: 0.027600109059777524\n",
      "Group 67, Epoch 6: G loss: 0.8489394341196332 vs. C loss: 0.049446561580730804\n",
      "Group 67, Epoch 7: G loss: 0.9054603389331273 vs. C loss: 0.009685451061361367\n",
      "Group 67, Epoch 8: G loss: 0.21768488846719267 vs. C loss: 0.19757580426004198\n",
      "Group 67, Epoch 9: G loss: 0.09025811244334495 vs. C loss: 0.11984842032608058\n",
      "Group 67, Epoch 10: G loss: 0.9504771062305996 vs. C loss: 0.04516883484191365\n",
      "Group 67, Epoch 11: G loss: 0.8907403860773359 vs. C loss: 0.034222168744438224\n",
      "Group 67, Epoch 12: G loss: 0.34501806284700115 vs. C loss: 0.07582250858346622\n",
      "Group 68, Epoch 1: G loss: 0.3283496903521675 vs. C loss: 0.18746880524688295\n",
      "Group 68, Epoch 2: G loss: 0.23607888945511415 vs. C loss: 0.15413055113620228\n",
      "Group 68, Epoch 3: G loss: 0.4558903411030769 vs. C loss: 0.18566726644833884\n",
      "Group 68, Epoch 4: G loss: 0.2539449376719339 vs. C loss: 0.21587428823113441\n",
      "Group 68, Epoch 5: G loss: 0.0432918317615986 vs. C loss: 0.19842415385776094\n",
      "Group 68, Epoch 6: G loss: 0.04594652450510433 vs. C loss: 0.18360747893651327\n",
      "Group 69, Epoch 1: G loss: 0.428333853823798 vs. C loss: 0.21713429027133516\n",
      "Group 69, Epoch 2: G loss: 0.40105289476258416 vs. C loss: 0.13806675374507904\n",
      "Group 69, Epoch 3: G loss: 0.4110516684395926 vs. C loss: 0.16310801605383554\n",
      "Group 69, Epoch 4: G loss: 0.24193210644381385 vs. C loss: 0.17699443797270456\n",
      "Group 69, Epoch 5: G loss: 0.3008133645568575 vs. C loss: 0.2256095326609082\n",
      "Group 69, Epoch 6: G loss: 0.09609151354857852 vs. C loss: 0.19441291441520056\n",
      "Group 70, Epoch 1: G loss: 0.1592562735080719 vs. C loss: 0.21363730563057792\n",
      "Group 70, Epoch 2: G loss: 0.21207448329244344 vs. C loss: 0.13690736724270713\n",
      "Group 70, Epoch 3: G loss: 0.5029093086719513 vs. C loss: 0.11230000439617367\n",
      "Group 70, Epoch 4: G loss: 0.47466948926448815 vs. C loss: 0.18033567236529457\n",
      "Group 70, Epoch 5: G loss: 0.25139696385179244 vs. C loss: 0.11564372811052535\n",
      "Group 70, Epoch 6: G loss: 0.8183164783886501 vs. C loss: 0.040807496963275805\n",
      "Group 70, Epoch 7: G loss: 0.49047415128776006 vs. C loss: 0.20054307579994202\n",
      "Group 70, Epoch 8: G loss: 0.06106542540448052 vs. C loss: 0.20556501381927064\n",
      "Group 70, Epoch 9: G loss: 0.05478051198380334 vs. C loss: 0.19591681162516275\n",
      "Group 70, Epoch 10: G loss: 0.19331369144575936 vs. C loss: 0.12702824506494734\n",
      "Group 70, Epoch 11: G loss: 0.612604512487139 vs. C loss: 0.0717172258430057\n",
      "Group 70, Epoch 12: G loss: 0.30932016521692274 vs. C loss: 0.2178798665603002\n",
      "Group 70, Epoch 13: G loss: 0.021587589223470007 vs. C loss: 0.20798198382059732\n",
      "Group 71, Epoch 1: G loss: 0.38231435332979474 vs. C loss: 0.1750941913988855\n",
      "Group 71, Epoch 2: G loss: 0.37895009943417135 vs. C loss: 0.14089841809537676\n",
      "Group 71, Epoch 3: G loss: 0.5006178557872772 vs. C loss: 0.18924419085184732\n",
      "Group 71, Epoch 4: G loss: 0.3724668187754495 vs. C loss: 0.14474642359548145\n",
      "Group 71, Epoch 5: G loss: 0.687567048413413 vs. C loss: 0.06486559990379545\n",
      "Group 71, Epoch 6: G loss: 0.41212264767714907 vs. C loss: 0.12932555460267597\n",
      "Group 72, Epoch 1: G loss: 0.09930130222014018 vs. C loss: 0.20254365271992156\n",
      "Group 72, Epoch 2: G loss: 0.12768051496573857 vs. C loss: 0.15647307700581023\n",
      "Group 72, Epoch 3: G loss: 0.24390785694122316 vs. C loss: 0.16965086840920976\n",
      "Group 72, Epoch 4: G loss: 0.3696061330182211 vs. C loss: 0.0918889513446225\n",
      "Group 72, Epoch 5: G loss: 0.7196370840072632 vs. C loss: 0.021513982882930174\n",
      "Group 72, Epoch 6: G loss: 0.8981691837310791 vs. C loss: 0.007382966448656387\n",
      "Group 73, Epoch 1: G loss: 0.2580021687916347 vs. C loss: 0.17749961134460238\n",
      "Group 73, Epoch 2: G loss: 0.35052902868815833 vs. C loss: 0.16735013160440657\n",
      "Group 73, Epoch 3: G loss: 0.4295910085950579 vs. C loss: 0.09511504198114078\n",
      "Group 73, Epoch 4: G loss: 0.7382725017411367 vs. C loss: 0.08656503570576508\n",
      "Group 73, Epoch 5: G loss: 0.31756603121757504 vs. C loss: 0.2091041405995687\n",
      "Group 73, Epoch 6: G loss: 0.010488402923302989 vs. C loss: 0.20771636068820953\n",
      "Group 74, Epoch 1: G loss: 0.1736284668956484 vs. C loss: 0.1796680324607425\n",
      "Group 74, Epoch 2: G loss: 0.29875135506902417 vs. C loss: 0.12673623694313896\n",
      "Group 74, Epoch 3: G loss: 0.3810527614184788 vs. C loss: 0.18727403051323357\n",
      "Group 74, Epoch 4: G loss: 0.2263372199875968 vs. C loss: 0.1756714234749476\n",
      "Group 74, Epoch 5: G loss: 0.20020726897886823 vs. C loss: 0.21653279330995348\n",
      "Group 74, Epoch 6: G loss: 0.05413972788623401 vs. C loss: 0.21598795221911538\n",
      "Group 75, Epoch 1: G loss: 0.4043455924306597 vs. C loss: 0.20886450343661836\n",
      "Group 75, Epoch 2: G loss: 0.2712908915110997 vs. C loss: 0.13331763943036398\n",
      "Group 75, Epoch 3: G loss: 0.5749462127685546 vs. C loss: 0.07953871496849589\n",
      "Group 75, Epoch 4: G loss: 0.5492371542113167 vs. C loss: 0.05693806231849723\n",
      "Group 75, Epoch 5: G loss: 0.7756282789366586 vs. C loss: 0.047833719187312655\n",
      "Group 75, Epoch 6: G loss: 0.31363354570099283 vs. C loss: 0.21516112983226776\n",
      "Group 76, Epoch 1: G loss: 0.4431561180523464 vs. C loss: 0.2289900365802977\n",
      "Group 76, Epoch 2: G loss: 0.4162660441228322 vs. C loss: 0.13562035560607907\n",
      "Group 76, Epoch 3: G loss: 0.46410637157303947 vs. C loss: 0.17592496673266092\n",
      "Group 76, Epoch 4: G loss: 0.21578521536929268 vs. C loss: 0.21836919585863748\n",
      "Group 76, Epoch 5: G loss: 0.04517440364829132 vs. C loss: 0.2035591784450743\n",
      "Group 76, Epoch 6: G loss: 0.060020640705313 vs. C loss: 0.16888442635536194\n",
      "Group 77, Epoch 1: G loss: 0.3531753744397844 vs. C loss: 0.15312377280659148\n",
      "Group 77, Epoch 2: G loss: 0.24578420711415153 vs. C loss: 0.20466477341122094\n",
      "Group 77, Epoch 3: G loss: 0.033428495803049636 vs. C loss: 0.20225735174285042\n",
      "Group 77, Epoch 4: G loss: 0.04652420867766653 vs. C loss: 0.18568431006537542\n",
      "Group 77, Epoch 5: G loss: 0.24629188179969783 vs. C loss: 0.10619480825132795\n",
      "Group 77, Epoch 6: G loss: 0.46202445115361895 vs. C loss: 0.13490105668703714\n",
      "Group 78, Epoch 1: G loss: 0.1058458856173924 vs. C loss: 0.18969940973652732\n",
      "Group 78, Epoch 2: G loss: 0.20612664818763735 vs. C loss: 0.14588143593735164\n",
      "Group 78, Epoch 3: G loss: 0.4884664714336395 vs. C loss: 0.0812403191294935\n",
      "Group 78, Epoch 4: G loss: 0.4618687947945936 vs. C loss: 0.21503282917870417\n",
      "Group 78, Epoch 5: G loss: 0.020359930010246382 vs. C loss: 0.20790152913994261\n",
      "Group 78, Epoch 6: G loss: 0.005308559393909361 vs. C loss: 0.20808748735321891\n",
      "Group 79, Epoch 1: G loss: 0.32232990009444096 vs. C loss: 0.19255011114809248\n",
      "Group 79, Epoch 2: G loss: 0.3807946341378348 vs. C loss: 0.11818316827217738\n",
      "Group 79, Epoch 3: G loss: 0.38302421782697954 vs. C loss: 0.19967039591736266\n",
      "Group 79, Epoch 4: G loss: 0.06944191732576915 vs. C loss: 0.19009743299749163\n",
      "Group 79, Epoch 5: G loss: 0.13041472520147052 vs. C loss: 0.14599444882737264\n",
      "Group 79, Epoch 6: G loss: 0.4702343991824559 vs. C loss: 0.07884833796156777\n",
      "Group 80, Epoch 1: G loss: 0.41946604677609045 vs. C loss: 0.18125995496908823\n",
      "Group 80, Epoch 2: G loss: 0.2420273963894163 vs. C loss: 0.12756683925787607\n",
      "Group 80, Epoch 3: G loss: 0.48766868880816866 vs. C loss: 0.1781608189145724\n",
      "Group 80, Epoch 4: G loss: 0.2027440635221345 vs. C loss: 0.15481891565852693\n",
      "Group 80, Epoch 5: G loss: 0.3193074579749789 vs. C loss: 0.22286064633064798\n",
      "Group 80, Epoch 6: G loss: 0.1751614743045398 vs. C loss: 0.16723156223694483\n",
      "Group 81, Epoch 1: G loss: 0.4478462900434222 vs. C loss: 0.1793523095548153\n",
      "Group 81, Epoch 2: G loss: 0.36381260326930454 vs. C loss: 0.09017234047253926\n",
      "Group 81, Epoch 3: G loss: 0.7899624228477478 vs. C loss: 0.028774417936801914\n",
      "Group 81, Epoch 4: G loss: 0.6769152754119465 vs. C loss: 0.1559493069847425\n",
      "Group 81, Epoch 5: G loss: 0.34070388930184503 vs. C loss: 0.2549535234769185\n",
      "Group 81, Epoch 6: G loss: 0.09568386524915694 vs. C loss: 0.15243159068955317\n",
      "Group 81, Epoch 7: G loss: 0.5367820552417211 vs. C loss: 0.04339671052164502\n",
      "Group 82, Epoch 1: G loss: 0.2847861519881657 vs. C loss: 0.18829387095239428\n",
      "Group 82, Epoch 2: G loss: 0.3900683943714415 vs. C loss: 0.10939254487554234\n",
      "Group 82, Epoch 3: G loss: 0.6781877892357963 vs. C loss: 0.06951409060921934\n",
      "Group 82, Epoch 4: G loss: 0.307480552047491 vs. C loss: 0.20995224598381257\n",
      "Group 82, Epoch 5: G loss: 0.016478474624454974 vs. C loss: 0.20788182525171173\n",
      "Group 82, Epoch 6: G loss: 0.010175690587077823 vs. C loss: 0.2080721060434977\n",
      "Group 83, Epoch 1: G loss: 0.3137287148407527 vs. C loss: 0.2211855368481742\n",
      "Group 83, Epoch 2: G loss: 0.33284239598682946 vs. C loss: 0.1137454683581988\n",
      "Group 83, Epoch 3: G loss: 0.4782324450356619 vs. C loss: 0.11020069155428146\n",
      "Group 83, Epoch 4: G loss: 0.6488265769822256 vs. C loss: 0.1411761078569624\n",
      "Group 83, Epoch 5: G loss: 0.25723216118557113 vs. C loss: 0.22284195986058977\n",
      "Group 83, Epoch 6: G loss: 0.03721872584096023 vs. C loss: 0.20781401958730486\n",
      "Group 84, Epoch 1: G loss: 0.41119633998189664 vs. C loss: 0.22981014351050058\n",
      "Group 84, Epoch 2: G loss: 0.2698393400226321 vs. C loss: 0.18933063083224824\n",
      "Group 84, Epoch 3: G loss: 0.12135014768157688 vs. C loss: 0.1715439549750752\n",
      "Group 84, Epoch 4: G loss: 0.2267131750072752 vs. C loss: 0.12686628682745826\n",
      "Group 84, Epoch 5: G loss: 0.5123320375170026 vs. C loss: 0.07793111850817999\n",
      "Group 84, Epoch 6: G loss: 0.3342893396105085 vs. C loss: 0.19859612153636086\n",
      "Group 85, Epoch 1: G loss: 0.2805096455982753 vs. C loss: 0.1582569405436516\n",
      "Group 85, Epoch 2: G loss: 0.2471769005060196 vs. C loss: 0.15124344329039255\n",
      "Group 85, Epoch 3: G loss: 0.22630006862538204 vs. C loss: 0.19585120512379542\n",
      "Group 85, Epoch 4: G loss: 0.2029642309461321 vs. C loss: 0.15722955928908453\n",
      "Group 85, Epoch 5: G loss: 0.40955418944358823 vs. C loss: 0.1296813901927736\n",
      "Group 85, Epoch 6: G loss: 0.5339467627661568 vs. C loss: 0.06617206624812551\n",
      "Group 86, Epoch 1: G loss: 0.4044239188943591 vs. C loss: 0.1720691439178255\n",
      "Group 86, Epoch 2: G loss: 0.4157199365752084 vs. C loss: 0.0948028133975135\n",
      "Group 86, Epoch 3: G loss: 0.513703634909221 vs. C loss: 0.10797317408853108\n",
      "Group 86, Epoch 4: G loss: 0.415397978254727 vs. C loss: 0.2144395096434487\n",
      "Group 86, Epoch 5: G loss: 0.04517643893403666 vs. C loss: 0.20198563569121888\n",
      "Group 86, Epoch 6: G loss: 0.043344093327011377 vs. C loss: 0.19495620909664368\n",
      "Group 87, Epoch 1: G loss: 0.3734182834625244 vs. C loss: 0.1880232857333289\n",
      "Group 87, Epoch 2: G loss: 0.24883197162832532 vs. C loss: 0.17763142122162712\n",
      "Group 87, Epoch 3: G loss: 0.30341209598949975 vs. C loss: 0.15039122270213232\n",
      "Group 87, Epoch 4: G loss: 0.3520916717393058 vs. C loss: 0.18747169772783914\n",
      "Group 87, Epoch 5: G loss: 0.18278518255267825 vs. C loss: 0.15079596473111048\n",
      "Group 87, Epoch 6: G loss: 0.5456376177924019 vs. C loss: 0.08251894265413286\n",
      "Group 88, Epoch 1: G loss: 0.21286020108631679 vs. C loss: 0.1606906010872788\n",
      "Group 88, Epoch 2: G loss: 0.3387601154191153 vs. C loss: 0.15486915078428057\n",
      "Group 88, Epoch 3: G loss: 0.32636482758181434 vs. C loss: 0.14995756579769984\n",
      "Group 88, Epoch 4: G loss: 0.359425667141165 vs. C loss: 0.17155821124712625\n",
      "Group 88, Epoch 5: G loss: 0.30807575775044305 vs. C loss: 0.18158232089545992\n",
      "Group 88, Epoch 6: G loss: 0.4308188625744411 vs. C loss: 0.07862433708376354\n",
      "Group 89, Epoch 1: G loss: 0.4791679084300995 vs. C loss: 0.20351271828015646\n",
      "Group 89, Epoch 2: G loss: 0.3630816676786969 vs. C loss: 0.15110932456122503\n",
      "Group 89, Epoch 3: G loss: 0.3403244342122759 vs. C loss: 0.19024051394727493\n",
      "Group 89, Epoch 4: G loss: 0.1363598424409117 vs. C loss: 0.2002039435837004\n",
      "Group 89, Epoch 5: G loss: 0.10113717573029656 vs. C loss: 0.20268545051415762\n",
      "Group 89, Epoch 6: G loss: 0.10478256280933107 vs. C loss: 0.17436852720048693\n",
      "Group 90, Epoch 1: G loss: 0.20052041879722052 vs. C loss: 0.15624815060032737\n",
      "Group 90, Epoch 2: G loss: 0.3695441671780178 vs. C loss: 0.1639616522524092\n",
      "Group 90, Epoch 3: G loss: 0.2300750289644514 vs. C loss: 0.15824674235449898\n",
      "Group 90, Epoch 4: G loss: 0.42262446880340576 vs. C loss: 0.09894763595528072\n",
      "Group 90, Epoch 5: G loss: 0.6685172625950404 vs. C loss: 0.06400089917911424\n",
      "Group 90, Epoch 6: G loss: 0.4034973506416594 vs. C loss: 0.21701000465287104\n",
      "Group 91, Epoch 1: G loss: 0.5011096162455423 vs. C loss: 0.26339125798808205\n",
      "Group 91, Epoch 2: G loss: 0.37125375781740455 vs. C loss: 0.1613664229710897\n",
      "Group 91, Epoch 3: G loss: 0.2645567681108202 vs. C loss: 0.12838956713676455\n",
      "Group 91, Epoch 4: G loss: 0.334613099694252 vs. C loss: 0.13824291278918585\n",
      "Group 91, Epoch 5: G loss: 0.45152888894081117 vs. C loss: 0.18399584293365479\n",
      "Group 91, Epoch 6: G loss: 0.10922727159091404 vs. C loss: 0.20586235986815557\n",
      "Group 92, Epoch 1: G loss: 0.3624858123915536 vs. C loss: 0.23131022188398576\n",
      "Group 92, Epoch 2: G loss: 0.2516348357711519 vs. C loss: 0.1576755079958174\n",
      "Group 92, Epoch 3: G loss: 0.2956523669617517 vs. C loss: 0.12237583266364203\n",
      "Group 92, Epoch 4: G loss: 0.3784248036997659 vs. C loss: 0.1962877594762378\n",
      "Group 92, Epoch 5: G loss: 0.15188804034675868 vs. C loss: 0.2289252214961582\n",
      "Group 92, Epoch 6: G loss: 0.01796591132879257 vs. C loss: 0.20659369975328445\n",
      "Group 93, Epoch 1: G loss: 0.28414338486535207 vs. C loss: 0.16662253853347567\n",
      "Group 93, Epoch 2: G loss: 0.4990960104124887 vs. C loss: 0.08655414978663127\n",
      "Group 93, Epoch 3: G loss: 0.5209591942174094 vs. C loss: 0.08410704591208035\n",
      "Group 93, Epoch 4: G loss: 0.643848957334246 vs. C loss: 0.046877109186930783\n",
      "Group 93, Epoch 5: G loss: 0.4746088796428272 vs. C loss: 0.15022006589505407\n",
      "Group 93, Epoch 6: G loss: 0.4325891062085118 vs. C loss: 0.18426735119687188\n",
      "Group 94, Epoch 1: G loss: 0.3832886329718998 vs. C loss: 0.18138351374202302\n",
      "Group 94, Epoch 2: G loss: 0.3738702322755541 vs. C loss: 0.16776353783077666\n",
      "Group 94, Epoch 3: G loss: 0.22644640526601248 vs. C loss: 0.1993420422077179\n",
      "Group 94, Epoch 4: G loss: 0.19141960229192462 vs. C loss: 0.11640670978360707\n",
      "Group 94, Epoch 5: G loss: 0.49302656565393715 vs. C loss: 0.07332131639122963\n",
      "Group 94, Epoch 6: G loss: 0.7175157121249607 vs. C loss: 0.049226219248440534\n",
      "Group 95, Epoch 1: G loss: 0.3371983749525888 vs. C loss: 0.23126942250463697\n",
      "Group 95, Epoch 2: G loss: 0.3487115979194641 vs. C loss: 0.12317804329925114\n",
      "Group 95, Epoch 3: G loss: 0.5893054161752973 vs. C loss: 0.07413163408637045\n",
      "Group 95, Epoch 4: G loss: 0.6765094314302719 vs. C loss: 0.04456427921023634\n",
      "Group 95, Epoch 5: G loss: 0.6951637761933462 vs. C loss: 0.05518991541531351\n",
      "Group 95, Epoch 6: G loss: 0.9131086451666697 vs. C loss: 0.039313009629646935\n",
      "Group 96, Epoch 1: G loss: 0.35691429291452687 vs. C loss: 0.19900575114621055\n",
      "Group 96, Epoch 2: G loss: 0.41868424160139905 vs. C loss: 0.11255277693271636\n",
      "Group 96, Epoch 3: G loss: 0.39598723756415516 vs. C loss: 0.21285549799601236\n",
      "Group 96, Epoch 4: G loss: 0.018005417846143245 vs. C loss: 0.2057154724995295\n",
      "Group 96, Epoch 5: G loss: 0.013218243127422672 vs. C loss: 0.20563144816292658\n",
      "Group 96, Epoch 6: G loss: 0.010549139710409302 vs. C loss: 0.2032727226614952\n",
      "Group 97, Epoch 1: G loss: 0.43390660371099204 vs. C loss: 0.16822418404950035\n",
      "Group 97, Epoch 2: G loss: 0.3555938086339406 vs. C loss: 0.1275817702213923\n",
      "Group 97, Epoch 3: G loss: 0.4172427134854453 vs. C loss: 0.1044935608903567\n",
      "Group 97, Epoch 4: G loss: 0.3489684499800206 vs. C loss: 0.21704202890396118\n",
      "Group 97, Epoch 5: G loss: 0.028400553017854692 vs. C loss: 0.20549384587340883\n",
      "Group 97, Epoch 6: G loss: 0.013109702989459035 vs. C loss: 0.20516752700010935\n",
      "Group 98, Epoch 1: G loss: 0.18582192744527545 vs. C loss: 0.1887214407324791\n",
      "Group 98, Epoch 2: G loss: 0.2581998386553356 vs. C loss: 0.17138215651114783\n",
      "Group 98, Epoch 3: G loss: 0.26814593289579663 vs. C loss: 0.14259310356444782\n",
      "Group 98, Epoch 4: G loss: 0.5092696641172682 vs. C loss: 0.0836438238620758\n",
      "Group 98, Epoch 5: G loss: 0.39917251361267914 vs. C loss: 0.2074856294525994\n",
      "Group 98, Epoch 6: G loss: 0.04114162485514368 vs. C loss: 0.1915190551016066\n",
      "Group 98, Epoch 7: G loss: 0.19435120586838042 vs. C loss: 0.10185760363108581\n",
      "Group 99, Epoch 1: G loss: 0.2430655428341457 vs. C loss: 0.15376383728451198\n",
      "Group 99, Epoch 2: G loss: 0.20912614294460843 vs. C loss: 0.16522535433371863\n",
      "Group 99, Epoch 3: G loss: 0.32361052291733877 vs. C loss: 0.14027163717481825\n",
      "Group 99, Epoch 4: G loss: 0.33150282353162763 vs. C loss: 0.207201459341579\n",
      "Group 99, Epoch 5: G loss: 0.08148522909198488 vs. C loss: 0.1903373139599959\n",
      "Group 99, Epoch 6: G loss: 0.14597577532487255 vs. C loss: 0.18170416686269972\n",
      "Group 100, Epoch 1: G loss: 0.35608677949224193 vs. C loss: 0.20558874805768332\n",
      "Group 100, Epoch 2: G loss: 0.357111291374479 vs. C loss: 0.13598076833619013\n",
      "Group 100, Epoch 3: G loss: 0.36792257087571284 vs. C loss: 0.10245849895808433\n",
      "Group 100, Epoch 4: G loss: 0.33148999235459736 vs. C loss: 0.20891870723830328\n",
      "Group 100, Epoch 5: G loss: 0.05526132572974478 vs. C loss: 0.20113776127497354\n",
      "Group 100, Epoch 6: G loss: 0.03569682836532593 vs. C loss: 0.1975643124638332\n",
      "Group 101, Epoch 1: G loss: 0.4250322869845799 vs. C loss: 0.21979295048448774\n",
      "Group 101, Epoch 2: G loss: 0.31544246162687023 vs. C loss: 0.1984904838932885\n",
      "Group 101, Epoch 3: G loss: 0.07063082839761461 vs. C loss: 0.20369789666599694\n",
      "Group 101, Epoch 4: G loss: 0.06523125150374005 vs. C loss: 0.19618638356526694\n",
      "Group 101, Epoch 5: G loss: 0.09450703124914851 vs. C loss: 0.20159576667679682\n",
      "Group 101, Epoch 6: G loss: 0.05177375365580831 vs. C loss: 0.19980885088443756\n",
      "Group 102, Epoch 1: G loss: 0.2849904328584671 vs. C loss: 0.1609257906675339\n",
      "Group 102, Epoch 2: G loss: 0.33063940576144624 vs. C loss: 0.16087477571434444\n",
      "Group 102, Epoch 3: G loss: 0.2690335984740938 vs. C loss: 0.2068541132741504\n",
      "Group 102, Epoch 4: G loss: 0.07965060121246746 vs. C loss: 0.21229232682122123\n",
      "Group 102, Epoch 5: G loss: 0.04706714940922601 vs. C loss: 0.19641589787271288\n",
      "Group 102, Epoch 6: G loss: 0.06402996789131846 vs. C loss: 0.17488246742222044\n",
      "Group 102, Epoch 7: G loss: 0.20212864215884888 vs. C loss: 0.14571132096979353\n",
      "Group 103, Epoch 1: G loss: 0.2778842738696507 vs. C loss: 0.19970225791136423\n",
      "Group 103, Epoch 2: G loss: 0.2977837903159006 vs. C loss: 0.1321419162882699\n",
      "Group 103, Epoch 3: G loss: 0.48665222866194596 vs. C loss: 0.0761323877506786\n",
      "Group 103, Epoch 4: G loss: 0.41630114891699377 vs. C loss: 0.19938583713438776\n",
      "Group 103, Epoch 5: G loss: 0.06703955201166016 vs. C loss: 0.16264322234524622\n",
      "Group 103, Epoch 6: G loss: 0.4812659859657288 vs. C loss: 0.05555186503463321\n",
      "Group 104, Epoch 1: G loss: 0.36363854152815683 vs. C loss: 0.19496093524826896\n",
      "Group 104, Epoch 2: G loss: 0.30668394991329734 vs. C loss: 0.14867130253050062\n",
      "Group 104, Epoch 3: G loss: 0.44423012222562513 vs. C loss: 0.13923985014359155\n",
      "Group 104, Epoch 4: G loss: 0.26720137777073044 vs. C loss: 0.21530987376657626\n",
      "Group 104, Epoch 5: G loss: 0.01795252284833363 vs. C loss: 0.20416246685716843\n",
      "Group 104, Epoch 6: G loss: 0.01086950573538031 vs. C loss: 0.20331805696090063\n",
      "Group 105, Epoch 1: G loss: 0.46906670757702423 vs. C loss: 0.231963285141521\n",
      "Group 105, Epoch 2: G loss: 0.2634344969476972 vs. C loss: 0.13856766621271768\n",
      "Group 105, Epoch 3: G loss: 0.5132708234446389 vs. C loss: 0.11095509098635781\n",
      "Group 105, Epoch 4: G loss: 0.3786210856267384 vs. C loss: 0.11560676909155315\n",
      "Group 105, Epoch 5: G loss: 0.3077701792120934 vs. C loss: 0.187607676618629\n",
      "Group 105, Epoch 6: G loss: 0.11674096563032695 vs. C loss: 0.16994461003277037\n",
      "Group 106, Epoch 1: G loss: 0.3283159243209021 vs. C loss: 0.18960816164811453\n",
      "Group 106, Epoch 2: G loss: 0.3594935523612159 vs. C loss: 0.17443318168322244\n",
      "Group 106, Epoch 3: G loss: 0.27302105001040866 vs. C loss: 0.1704988239539994\n",
      "Group 106, Epoch 4: G loss: 0.25204997318131583 vs. C loss: 0.17667225499947867\n",
      "Group 106, Epoch 5: G loss: 0.2892072822366442 vs. C loss: 0.16351278954082069\n",
      "Group 106, Epoch 6: G loss: 0.26856243652956824 vs. C loss: 0.1782176825735304\n",
      "Group 107, Epoch 1: G loss: 0.19350551494530271 vs. C loss: 0.15630918161736593\n",
      "Group 107, Epoch 2: G loss: 0.3072188402925219 vs. C loss: 0.14400077362855274\n",
      "Group 107, Epoch 3: G loss: 0.518436029127666 vs. C loss: 0.09757951233122082\n",
      "Group 107, Epoch 4: G loss: 0.2712439477443695 vs. C loss: 0.21318134003215364\n",
      "Group 107, Epoch 5: G loss: 0.010644606833479233 vs. C loss: 0.20799027052190566\n",
      "Group 107, Epoch 6: G loss: 0.003847283936504807 vs. C loss: 0.20787778703702822\n",
      "Group 108, Epoch 1: G loss: 0.44521722963878085 vs. C loss: 0.2669073757198122\n",
      "Group 108, Epoch 2: G loss: 0.4347669644015176 vs. C loss: 0.0966354322930177\n",
      "Group 108, Epoch 3: G loss: 0.6737105471747263 vs. C loss: 0.0497543586211072\n",
      "Group 108, Epoch 4: G loss: 0.5498228622334344 vs. C loss: 0.20163645512527886\n",
      "Group 108, Epoch 5: G loss: 0.05522431555603231 vs. C loss: 0.2126461582051383\n",
      "Group 108, Epoch 6: G loss: 0.021778050118259017 vs. C loss: 0.214491605758667\n",
      "Group 109, Epoch 1: G loss: 0.3352876842021942 vs. C loss: 0.16668330546882418\n",
      "Group 109, Epoch 2: G loss: 0.36350150278636384 vs. C loss: 0.10897343771325219\n",
      "Group 109, Epoch 3: G loss: 0.5436859607696534 vs. C loss: 0.059733903242482074\n",
      "Group 109, Epoch 4: G loss: 0.41256201501403533 vs. C loss: 0.21583555142084757\n",
      "Group 109, Epoch 5: G loss: 0.025052235408553055 vs. C loss: 0.20792728993627763\n",
      "Group 109, Epoch 6: G loss: 0.02183806262910366 vs. C loss: 0.2081582934596958\n",
      "Group 110, Epoch 1: G loss: 0.21264078489371707 vs. C loss: 0.2127727485365338\n",
      "Group 110, Epoch 2: G loss: 0.3661673328706197 vs. C loss: 0.10997138089603847\n",
      "Group 110, Epoch 3: G loss: 0.7517479317528861 vs. C loss: 0.047183612154589764\n",
      "Group 110, Epoch 4: G loss: 0.605835069503103 vs. C loss: 0.1335500507718987\n",
      "Group 110, Epoch 5: G loss: 0.8529194482735225 vs. C loss: 0.15630490663978788\n",
      "Group 110, Epoch 6: G loss: 0.4719346635043621 vs. C loss: 0.23009026050567627\n",
      "Group 111, Epoch 1: G loss: 0.3635632855551583 vs. C loss: 0.20788717104329002\n",
      "Group 111, Epoch 2: G loss: 0.44335757493972777 vs. C loss: 0.17343160013357797\n",
      "Group 111, Epoch 3: G loss: 0.290655923315457 vs. C loss: 0.1296304596794976\n",
      "Group 111, Epoch 4: G loss: 0.43253629973956514 vs. C loss: 0.1040972073872884\n",
      "Group 111, Epoch 5: G loss: 0.467951512336731 vs. C loss: 0.08778591247068511\n",
      "Group 111, Epoch 6: G loss: 0.5815004361527307 vs. C loss: 0.10991415133078893\n",
      "Group 112, Epoch 1: G loss: 0.28326596106801716 vs. C loss: 0.19980963236755797\n",
      "Group 112, Epoch 2: G loss: 0.3351703422410147 vs. C loss: 0.09562833358844121\n",
      "Group 112, Epoch 3: G loss: 0.4508897594043187 vs. C loss: 0.09381101611587737\n",
      "Group 112, Epoch 4: G loss: 0.29563662154333936 vs. C loss: 0.21835187160306505\n",
      "Group 112, Epoch 5: G loss: 0.005810390120106085 vs. C loss: 0.20799904730584884\n",
      "Group 112, Epoch 6: G loss: 0.001017258020250925 vs. C loss: 0.20809072918362084\n",
      "Group 113, Epoch 1: G loss: 0.428623446396419 vs. C loss: 0.267900601029396\n",
      "Group 113, Epoch 2: G loss: 0.3375463660274233 vs. C loss: 0.18076674474610221\n",
      "Group 113, Epoch 3: G loss: 0.15193789771624974 vs. C loss: 0.16866988440354666\n",
      "Group 113, Epoch 4: G loss: 0.29160569821085247 vs. C loss: 0.14191614422533247\n",
      "Group 113, Epoch 5: G loss: 0.3500011729342597 vs. C loss: 0.20374575754006705\n",
      "Group 113, Epoch 6: G loss: 0.06293086452143533 vs. C loss: 0.20079328533675936\n",
      "Group 114, Epoch 1: G loss: 0.25998062704290664 vs. C loss: 0.1417140695783827\n",
      "Group 114, Epoch 2: G loss: 0.371696418098041 vs. C loss: 0.13483008080058628\n",
      "Group 114, Epoch 3: G loss: 0.561482231106077 vs. C loss: 0.12730416076050863\n",
      "Group 114, Epoch 4: G loss: 0.7020879626274109 vs. C loss: 0.02488330234256056\n",
      "Group 114, Epoch 5: G loss: 0.6858759573527745 vs. C loss: 0.11184278730716969\n",
      "Group 114, Epoch 6: G loss: 0.6176798570368971 vs. C loss: 0.1716318709982766\n",
      "Group 114, Epoch 7: G loss: 0.38795931722436633 vs. C loss: 0.22923621700869665\n",
      "Group 115, Epoch 1: G loss: 0.3025038072041103 vs. C loss: 0.15741977509525087\n",
      "Group 115, Epoch 2: G loss: 0.22183692157268525 vs. C loss: 0.20894350856542587\n",
      "Group 115, Epoch 3: G loss: 0.029282140891466822 vs. C loss: 0.20187503099441528\n",
      "Group 115, Epoch 4: G loss: 0.03727237337401935 vs. C loss: 0.1829596625434028\n",
      "Group 115, Epoch 5: G loss: 0.24151424424988882 vs. C loss: 0.1281173312001758\n",
      "Group 115, Epoch 6: G loss: 0.6460837270532336 vs. C loss: 0.08883284777402878\n",
      "Group 115, Epoch 7: G loss: 0.37047106463994295 vs. C loss: 0.22317315803633797\n",
      "Group 116, Epoch 1: G loss: 0.298007931453841 vs. C loss: 0.1760494477219052\n",
      "Group 116, Epoch 2: G loss: 0.3388479254075459 vs. C loss: 0.11072023668222956\n",
      "Group 116, Epoch 3: G loss: 0.318887505361012 vs. C loss: 0.13309058960941103\n",
      "Group 116, Epoch 4: G loss: 0.4195397466421128 vs. C loss: 0.20292973352803123\n",
      "Group 116, Epoch 5: G loss: 0.3885584833366531 vs. C loss: 0.21236374808682337\n",
      "Group 116, Epoch 6: G loss: 0.05663361240710531 vs. C loss: 0.1965573231379191\n",
      "Group 117, Epoch 1: G loss: 0.5585229805537633 vs. C loss: 0.24479095803366768\n",
      "Group 117, Epoch 2: G loss: 0.2655750091586794 vs. C loss: 0.12812364515331057\n",
      "Group 117, Epoch 3: G loss: 0.38761547889028275 vs. C loss: 0.14580125858386359\n",
      "Group 117, Epoch 4: G loss: 0.5083290917532785 vs. C loss: 0.0892833781739076\n",
      "Group 117, Epoch 5: G loss: 0.3762257418462208 vs. C loss: 0.15120283597045472\n",
      "Group 117, Epoch 6: G loss: 0.3017131005014692 vs. C loss: 0.12347248237993981\n",
      "Group 118, Epoch 1: G loss: 0.5355822886739459 vs. C loss: 0.23681679285234872\n",
      "Group 118, Epoch 2: G loss: 0.35030179790088106 vs. C loss: 0.11022907454106544\n",
      "Group 118, Epoch 3: G loss: 0.3905254819563457 vs. C loss: 0.1947846627897686\n",
      "Group 118, Epoch 4: G loss: 0.045460709503718784 vs. C loss: 0.1935052565402455\n",
      "Group 118, Epoch 5: G loss: 0.08878925038235529 vs. C loss: 0.1898446414205763\n",
      "Group 118, Epoch 6: G loss: 0.21091845461300437 vs. C loss: 0.14489742616812387\n",
      "Group 119, Epoch 1: G loss: 0.3749841843332563 vs. C loss: 0.16873990495999655\n",
      "Group 119, Epoch 2: G loss: 0.35195236461503165 vs. C loss: 0.1133320364687178\n",
      "Group 119, Epoch 3: G loss: 0.6137598889214653 vs. C loss: 0.040062159299850464\n",
      "Group 119, Epoch 4: G loss: 0.6109222454684121 vs. C loss: 0.05109831111298666\n",
      "Group 119, Epoch 5: G loss: 0.9315413134438651 vs. C loss: 0.007807911994556586\n",
      "Group 119, Epoch 6: G loss: 0.6882799325244767 vs. C loss: 0.11607823293242191\n",
      "Group 120, Epoch 1: G loss: 0.27471641855580464 vs. C loss: 0.1918381949265798\n",
      "Group 120, Epoch 2: G loss: 0.29605039443288533 vs. C loss: 0.10317811783817081\n",
      "Group 120, Epoch 3: G loss: 0.5216923560414995 vs. C loss: 0.048925080233150064\n",
      "Group 120, Epoch 4: G loss: 0.676362783568246 vs. C loss: 0.027243787422776222\n",
      "Group 120, Epoch 5: G loss: 0.6731946732316699 vs. C loss: 0.06913654547598627\n",
      "Group 120, Epoch 6: G loss: 0.8515019008091518 vs. C loss: 0.032134689597619906\n",
      "Group 121, Epoch 1: G loss: 0.3468894302845001 vs. C loss: 0.19734420958492493\n",
      "Group 121, Epoch 2: G loss: 0.31229949721268246 vs. C loss: 0.13420255316628352\n",
      "Group 121, Epoch 3: G loss: 0.4526497828108923 vs. C loss: 0.1245869365003374\n",
      "Group 121, Epoch 4: G loss: 0.5946160078048707 vs. C loss: 0.0793446766005622\n",
      "Group 121, Epoch 5: G loss: 0.6125235864094325 vs. C loss: 0.18316949324475396\n",
      "Group 121, Epoch 6: G loss: 0.10484435462525914 vs. C loss: 0.20736589696672228\n",
      "Group 122, Epoch 1: G loss: 0.4735286644526891 vs. C loss: 0.23168062335915038\n",
      "Group 122, Epoch 2: G loss: 0.26084280737808774 vs. C loss: 0.1989085798462232\n",
      "Group 122, Epoch 3: G loss: 0.18740890409265248 vs. C loss: 0.12474274138609569\n",
      "Group 122, Epoch 4: G loss: 0.2906949266791344 vs. C loss: 0.21531136002805498\n",
      "Group 122, Epoch 5: G loss: 0.03548305886132377 vs. C loss: 0.20660720931159124\n",
      "Group 122, Epoch 6: G loss: 0.019179085003478185 vs. C loss: 0.20510763923327127\n",
      "Group 123, Epoch 1: G loss: 0.3473669052124023 vs. C loss: 0.19434723920292327\n",
      "Group 123, Epoch 2: G loss: 0.29930535980633327 vs. C loss: 0.10808899501959483\n",
      "Group 123, Epoch 3: G loss: 0.33088581349168505 vs. C loss: 0.19358034080101383\n",
      "Group 123, Epoch 4: G loss: 0.1023863903113774 vs. C loss: 0.19268427126937446\n",
      "Group 123, Epoch 5: G loss: 0.19380820010389596 vs. C loss: 0.16956368254290688\n",
      "Group 123, Epoch 6: G loss: 0.3796578441347394 vs. C loss: 0.1297133531835344\n",
      "Group 124, Epoch 1: G loss: 0.5218542822769711 vs. C loss: 0.24671565161810982\n",
      "Group 124, Epoch 2: G loss: 0.3450788795948028 vs. C loss: 0.1447270918223593\n",
      "Group 124, Epoch 3: G loss: 0.38081331934247703 vs. C loss: 0.1435075600941976\n",
      "Group 124, Epoch 4: G loss: 0.5139176700796401 vs. C loss: 0.1111375018954277\n",
      "Group 124, Epoch 5: G loss: 0.4234680218355996 vs. C loss: 0.21972395645247567\n",
      "Group 124, Epoch 6: G loss: 0.010380146439586369 vs. C loss: 0.20778152470787367\n",
      "Group 125, Epoch 1: G loss: 0.43523901530674525 vs. C loss: 0.20699074533250597\n",
      "Group 125, Epoch 2: G loss: 0.31841441605772297 vs. C loss: 0.14495730648438135\n",
      "Group 125, Epoch 3: G loss: 0.3588155065264021 vs. C loss: 0.10529844214518864\n",
      "Group 125, Epoch 4: G loss: 0.41064138114452364 vs. C loss: 0.14683577252758875\n",
      "Group 125, Epoch 5: G loss: 0.3352386087179185 vs. C loss: 0.1544597393108739\n",
      "Group 125, Epoch 6: G loss: 0.351626553173576 vs. C loss: 0.2273893008629481\n",
      "Group 126, Epoch 1: G loss: 0.48190908687455314 vs. C loss: 0.16533020304308998\n",
      "Group 126, Epoch 2: G loss: 0.5999691929136004 vs. C loss: 0.03393634087923501\n",
      "Group 126, Epoch 3: G loss: 0.4050933316349982 vs. C loss: 0.19501352641317582\n",
      "Group 126, Epoch 4: G loss: 0.10958798889602932 vs. C loss: 0.09183492056197591\n",
      "Group 126, Epoch 5: G loss: 0.7085667295115334 vs. C loss: 0.05777180774344339\n",
      "Group 126, Epoch 6: G loss: 0.6928866948400225 vs. C loss: 0.08365237154066563\n",
      "Group 127, Epoch 1: G loss: 0.4482482518468584 vs. C loss: 0.22782708870040047\n",
      "Group 127, Epoch 2: G loss: 0.32286894193717414 vs. C loss: 0.1504658568236563\n",
      "Group 127, Epoch 3: G loss: 0.25666247691426963 vs. C loss: 0.1536000867684682\n",
      "Group 127, Epoch 4: G loss: 0.4225593507289886 vs. C loss: 0.08043254911899568\n",
      "Group 127, Epoch 5: G loss: 0.617051877294268 vs. C loss: 0.049598986489905246\n",
      "Group 127, Epoch 6: G loss: 0.5392140814236231 vs. C loss: 0.21519904666476783\n",
      "Group 128, Epoch 1: G loss: 0.293696974005018 vs. C loss: 0.14162360835406515\n",
      "Group 128, Epoch 2: G loss: 0.3391023284622601 vs. C loss: 0.17347423566712272\n",
      "Group 128, Epoch 3: G loss: 0.2540365334068026 vs. C loss: 0.13937785973151523\n",
      "Group 128, Epoch 4: G loss: 0.5697698107787541 vs. C loss: 0.06460995765195952\n",
      "Group 128, Epoch 5: G loss: 0.740101703149932 vs. C loss: 0.2365280439456304\n",
      "Group 128, Epoch 6: G loss: 0.062039551138877866 vs. C loss: 0.11499450190199746\n",
      "Group 129, Epoch 1: G loss: 0.3403306944029672 vs. C loss: 0.1552355339129766\n",
      "Group 129, Epoch 2: G loss: 0.34877553539616724 vs. C loss: 0.08490254191888703\n",
      "Group 129, Epoch 3: G loss: 0.7088452041149139 vs. C loss: 0.053397760209110044\n",
      "Group 129, Epoch 4: G loss: 0.32471215724945063 vs. C loss: 0.1808791442049874\n",
      "Group 129, Epoch 5: G loss: 0.07240014326359545 vs. C loss: 0.21764162182807922\n",
      "Group 129, Epoch 6: G loss: 0.022577189254973613 vs. C loss: 0.2223782738049825\n",
      "Group 130, Epoch 1: G loss: 0.3297555821282523 vs. C loss: 0.24111401869191065\n",
      "Group 130, Epoch 2: G loss: 0.3728137816701617 vs. C loss: 0.10591743555333878\n",
      "Group 130, Epoch 3: G loss: 0.5101147728306907 vs. C loss: 0.12219860073592927\n",
      "Group 130, Epoch 4: G loss: 0.28711642005613874 vs. C loss: 0.20459363361199698\n",
      "Group 130, Epoch 5: G loss: 0.06553379180175918 vs. C loss: 0.19627212319109175\n",
      "Group 130, Epoch 6: G loss: 0.1081990591117314 vs. C loss: 0.17080397241645387\n",
      "Group 131, Epoch 1: G loss: 0.365975455726896 vs. C loss: 0.18424104298982355\n",
      "Group 131, Epoch 2: G loss: 0.227914083642619 vs. C loss: 0.2097284628285302\n",
      "Group 131, Epoch 3: G loss: 0.06750735278640474 vs. C loss: 0.16475541806883282\n",
      "Group 131, Epoch 4: G loss: 0.32842382448060176 vs. C loss: 0.17598194380601248\n",
      "Group 131, Epoch 5: G loss: 0.22936685319457736 vs. C loss: 0.21573825346099007\n",
      "Group 131, Epoch 6: G loss: 0.08361916648490089 vs. C loss: 0.15554722481303743\n",
      "Group 132, Epoch 1: G loss: 0.41742265479905266 vs. C loss: 0.24169506049818465\n",
      "Group 132, Epoch 2: G loss: 0.4003695828574044 vs. C loss: 0.13558745094471505\n",
      "Group 132, Epoch 3: G loss: 0.24896818505866183 vs. C loss: 0.1636373334460788\n",
      "Group 132, Epoch 4: G loss: 0.384875548524516 vs. C loss: 0.27194272147284615\n",
      "Group 132, Epoch 5: G loss: 0.028142378319587026 vs. C loss: 0.20736682994498146\n",
      "Group 132, Epoch 6: G loss: 0.010611097594457015 vs. C loss: 0.20446682969729105\n",
      "Group 133, Epoch 1: G loss: 0.25673909868512834 vs. C loss: 0.18682954791519377\n",
      "Group 133, Epoch 2: G loss: 0.25101915087018695 vs. C loss: 0.17006286150879332\n",
      "Group 133, Epoch 3: G loss: 0.18911659728203503 vs. C loss: 0.23663162522845796\n",
      "Group 133, Epoch 4: G loss: 0.015211543999612331 vs. C loss: 0.2107975449826982\n",
      "Group 133, Epoch 5: G loss: 0.009418249715651786 vs. C loss: 0.206133547756407\n",
      "Group 133, Epoch 6: G loss: 0.006310731598309109 vs. C loss: 0.20596122990051904\n",
      "Group 134, Epoch 1: G loss: 0.3021286300250462 vs. C loss: 0.21747410628530717\n",
      "Group 134, Epoch 2: G loss: 0.31089844192777366 vs. C loss: 0.12031907919380401\n",
      "Group 134, Epoch 3: G loss: 0.423761892744473 vs. C loss: 0.14042735348145166\n",
      "Group 134, Epoch 4: G loss: 0.2262077791350228 vs. C loss: 0.18279161221451226\n",
      "Group 134, Epoch 5: G loss: 0.15145826084273203 vs. C loss: 0.23248244987593755\n",
      "Group 134, Epoch 6: G loss: 0.0069002724119595115 vs. C loss: 0.20632458478212357\n",
      "Group 135, Epoch 1: G loss: 0.29457082067217144 vs. C loss: 0.16081629693508148\n",
      "Group 135, Epoch 2: G loss: 0.36009408874171117 vs. C loss: 0.09926507125298183\n",
      "Group 135, Epoch 3: G loss: 0.5050527589661734 vs. C loss: 0.07570474387870894\n",
      "Group 135, Epoch 4: G loss: 0.7369836594377246 vs. C loss: 0.05520008349170288\n",
      "Group 135, Epoch 5: G loss: 0.7321023344993592 vs. C loss: 0.06532012267659108\n",
      "Group 135, Epoch 6: G loss: 0.9196140715054104 vs. C loss: 0.005161733340678943\n",
      "Group 136, Epoch 1: G loss: 0.3448035146508898 vs. C loss: 0.15615248597330517\n",
      "Group 136, Epoch 2: G loss: 0.3631168093000139 vs. C loss: 0.14219042658805844\n",
      "Group 136, Epoch 3: G loss: 0.5245943989072528 vs. C loss: 0.06660823234253459\n",
      "Group 136, Epoch 4: G loss: 0.616236971957343 vs. C loss: 0.12622634652588102\n",
      "Group 136, Epoch 5: G loss: 0.2752663914646421 vs. C loss: 0.14054538475142586\n",
      "Group 136, Epoch 6: G loss: 0.35128072212849343 vs. C loss: 0.23680691255463493\n",
      "Group 137, Epoch 1: G loss: 0.3897930707250322 vs. C loss: 0.14098931724826494\n",
      "Group 137, Epoch 2: G loss: 0.2981029272079468 vs. C loss: 0.1452535931020975\n",
      "Group 137, Epoch 3: G loss: 0.23063223149095266 vs. C loss: 0.17685450613498688\n",
      "Group 137, Epoch 4: G loss: 0.1454417428800038 vs. C loss: 0.22590737210379708\n",
      "Group 137, Epoch 5: G loss: 0.023344639608902592 vs. C loss: 0.20840053922600218\n",
      "Group 137, Epoch 6: G loss: 0.019260024971195628 vs. C loss: 0.19788065883848405\n",
      "Group 138, Epoch 1: G loss: 0.311445848430906 vs. C loss: 0.20762410180436242\n",
      "Group 138, Epoch 2: G loss: 0.3276194193533488 vs. C loss: 0.16935158024231592\n",
      "Group 138, Epoch 3: G loss: 0.2158668215785708 vs. C loss: 0.16557877179649141\n",
      "Group 138, Epoch 4: G loss: 0.589170777797699 vs. C loss: 0.08353428294261296\n",
      "Group 138, Epoch 5: G loss: 0.4493476314204079 vs. C loss: 0.05415519099268648\n",
      "Group 138, Epoch 6: G loss: 0.9321145227977208 vs. C loss: 0.03597307619121339\n",
      "Group 139, Epoch 1: G loss: 0.4517911093575614 vs. C loss: 0.21618969821267656\n",
      "Group 139, Epoch 2: G loss: 0.3412339159420559 vs. C loss: 0.1323238934079806\n",
      "Group 139, Epoch 3: G loss: 0.5625722987311226 vs. C loss: 0.08839744495020974\n",
      "Group 139, Epoch 4: G loss: 0.47734114612851825 vs. C loss: 0.09574536565277313\n",
      "Group 139, Epoch 5: G loss: 0.7121567385537284 vs. C loss: 0.06703781874643432\n",
      "Group 139, Epoch 6: G loss: 0.8373542053358894 vs. C loss: 0.02514607459306717\n",
      "Group 140, Epoch 1: G loss: 0.1680131856884275 vs. C loss: 0.18031609223948586\n",
      "Group 140, Epoch 2: G loss: 0.18865481104169574 vs. C loss: 0.1508331075310707\n",
      "Group 140, Epoch 3: G loss: 0.21776496853147234 vs. C loss: 0.1174615000685056\n",
      "Group 140, Epoch 4: G loss: 0.556564211845398 vs. C loss: 0.07338482638200124\n",
      "Group 140, Epoch 5: G loss: 0.3805909603301968 vs. C loss: 0.21476775738928053\n",
      "Group 140, Epoch 6: G loss: 0.0066355958314878605 vs. C loss: 0.20756708996163473\n",
      "Group 141, Epoch 1: G loss: 0.3705688834190369 vs. C loss: 0.1848912818564309\n",
      "Group 141, Epoch 2: G loss: 0.3192807359354836 vs. C loss: 0.17145355501108703\n",
      "Group 141, Epoch 3: G loss: 0.2735176099198205 vs. C loss: 0.11203254262606303\n",
      "Group 141, Epoch 4: G loss: 0.6085381346089499 vs. C loss: 0.10588019755151536\n",
      "Group 141, Epoch 5: G loss: 0.3889989163194384 vs. C loss: 0.12949230439133114\n",
      "Group 141, Epoch 6: G loss: 0.3827057337654488 vs. C loss: 0.22370995415581596\n",
      "Group 142, Epoch 1: G loss: 0.2631828942469188 vs. C loss: 0.19553502400716147\n",
      "Group 142, Epoch 2: G loss: 0.23021144909518104 vs. C loss: 0.149541474878788\n",
      "Group 142, Epoch 3: G loss: 0.3423953545945032 vs. C loss: 0.14553498228391012\n",
      "Group 142, Epoch 4: G loss: 0.275900776897158 vs. C loss: 0.1720361395014657\n",
      "Group 142, Epoch 5: G loss: 0.17853395183171547 vs. C loss: 0.2083026518424352\n",
      "Group 142, Epoch 6: G loss: 0.04591400165643011 vs. C loss: 0.1928488810857137\n",
      "Group 143, Epoch 1: G loss: 0.4375044592789242 vs. C loss: 0.23749193052450815\n",
      "Group 143, Epoch 2: G loss: 0.3234719574451447 vs. C loss: 0.13765616714954376\n",
      "Group 143, Epoch 3: G loss: 0.4603232962744577 vs. C loss: 0.07683540797895856\n",
      "Group 143, Epoch 4: G loss: 0.6742778965405055 vs. C loss: 0.08783328864309521\n",
      "Group 143, Epoch 5: G loss: 0.5156118669680188 vs. C loss: 0.21490223374631667\n",
      "Group 143, Epoch 6: G loss: 0.08323483754481587 vs. C loss: 0.19953504287534288\n",
      "Group 144, Epoch 1: G loss: 0.27520656841141833 vs. C loss: 0.19429986344443428\n",
      "Group 144, Epoch 2: G loss: 0.3470642736979893 vs. C loss: 0.1512995246383879\n",
      "Group 144, Epoch 3: G loss: 0.3168367737105915 vs. C loss: 0.21687472032176122\n",
      "Group 144, Epoch 4: G loss: 0.029554134554096636 vs. C loss: 0.20317655139499238\n",
      "Group 144, Epoch 5: G loss: 0.011154913689408982 vs. C loss: 0.20394306121549258\n",
      "Group 144, Epoch 6: G loss: 0.014607901232583181 vs. C loss: 0.20039717853069305\n",
      "Group 145, Epoch 1: G loss: 0.2834653573376792 vs. C loss: 0.18583031164275277\n",
      "Group 145, Epoch 2: G loss: 0.21814252776759013 vs. C loss: 0.14044199304448232\n",
      "Group 145, Epoch 3: G loss: 0.44423019204820907 vs. C loss: 0.10301599113477601\n",
      "Group 145, Epoch 4: G loss: 0.6925704853875297 vs. C loss: 0.08372039099534352\n",
      "Group 145, Epoch 5: G loss: 0.5860116355121135 vs. C loss: 0.21429380774497986\n",
      "Group 145, Epoch 6: G loss: 0.009381430662636246 vs. C loss: 0.2124024861388736\n",
      "Group 146, Epoch 1: G loss: 0.21748611288411276 vs. C loss: 0.17629859182569718\n",
      "Group 146, Epoch 2: G loss: 0.3885694895471845 vs. C loss: 0.10906801455550724\n",
      "Group 146, Epoch 3: G loss: 0.4894314595631191 vs. C loss: 0.06415351521637704\n",
      "Group 146, Epoch 4: G loss: 0.21116983438176765 vs. C loss: 0.21246911254194048\n",
      "Group 146, Epoch 5: G loss: 0.004110973720837917 vs. C loss: 0.2076153109471003\n",
      "Group 146, Epoch 6: G loss: 0.00233973093064768 vs. C loss: 0.20786873499552408\n",
      "Group 147, Epoch 1: G loss: 0.3120326714856284 vs. C loss: 0.21381970577769813\n",
      "Group 147, Epoch 2: G loss: 0.2975675599915641 vs. C loss: 0.10571148660447861\n",
      "Group 147, Epoch 3: G loss: 0.3778461405209133 vs. C loss: 0.06948717145456208\n",
      "Group 147, Epoch 4: G loss: 0.8067336899893622 vs. C loss: 0.007175329762200515\n",
      "Group 147, Epoch 5: G loss: 0.4658380870308195 vs. C loss: 0.050352630929814436\n",
      "Group 147, Epoch 6: G loss: 0.9530309200286864 vs. C loss: 0.03986086458381679\n",
      "Group 148, Epoch 1: G loss: 0.4355447930949075 vs. C loss: 0.18172861138979593\n",
      "Group 148, Epoch 2: G loss: 0.3657562345266342 vs. C loss: 0.13372914575868183\n",
      "Group 148, Epoch 3: G loss: 0.4434224392686571 vs. C loss: 0.10680445283651352\n",
      "Group 148, Epoch 4: G loss: 0.48703062917504997 vs. C loss: 0.11743630924158625\n",
      "Group 148, Epoch 5: G loss: 0.3061712635414941 vs. C loss: 0.22056912879149118\n",
      "Group 148, Epoch 6: G loss: 0.01577140372246504 vs. C loss: 0.2027024858527713\n",
      "Group 149, Epoch 1: G loss: 0.2696927164282118 vs. C loss: 0.15292825301488241\n",
      "Group 149, Epoch 2: G loss: 0.27937018445559914 vs. C loss: 0.18975454568862915\n",
      "Group 149, Epoch 3: G loss: 0.18047435730695724 vs. C loss: 0.1106264582938618\n",
      "Group 149, Epoch 4: G loss: 0.6688680768013001 vs. C loss: 0.04694185757802593\n",
      "Group 149, Epoch 5: G loss: 0.2891404143401554 vs. C loss: 0.19120219432645372\n",
      "Group 149, Epoch 6: G loss: 0.045343586163861416 vs. C loss: 0.20908274915483263\n",
      "Group 150, Epoch 1: G loss: 0.29787520723683497 vs. C loss: 0.20787689255343544\n",
      "Group 150, Epoch 2: G loss: 0.2768911944968359 vs. C loss: 0.16431768900818294\n",
      "Group 150, Epoch 3: G loss: 0.385673052072525 vs. C loss: 0.10321854137712055\n",
      "Group 150, Epoch 4: G loss: 0.7074693134852819 vs. C loss: 0.037345096055004336\n",
      "Group 150, Epoch 5: G loss: 0.4856580868363381 vs. C loss: 0.11806236414445771\n",
      "Group 150, Epoch 6: G loss: 0.4616261595061847 vs. C loss: 0.20001066227753958\n",
      "Group 151, Epoch 1: G loss: 0.434140728201185 vs. C loss: 0.20270111329025695\n",
      "Group 151, Epoch 2: G loss: 0.3876097474779402 vs. C loss: 0.13291158609920078\n",
      "Group 151, Epoch 3: G loss: 0.356089643069676 vs. C loss: 0.1069963996609052\n",
      "Group 151, Epoch 4: G loss: 0.6915440678596497 vs. C loss: 0.04938413513203462\n",
      "Group 151, Epoch 5: G loss: 0.3514407766716821 vs. C loss: 0.21812521914641061\n",
      "Group 151, Epoch 6: G loss: 0.0748266788465636 vs. C loss: 0.17424998349613616\n",
      "Group 152, Epoch 1: G loss: 0.2722626732928412 vs. C loss: 0.1736960121326976\n",
      "Group 152, Epoch 2: G loss: 0.2928193960871015 vs. C loss: 0.13502045969168344\n",
      "Group 152, Epoch 3: G loss: 0.42269787064620434 vs. C loss: 0.14664532161421245\n",
      "Group 152, Epoch 4: G loss: 0.39685621793781006 vs. C loss: 0.21037782894240484\n",
      "Group 152, Epoch 5: G loss: 0.0654205257339137 vs. C loss: 0.17189233501752219\n",
      "Group 152, Epoch 6: G loss: 0.2691715785435268 vs. C loss: 0.15129808419280583\n",
      "Group 153, Epoch 1: G loss: 0.506125144447599 vs. C loss: 0.22938734458552465\n",
      "Group 153, Epoch 2: G loss: 0.28019546866416933 vs. C loss: 0.13687224355008867\n",
      "Group 153, Epoch 3: G loss: 0.38432637155056 vs. C loss: 0.1322506790359815\n",
      "Group 153, Epoch 4: G loss: 0.6999845598425184 vs. C loss: 0.14134150246779123\n",
      "Group 153, Epoch 5: G loss: 0.23878266151462285 vs. C loss: 0.17426275544696382\n",
      "Group 153, Epoch 6: G loss: 0.27694644651242667 vs. C loss: 0.1441172245475981\n",
      "Group 154, Epoch 1: G loss: 0.3913417765072414 vs. C loss: 0.18607408222224978\n",
      "Group 154, Epoch 2: G loss: 0.4157880663871764 vs. C loss: 0.10561540640062757\n",
      "Group 154, Epoch 3: G loss: 0.4514086995806013 vs. C loss: 0.1303592502242989\n",
      "Group 154, Epoch 4: G loss: 0.28279123732021877 vs. C loss: 0.211356391509374\n",
      "Group 154, Epoch 5: G loss: 0.07367021377597537 vs. C loss: 0.18257765720287958\n",
      "Group 154, Epoch 6: G loss: 0.16948627212217876 vs. C loss: 0.13394070665041605\n",
      "Group 155, Epoch 1: G loss: 0.28949933520385196 vs. C loss: 0.17327517353826102\n",
      "Group 155, Epoch 2: G loss: 0.22632991373538972 vs. C loss: 0.15550733854373297\n",
      "Group 155, Epoch 3: G loss: 0.33504016612257276 vs. C loss: 0.12659935653209686\n",
      "Group 155, Epoch 4: G loss: 0.56547155720847 vs. C loss: 0.057912857996092894\n",
      "Group 155, Epoch 5: G loss: 0.36513022865567885 vs. C loss: 0.16987680064307317\n",
      "Group 155, Epoch 6: G loss: 0.3716471769980022 vs. C loss: 0.202594174279107\n",
      "Group 156, Epoch 1: G loss: 0.21629977779729023 vs. C loss: 0.1890031480126911\n",
      "Group 156, Epoch 2: G loss: 0.311639649953161 vs. C loss: 0.1348632781041993\n",
      "Group 156, Epoch 3: G loss: 0.4760315763098853 vs. C loss: 0.13407731552918753\n",
      "Group 156, Epoch 4: G loss: 0.33302876736436576 vs. C loss: 0.20943480564488304\n",
      "Group 156, Epoch 5: G loss: 0.03840197399258614 vs. C loss: 0.19559334880775878\n",
      "Group 156, Epoch 6: G loss: 0.09249892554112844 vs. C loss: 0.13892285029093426\n",
      "Group 157, Epoch 1: G loss: 0.31707649571555 vs. C loss: 0.22772769464386833\n",
      "Group 157, Epoch 2: G loss: 0.27040883302688595 vs. C loss: 0.1414695613914066\n",
      "Group 157, Epoch 3: G loss: 0.3289755621126719 vs. C loss: 0.11948976003461415\n",
      "Group 157, Epoch 4: G loss: 0.407294979265758 vs. C loss: 0.12769638664192626\n",
      "Group 157, Epoch 5: G loss: 0.41022401430777145 vs. C loss: 0.16406997500194442\n",
      "Group 157, Epoch 6: G loss: 0.25704524192426886 vs. C loss: 0.22065689663092294\n",
      "Group 158, Epoch 1: G loss: 0.2387579607112067 vs. C loss: 0.1656417664554384\n",
      "Group 158, Epoch 2: G loss: 0.3467447672571455 vs. C loss: 0.11473313305113052\n",
      "Group 158, Epoch 3: G loss: 0.7841121179716927 vs. C loss: 0.05535456434720092\n",
      "Group 158, Epoch 4: G loss: 0.5621238225272724 vs. C loss: 0.17701074729363123\n",
      "Group 158, Epoch 5: G loss: 0.18025030706609999 vs. C loss: 0.21635375503036713\n",
      "Group 158, Epoch 6: G loss: 0.012290538927274091 vs. C loss: 0.20558033511042595\n",
      "Group 159, Epoch 1: G loss: 0.3425248239721571 vs. C loss: 0.22265474332703483\n",
      "Group 159, Epoch 2: G loss: 0.4385742153440204 vs. C loss: 0.11997570428583355\n",
      "Group 159, Epoch 3: G loss: 0.450596159696579 vs. C loss: 0.14518621895048353\n",
      "Group 159, Epoch 4: G loss: 0.36085966442312506 vs. C loss: 0.15485508574379817\n",
      "Group 159, Epoch 5: G loss: 0.37064012416771475 vs. C loss: 0.218469032810794\n",
      "Group 159, Epoch 6: G loss: 0.11723701687795776 vs. C loss: 0.20089763071801928\n",
      "Group 160, Epoch 1: G loss: 0.5065328879015787 vs. C loss: 0.26622922884093386\n",
      "Group 160, Epoch 2: G loss: 0.45423227208001277 vs. C loss: 0.1185413371357653\n",
      "Group 160, Epoch 3: G loss: 0.49635012064661294 vs. C loss: 0.09040490951802997\n",
      "Group 160, Epoch 4: G loss: 0.6707250952720643 vs. C loss: 0.10362171878417333\n",
      "Group 160, Epoch 5: G loss: 0.255957194045186 vs. C loss: 0.22144653068648443\n",
      "Group 160, Epoch 6: G loss: 0.004397877259179951 vs. C loss: 0.20765292644500732\n",
      "Group 161, Epoch 1: G loss: 0.25207950132233753 vs. C loss: 0.16613907284206814\n",
      "Group 161, Epoch 2: G loss: 0.331975371497018 vs. C loss: 0.12522913101646635\n",
      "Group 161, Epoch 3: G loss: 0.35452385076454707 vs. C loss: 0.16103899644480812\n",
      "Group 161, Epoch 4: G loss: 0.4214496612548828 vs. C loss: 0.11597724010547004\n",
      "Group 161, Epoch 5: G loss: 0.5299367478915624 vs. C loss: 0.1049371709426244\n",
      "Group 161, Epoch 6: G loss: 0.5636737453086036 vs. C loss: 0.15552722621295187\n",
      "Group 161, Epoch 7: G loss: 0.22968523991959433 vs. C loss: 0.22790348695384133\n",
      "Group 162, Epoch 1: G loss: 0.2384739637374878 vs. C loss: 0.17648172544108498\n",
      "Group 162, Epoch 2: G loss: 0.3633476368018559 vs. C loss: 0.11891408926910825\n",
      "Group 162, Epoch 3: G loss: 0.2968518367835454 vs. C loss: 0.1637363607684771\n",
      "Group 162, Epoch 4: G loss: 0.2660937175154686 vs. C loss: 0.11131330579519272\n",
      "Group 162, Epoch 5: G loss: 0.7225014448165893 vs. C loss: 0.046694531001978457\n",
      "Group 162, Epoch 6: G loss: 0.42919049443943164 vs. C loss: 0.20798950725131568\n"
     ]
    }
   ],
   "source": [
    "inputs  = tfe.Variable(tf.random_normal(shape=[40]))\n",
    "\n",
    "Cs = []\n",
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.002)\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "class PrintCross(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('x', end='')\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, 50));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    \n",
    "    classifier_model = build_classifier_model()\n",
    "    extvar[\"classifier_model\"] = classifier_model;\n",
    "    \n",
    "    def loss_function_for_generative_model(y_true, y_pred):\n",
    "        #print(y_pred.shape); #(?, 20)\n",
    "        return construct_map_and_calc_loss(y_pred, extvar);\n",
    "    \n",
    "#     classifier_true_set_group = special_train_data[np.random.randint(0, special_train_data.shape[0], (500,))];\n",
    "    \n",
    "#     classifier_model.summary()\n",
    "    gmodel = generative_model(50, 40, loss_function_for_generative_model);\n",
    "#     gmodel.summary()\n",
    "    max_epoch = 30\n",
    "    good_epoch = 5\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        g_multiplier = 7;\n",
    "        c_multiplier = 3;\n",
    "        gnoise = np.random.random((50, 50));\n",
    "        glabel = np.ones((50,))\n",
    "\n",
    "        history = gmodel.fit(gnoise, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintDot()\n",
    "        \n",
    "        predicted_maps_data = gmodel.predict(np.random.random((10, 50)));\n",
    "        new_false_maps = construct_map_with_sliders(tf.convert_to_tensor(predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "        new_false_labels = np.zeros(10);\n",
    "        \n",
    "#         not_predicted_maps_data = np.random.random((10, 40));\n",
    "    #     new_false_maps2 = construct_map_without_sliders(tf.convert_to_tensor(not_predicted_maps_data, dtype=\"float32\")).numpy();\n",
    "    #     new_false_labels2 = np.zeros(10);\n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (50,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "    #     actual_train_data = special_train_data[st:se];\n",
    "    #     actual_train_labels = special_train_labels[st:se];\n",
    "\n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintCross()\n",
    "\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        res = gmodel.predict(plot_noise);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        if i >= good_epoch:\n",
    "            current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # it's the same anyways\n",
    "            res = gmodel.predict(np.random.random((1, 50)));\n",
    "#         print(construct_map_with_sliders(tf.convert_to_tensor(res), extvar=extvar).numpy().squeeze());\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "\n",
    "    onoise = np.random.random((1, 50));\n",
    "    \n",
    "    return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    for i in range(timestamps.shape[0] // 10):\n",
    "        z = generate_set(begin = i * 10, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    generate_set(begin = 30, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success! finished on: 2018-08-17 00:25:54.916286\n"
     ]
    }
   ],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@kotri_lv204 / ar3sgice, 2018/8/16"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "custom_training_002",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1RStBySCoGq8pIA0ya3mOFaYufAvyqvs8",
     "timestamp": 1533259533996
    },
    {
     "file_id": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/notebooks/custom_training.ipynb",
     "timestamp": 1533259167668
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
