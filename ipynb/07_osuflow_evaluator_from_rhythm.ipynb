{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2018/8/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "#### First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://ar3.moe/files/sophie.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "GAN is, kind of, hard to train; and I personally felt the pain when I downloaded 10+ github repos where no one worked. While all of those code are based on MNIST, they either do not train any image that looks good, or simply fail to run.\n",
    "\n",
    "As a consequence, I coded this notebook myself using tf.contrib.eager and tf.keras - took a while to find out how to modify that loss function.\n",
    "\n",
    "tf.contrib.eager is also said to be a pretty new feature in Tensorflow. ~~Idk how to write the other style of code using sessions without getting error anyways,~~ so make sure Tensorflow has the right version. My env is tensorflow v1.9.0. On Win10, python3.5, no cuda.\n",
    "\n",
    "UPDATE: updated tensorflow to 1.10.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "3LXMVuV0VhDr"
   },
   "source": [
    "Import the wheels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# this line below can only run once in the notebook! otherwise it will cause errors\n",
    "try:\n",
    "    tf.enable_eager_execution();\n",
    "except:\n",
    "    pass\n",
    "tfe = tf.contrib.eager;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some parameters related to GAN training.\n",
    "\n",
    "May or may not affect the result; haven't experimented a lot about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GAN_PARAMS = {\n",
    "    \"good_epoch\" : 6,\n",
    "    \"max_epoch\" : 25,\n",
    "    \"note_group_size\" : 10,\n",
    "    \"g_epochs\" : 7,\n",
    "    \"c_epochs\" : 3,\n",
    "    \"g_batch\" : 50,\n",
    "    \"g_input_size\" : 50,\n",
    "    \"c_true_batch\" : 50,\n",
    "    \"c_false_batch\" : 10\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "#### Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, unfiltered_sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "first_step_sv =          unfiltered_sv[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + 5: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "sv =          first_step_sv[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // 4;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 3, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "timestamps_after = timestamps_plus_1 - timestamps;\n",
    "timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "\n",
    "note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plotting functions found from stackoverflow. Probably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib.text as mtext\n",
    "\n",
    "\n",
    "class MyLine(lines.Line2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # we'll update the position when the line data is set\n",
    "        self.text = mtext.Text(0, 0, '')\n",
    "        lines.Line2D.__init__(self, *args, **kwargs)\n",
    "\n",
    "        # we can't access the label attr until *after* the line is\n",
    "        # inited\n",
    "        self.text.set_text(self.get_label())\n",
    "\n",
    "    def set_figure(self, figure):\n",
    "        self.text.set_figure(figure)\n",
    "        lines.Line2D.set_figure(self, figure)\n",
    "\n",
    "    def set_axes(self, axes):\n",
    "        self.text.set_axes(axes)\n",
    "        lines.Line2D.set_axes(self, axes)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        # 2 pixel offset\n",
    "        texttrans = transform + mtransforms.Affine2D().translate(2, 2)\n",
    "        self.text.set_transform(texttrans)\n",
    "        lines.Line2D.set_transform(self, transform)\n",
    "\n",
    "    def set_data(self, x, y):\n",
    "        if len(x):\n",
    "            self.text.set_position((x[-1], y[-1]))\n",
    "\n",
    "        lines.Line2D.set_data(self, x, y)\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        # draw my label at the end of the line with 2 pixel offset\n",
    "        lines.Line2D.draw(self, renderer)\n",
    "        self.text.draw(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import numpy as np;\n",
    "\n",
    "root = \"mapdata/\";\n",
    "\n",
    "chunk_size = GAN_PARAMS[\"note_group_size\"];\n",
    "step_size = 5;\n",
    "divisor = 4;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define some sort of plotting functions to show the generator and discriminator losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Limitless]')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# actual_train_data = np.concatenate((special_train_data[0:1], special_false_data[0:1]), axis=0);\n",
    "# actual_train_labels = np.concatenate((special_train_labels[0:1], special_false_labels[0:1]), axis=0);\n",
    "\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0.2)) * tf.square(0.3 - vg);\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 0.8)) * tf.square(vg - 0.7);\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0));\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 1));\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "    var_tensor = tf.to_float(var_tensor);\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_tensor.shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    batch_size = var_tensor.shape[0];\n",
    "    note_distances_now = length_multiplier * np.tile(np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "    note_angles_now = np.tile(np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "    \n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "    \n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.to_float(tf.greater(l, y_max / 2));\n",
    "    not_rerand = tf.to_float(tf.less_equal(l, y_max / 2));\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _px, _py = extvar[\"start_pos\"];\n",
    "    else:\n",
    "        _px = 256;\n",
    "        _py = 192;\n",
    "    _pa = 0;\n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = 256;\n",
    "    _y = 192;\n",
    "    for k in range(half_tensor):\n",
    "        note_index = begin_offset + k;\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l = tf.to_float(tf.less(_px, wall_l[:, k]));\n",
    "        wall_value_r = tf.to_float(tf.greater(_px, wall_r[:, k]));\n",
    "        wall_value_xmid = tf.to_float(tf.greater(_px, wall_l[:, k])) * tf.to_float(tf.less(_px, wall_r[:, k]));\n",
    "        wall_value_t = tf.to_float(tf.less(_py, wall_t[:, k]));\n",
    "        wall_value_b = tf.to_float(tf.greater(_py, wall_b[:, k]));\n",
    "        wall_value_ymid = tf.to_float(tf.greater(_py, wall_t[:, k])) * tf.to_float(tf.less(_py, wall_b[:, k]));\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        if is_slider[note_index]:\n",
    "            sln = slider_lengths[note_index];\n",
    "            slider_type = slider_types[note_index];\n",
    "            scos = slider_cos[slider_type];\n",
    "            ssin = slider_sin[slider_type];\n",
    "            _a = cos_list[:, k + half_tensor];\n",
    "            _b = sin_list[:, k + half_tensor];\n",
    "            # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "            # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "            _oa = _a * scos - _b * ssin;\n",
    "            _ob = _a * ssin + _b * scos;\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x + _a * sln;\n",
    "            _py = _y + _b * sln;\n",
    "        else:\n",
    "            _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "            _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x;\n",
    "            _py = _y;\n",
    "        _pa = tf.mod(_pa + 0, 6.283);\n",
    "    return tf.transpose(tf.stack(out, axis=0), [1, 0, 2]);\n",
    "\n",
    "def stack_loss(tensor):\n",
    "    complex_list = tf.complex(tensor[:, :, 0] * 512, tensor[:, :, 1] * 384);\n",
    "    stack_limit = 30;\n",
    "    precise_limit = 1;\n",
    "    a = [];\n",
    "    for k in range(tensor.shape[1]):\n",
    "        w = tf.tile(tf.expand_dims(complex_list[:, k], axis=1), [1, tensor.shape[1]]);\n",
    "        r = tf.abs(w - complex_list);\n",
    "        rless = tf.to_float(tf.less(r, stack_limit)) * tf.to_float(tf.greater(r, precise_limit));\n",
    "        rmean = tf.reduce_mean(rless * (stack_limit - r) / stack_limit);\n",
    "        a.append(rmean);\n",
    "    b = tf.reduce_sum(a);\n",
    "#         print(tf.tile(w, [1, tensor.shape[1]]));\n",
    "#         print(complex_list);\n",
    "    return b;\n",
    "\n",
    "# This polygon loss was an attempt to make the map less likely to overlap each other.\n",
    "# The idea is: calculate the area of polygon formed from the note positions;\n",
    "# If it is big, then it is good - they form a convex shape, no overlap.\n",
    "# ... of course it totally doesn't work like that.\n",
    "def polygon_loss(tensor):\n",
    "    tensor_this = tensor[:, :, 0:2];\n",
    "    tensor_next = tf.concat([tensor[:, 1:, 0:2], tensor[:, 0:1, 0:2]], axis=1);\n",
    "    sa = (tensor_this[:, :, 0] + tensor_next[:, :, 0]) * (tensor_next[:, :, 1] - tensor_this[:, :, 0]);\n",
    "    surface = tf.abs(tf.reduce_sum(sa, axis=1))/2;\n",
    "    return surface;\n",
    "\n",
    "def construct_map_and_calc_loss(var_tensor, extvar):\n",
    "    # first make a map from the outputs of generator, then ask the classifier (discriminator) to classify it\n",
    "    classifier_model = extvar[\"classifier_model\"]\n",
    "    out = construct_map_with_sliders(var_tensor, extvar=extvar);\n",
    "    cm = classifier_model(out);\n",
    "    predmean = 1 - tf.reduce_mean(cm, axis=1);\n",
    "#    regulator = tf.reduce_mean(tf.reduce_mean(- 0.1 * tf.square(out[:, :, 1:2] - 0.5), axis=2), axis=1); # * tf.square(out[:, :, 1:2] - 2)\n",
    "    box_loss = inblock_loss(out[:, :, 0:2]);\n",
    "    box_loss2 = inblock_loss(out[:, :, 4:6]);\n",
    "#     polygon = polygon_loss(out);\n",
    "    # print(out.shape); shape is (10, X, 4)\n",
    "    #return predmean + box_loss*100; <-- this was too harsh, in fact breaking SGD and creating waveforms on the borderline\n",
    "    return predmean + box_loss + box_loss2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "This will take some time. It splits map into groups of 10 (10 by default), and trains GAN to play with the flow of maps in the dataset. It will train for floor(note_count / 10) groups, for example, if you have 820 then it needs to train 82 groups.\n",
    "\n",
    "Some notes are converted to sliders, so it has less than the predicted count in #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "XdfkR223D9dW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0, Epoch 1: G loss: 0.3124592078583581 vs. C loss: 0.18397928608788386\n",
      "Group 0, Epoch 2: G loss: 0.36361129454204016 vs. C loss: 0.21571619974242318\n",
      "Group 0, Epoch 3: G loss: 0.03829958752862045 vs. C loss: 0.2046943162050512\n",
      "Group 0, Epoch 4: G loss: 0.02667006312736443 vs. C loss: 0.19951099819607207\n",
      "Group 0, Epoch 5: G loss: 0.03156765771231481 vs. C loss: 0.20081918024354514\n",
      "Group 0, Epoch 6: G loss: 0.041060418316296164 vs. C loss: 0.20495991657177606\n",
      "Group 1, Epoch 1: G loss: 0.34700260588100973 vs. C loss: 0.22328397962782118\n",
      "Group 1, Epoch 2: G loss: 0.356879910826683 vs. C loss: 0.15738209750917223\n",
      "Group 1, Epoch 3: G loss: 0.36731821894645694 vs. C loss: 0.13003153560890093\n",
      "Group 1, Epoch 4: G loss: 0.3895987732069833 vs. C loss: 0.11535892718368107\n",
      "Group 1, Epoch 5: G loss: 0.6012033326285225 vs. C loss: 0.21524506641758812\n",
      "Group 1, Epoch 6: G loss: 0.1807093647973878 vs. C loss: 0.22272109985351562\n",
      "Group 1, Epoch 7: G loss: 0.0725351774266788 vs. C loss: 0.20274608996179366\n",
      "Group 1, Epoch 8: G loss: 0.03407198398240975 vs. C loss: 0.20379451248380875\n",
      "Group 2, Epoch 1: G loss: 0.30933367865426203 vs. C loss: 0.20934242175685033\n",
      "Group 2, Epoch 2: G loss: 0.17780772681747165 vs. C loss: 0.20482590463426378\n",
      "Group 2, Epoch 3: G loss: 0.03207875836108412 vs. C loss: 0.20300699770450592\n",
      "Group 2, Epoch 4: G loss: 0.025265734802399364 vs. C loss: 0.19438499957323074\n",
      "Group 2, Epoch 5: G loss: 0.07555674048406737 vs. C loss: 0.19821301268206704\n",
      "Group 2, Epoch 6: G loss: 0.10873992102486744 vs. C loss: 0.1877208045787281\n",
      "Group 3, Epoch 1: G loss: 0.27855571721281325 vs. C loss: 0.22822181549337175\n",
      "Group 3, Epoch 2: G loss: 0.18815490198986873 vs. C loss: 0.17030493583944106\n",
      "Group 3, Epoch 3: G loss: 0.21310398174183712 vs. C loss: 0.21567758752240074\n",
      "Group 3, Epoch 4: G loss: 0.06997442053897039 vs. C loss: 0.21059234109189773\n",
      "Group 3, Epoch 5: G loss: 0.012126909395945922 vs. C loss: 0.20837438272105324\n",
      "Group 3, Epoch 6: G loss: 0.004378864634782076 vs. C loss: 0.20698066842224863\n",
      "Group 4, Epoch 1: G loss: 0.302359505210604 vs. C loss: 0.18579252892070344\n",
      "Group 4, Epoch 2: G loss: 0.34733771681785586 vs. C loss: 0.10396237588591047\n",
      "Group 4, Epoch 3: G loss: 0.4179574148995536 vs. C loss: 0.1493279089530309\n",
      "Group 4, Epoch 4: G loss: 0.1985241801611015 vs. C loss: 0.2292974872721566\n",
      "Group 4, Epoch 5: G loss: 0.03704197422734329 vs. C loss: 0.2082586089769999\n",
      "Group 4, Epoch 6: G loss: 0.03315763824752399 vs. C loss: 0.2007801334063212\n",
      "Group 5, Epoch 1: G loss: 0.5236439185483115 vs. C loss: 0.20377624283234277\n",
      "Group 5, Epoch 2: G loss: 0.3221390230315072 vs. C loss: 0.19410131209426454\n",
      "Group 5, Epoch 3: G loss: 0.09901608441557204 vs. C loss: 0.18135569161838952\n",
      "Group 5, Epoch 4: G loss: 0.2297004746539252 vs. C loss: 0.13410569644636577\n",
      "Group 5, Epoch 5: G loss: 0.5748264585222517 vs. C loss: 0.05678837249676386\n",
      "Group 5, Epoch 6: G loss: 0.7442708918026516 vs. C loss: 0.0667361749543084\n",
      "Group 6, Epoch 1: G loss: 0.33503249798502244 vs. C loss: 0.17580464979012808\n",
      "Group 6, Epoch 2: G loss: 0.3320003407342093 vs. C loss: 0.12764365474383038\n",
      "Group 6, Epoch 3: G loss: 0.3406967082193919 vs. C loss: 0.209995626575417\n",
      "Group 6, Epoch 4: G loss: 0.05126317398888724 vs. C loss: 0.20652011699146697\n",
      "Group 6, Epoch 5: G loss: 0.032133238602961814 vs. C loss: 0.20287153787083093\n",
      "Group 6, Epoch 6: G loss: 0.03794185082827295 vs. C loss: 0.18209812541802725\n",
      "Group 7, Epoch 1: G loss: 0.24962810277938838 vs. C loss: 0.19088682449526254\n",
      "Group 7, Epoch 2: G loss: 0.4047407354627337 vs. C loss: 0.18774521516429052\n",
      "Group 7, Epoch 3: G loss: 0.2127536075455802 vs. C loss: 0.17797186308436921\n",
      "Group 7, Epoch 4: G loss: 0.26496069388730187 vs. C loss: 0.11045507755544452\n",
      "Group 7, Epoch 5: G loss: 0.698160151072911 vs. C loss: 0.056281476798984736\n",
      "Group 7, Epoch 6: G loss: 0.5873832949570247 vs. C loss: 0.053024097035328545\n",
      "Group 7, Epoch 7: G loss: 0.466927647803511 vs. C loss: 0.2317224525743061\n",
      "Group 7, Epoch 8: G loss: 0.05308144784399441 vs. C loss: 0.20755648530191848\n",
      "Group 7, Epoch 9: G loss: 0.03815716153809002 vs. C loss: 0.2078926447365019\n",
      "Group 8, Epoch 1: G loss: 0.35474777902875626 vs. C loss: 0.18533175686995187\n",
      "Group 8, Epoch 2: G loss: 0.3726565488747187 vs. C loss: 0.16734414630466035\n",
      "Group 8, Epoch 3: G loss: 0.19783066489866802 vs. C loss: 0.21176043152809143\n",
      "Group 8, Epoch 4: G loss: 0.0594193982226508 vs. C loss: 0.20878452890449098\n",
      "Group 8, Epoch 5: G loss: 0.042987669897930965 vs. C loss: 0.20442484815915427\n",
      "Group 8, Epoch 6: G loss: 0.022176841433559146 vs. C loss: 0.20430936581558648\n",
      "Group 9, Epoch 1: G loss: 0.2927873373031616 vs. C loss: 0.16662085966931448\n",
      "Group 9, Epoch 2: G loss: 0.449198466539383 vs. C loss: 0.11156391269630855\n",
      "Group 9, Epoch 3: G loss: 0.48736467276300705 vs. C loss: 0.1539397413531939\n",
      "Group 9, Epoch 4: G loss: 0.3973710387945175 vs. C loss: 0.16911200682322183\n",
      "Group 9, Epoch 5: G loss: 0.3435742146202497 vs. C loss: 0.21298446589046058\n",
      "Group 9, Epoch 6: G loss: 0.42347084368978233 vs. C loss: 0.1821260940697458\n",
      "Group 10, Epoch 1: G loss: 0.2376676653112684 vs. C loss: 0.1890020188358095\n",
      "Group 10, Epoch 2: G loss: 0.27285507789679936 vs. C loss: 0.16503529995679855\n",
      "Group 10, Epoch 3: G loss: 0.44728185875075205 vs. C loss: 0.11499230770601167\n",
      "Group 10, Epoch 4: G loss: 0.6774019011429379 vs. C loss: 0.1358228512108326\n",
      "Group 10, Epoch 5: G loss: 0.4588080082620893 vs. C loss: 0.17210990687211355\n",
      "Group 10, Epoch 6: G loss: 0.15605592748948507 vs. C loss: 0.21063245832920074\n",
      "Group 10, Epoch 7: G loss: 0.1398134878703526 vs. C loss: 0.2094103139307764\n",
      "Group 10, Epoch 8: G loss: 0.08054512036698205 vs. C loss: 0.19836167825592887\n",
      "Group 10, Epoch 9: G loss: 0.12403338551521301 vs. C loss: 0.16319729553328619\n",
      "Group 11, Epoch 1: G loss: 0.26027411477906365 vs. C loss: 0.23565101706319388\n",
      "Group 11, Epoch 2: G loss: 0.20760906083243233 vs. C loss: 0.12583106011152267\n",
      "Group 11, Epoch 3: G loss: 0.7252018877438137 vs. C loss: 0.04438183498051431\n",
      "Group 11, Epoch 4: G loss: 0.307480603562934 vs. C loss: 0.22770425015025672\n",
      "Group 11, Epoch 5: G loss: 0.02239003926515579 vs. C loss: 0.20853002038266924\n",
      "Group 11, Epoch 6: G loss: 0.015074088743754793 vs. C loss: 0.20821558932463327\n",
      "Group 12, Epoch 1: G loss: 0.2704096053327833 vs. C loss: 0.1803999791542689\n",
      "Group 12, Epoch 2: G loss: 0.32802132027489794 vs. C loss: 0.10469316939512889\n",
      "Group 12, Epoch 3: G loss: 0.6322865690503802 vs. C loss: 0.16205144425233206\n",
      "Group 12, Epoch 4: G loss: 0.2085717554603304 vs. C loss: 0.22022644678751627\n",
      "Group 12, Epoch 5: G loss: 0.04863245061465672 vs. C loss: 0.20533260371949935\n",
      "Group 12, Epoch 6: G loss: 0.02809035144746303 vs. C loss: 0.20056657824251387\n",
      "Group 13, Epoch 1: G loss: 0.38734812906810223 vs. C loss: 0.19158166067467797\n",
      "Group 13, Epoch 2: G loss: 0.28299959216799053 vs. C loss: 0.20222780108451843\n",
      "Group 13, Epoch 3: G loss: 0.08338994405099323 vs. C loss: 0.19563148253493837\n",
      "Group 13, Epoch 4: G loss: 0.0795865526156766 vs. C loss: 0.17912859635220635\n",
      "Group 13, Epoch 5: G loss: 0.22170489217553818 vs. C loss: 0.15313530796104005\n",
      "Group 13, Epoch 6: G loss: 0.3583386966160365 vs. C loss: 0.16591553721163008\n",
      "Group 13, Epoch 7: G loss: 0.26925145983695986 vs. C loss: 0.18876932892534468\n",
      "Group 13, Epoch 8: G loss: 0.1895129165479115 vs. C loss: 0.20876368714703455\n",
      "Group 13, Epoch 9: G loss: 0.0641701446047851 vs. C loss: 0.20375075936317444\n",
      "Group 14, Epoch 1: G loss: 0.28979086918490277 vs. C loss: 0.18334356943766275\n",
      "Group 14, Epoch 2: G loss: 0.3609475033623832 vs. C loss: 0.1226118016574118\n",
      "Group 14, Epoch 3: G loss: 0.6677188396453858 vs. C loss: 0.07609344936079449\n",
      "Group 14, Epoch 4: G loss: 0.37053357873644144 vs. C loss: 0.128448360082176\n",
      "Group 14, Epoch 5: G loss: 0.3864917546510696 vs. C loss: 0.195835593673918\n",
      "Group 14, Epoch 6: G loss: 0.19676053098269872 vs. C loss: 0.17829172954791125\n",
      "Group 14, Epoch 7: G loss: 0.4688073166779109 vs. C loss: 0.075028029580911\n",
      "Group 14, Epoch 8: G loss: 0.8304278799465725 vs. C loss: 0.019187182498474915\n",
      "Group 14, Epoch 9: G loss: 0.9557952966008869 vs. C loss: 0.01726501564391785\n",
      "Group 15, Epoch 1: G loss: 0.451524327482496 vs. C loss: 0.24429607391357422\n",
      "Group 15, Epoch 2: G loss: 0.21294439690453665 vs. C loss: 0.14049528042475384\n",
      "Group 15, Epoch 3: G loss: 0.642406495979854 vs. C loss: 0.06889811903238298\n",
      "Group 15, Epoch 4: G loss: 0.38084086562905994 vs. C loss: 0.15067120641469955\n",
      "Group 15, Epoch 5: G loss: 0.3090687509093966 vs. C loss: 0.24223589731587306\n",
      "Group 15, Epoch 6: G loss: 0.033207595295139725 vs. C loss: 0.20563512212700316\n",
      "Group 15, Epoch 7: G loss: 0.03145545167582376 vs. C loss: 0.1914717803398768\n",
      "Group 15, Epoch 8: G loss: 0.29126701184681486 vs. C loss: 0.08835976819197337\n",
      "Group 15, Epoch 9: G loss: 0.8015763384955271 vs. C loss: 0.020970463752746582\n",
      "Group 15, Epoch 10: G loss: 0.8344204817499433 vs. C loss: 0.05004570291688045\n",
      "Group 15, Epoch 11: G loss: 0.8236407829182488 vs. C loss: 0.18974429865678152\n",
      "Group 16, Epoch 1: G loss: 0.2244542807340622 vs. C loss: 0.19118724680609175\n",
      "Group 16, Epoch 2: G loss: 0.2246789893933705 vs. C loss: 0.13937336620357302\n",
      "Group 16, Epoch 3: G loss: 0.4317580231598446 vs. C loss: 0.10716356254286236\n",
      "Group 16, Epoch 4: G loss: 0.4474642549242292 vs. C loss: 0.1135514502724012\n",
      "Group 16, Epoch 5: G loss: 0.6394881674221585 vs. C loss: 0.04495007544755936\n",
      "Group 16, Epoch 6: G loss: 0.6985837118966238 vs. C loss: 0.0826177087922891\n",
      "Group 16, Epoch 7: G loss: 0.833189446585519 vs. C loss: 0.04039526482423147\n",
      "Group 16, Epoch 8: G loss: 0.6281768185751779 vs. C loss: 0.16000735863215393\n",
      "Group 16, Epoch 9: G loss: 0.24823810585907524 vs. C loss: 0.12964772060513496\n",
      "Group 16, Epoch 10: G loss: 0.6294228555900709 vs. C loss: 0.2358555131488376\n",
      "Group 16, Epoch 11: G loss: 0.04379389190248081 vs. C loss: 0.20901378326945833\n",
      "Group 16, Epoch 12: G loss: 0.026033698075584002 vs. C loss: 0.2099500430954827\n",
      "Group 16, Epoch 13: G loss: 0.02154255849974496 vs. C loss: 0.20818562143378785\n",
      "Group 16, Epoch 14: G loss: 0.01917537035686629 vs. C loss: 0.20838019086254966\n",
      "Group 16, Epoch 15: G loss: 0.01922496200672218 vs. C loss: 0.20822722382015654\n",
      "Group 16, Epoch 16: G loss: 0.019227305799722674 vs. C loss: 0.20820865945683587\n",
      "Group 16, Epoch 17: G loss: 0.019292791347418514 vs. C loss: 0.20817528499497306\n",
      "Group 16, Epoch 18: G loss: 0.019395811589700836 vs. C loss: 0.20815867020024192\n",
      "Group 16, Epoch 19: G loss: 0.019601069603647502 vs. C loss: 0.20809451738993326\n",
      "Group 16, Epoch 20: G loss: 0.019669845380953378 vs. C loss: 0.2080189660191536\n",
      "Group 16, Epoch 21: G loss: 0.02055220428322043 vs. C loss: 0.20783733493751952\n",
      "Group 16, Epoch 22: G loss: 0.021665492973157337 vs. C loss: 0.20750824734568596\n",
      "Group 16, Epoch 23: G loss: 0.023575639139328686 vs. C loss: 0.20580003162225088\n",
      "Group 16, Epoch 24: G loss: 0.036114583696637836 vs. C loss: 0.19917685621314576\n",
      "Group 16, Epoch 25: G loss: 0.08291298544832637 vs. C loss: 0.19336876273155212\n",
      "Group 17, Epoch 1: G loss: 0.30618918112346105 vs. C loss: 0.17712774748603502\n",
      "Group 17, Epoch 2: G loss: 0.1876287430524826 vs. C loss: 0.19847456531392205\n",
      "Group 17, Epoch 3: G loss: 0.08033269015806062 vs. C loss: 0.1941844754748874\n",
      "Group 17, Epoch 4: G loss: 0.10834181500332696 vs. C loss: 0.19292477849457002\n",
      "Group 17, Epoch 5: G loss: 0.16425403313977377 vs. C loss: 0.1552640762594011\n",
      "Group 17, Epoch 6: G loss: 0.3237726645810263 vs. C loss: 0.11548259026474422\n",
      "Group 17, Epoch 7: G loss: 0.3728375600916998 vs. C loss: 0.21113961024416816\n",
      "Group 18, Epoch 1: G loss: 0.4998261400631496 vs. C loss: 0.22951377100414697\n",
      "Group 18, Epoch 2: G loss: 0.3229930775506156 vs. C loss: 0.15780935560663542\n",
      "Group 18, Epoch 3: G loss: 0.381251779624394 vs. C loss: 0.10703087349732716\n",
      "Group 18, Epoch 4: G loss: 0.3277491765362876 vs. C loss: 0.18812981247901917\n",
      "Group 18, Epoch 5: G loss: 0.11138268581458499 vs. C loss: 0.20427544083860186\n",
      "Group 18, Epoch 6: G loss: 0.05217781173331397 vs. C loss: 0.1984617163737615\n",
      "Group 19, Epoch 1: G loss: 0.2324714596782412 vs. C loss: 0.1562193673517969\n",
      "Group 19, Epoch 2: G loss: 0.32080176600388116 vs. C loss: 0.1718419310119417\n",
      "Group 19, Epoch 3: G loss: 0.43824635573795867 vs. C loss: 0.0774273748199145\n",
      "Group 19, Epoch 4: G loss: 0.7085926430565971 vs. C loss: 0.038847561925649636\n",
      "Group 19, Epoch 5: G loss: 0.6111345410346984 vs. C loss: 0.05232632905244827\n",
      "Group 19, Epoch 6: G loss: 0.7878246716090611 vs. C loss: 0.037327392440703176\n",
      "Group 20, Epoch 1: G loss: 0.4311857453414372 vs. C loss: 0.21061602731545767\n",
      "Group 20, Epoch 2: G loss: 0.27446758661951337 vs. C loss: 0.15147854139407477\n",
      "Group 20, Epoch 3: G loss: 0.4155760083879743 vs. C loss: 0.13991126169761023\n",
      "Group 20, Epoch 4: G loss: 0.24077340939215253 vs. C loss: 0.2105471847785844\n",
      "Group 20, Epoch 5: G loss: 0.06279515187655176 vs. C loss: 0.1790543264812893\n",
      "Group 20, Epoch 6: G loss: 0.22650136947631835 vs. C loss: 0.14693793406089148\n",
      "Group 20, Epoch 7: G loss: 0.38702061985220226 vs. C loss: 0.1807001887096299\n",
      "Group 20, Epoch 8: G loss: 0.21859878386769974 vs. C loss: 0.18748081309927833\n",
      "Group 20, Epoch 9: G loss: 0.29538629693644386 vs. C loss: 0.17306746542453766\n",
      "Group 20, Epoch 10: G loss: 0.29596276964460105 vs. C loss: 0.14632433487309351\n",
      "Group 20, Epoch 11: G loss: 0.2947084807923862 vs. C loss: 0.214019184311231\n",
      "Group 20, Epoch 12: G loss: 0.03277893476188183 vs. C loss: 0.20598908762137094\n",
      "Group 20, Epoch 13: G loss: 0.024505184165069034 vs. C loss: 0.20641633537080553\n",
      "Group 20, Epoch 14: G loss: 0.019052389955946377 vs. C loss: 0.20564445190959504\n",
      "Group 20, Epoch 15: G loss: 0.013459435531071253 vs. C loss: 0.20284461643960738\n",
      "Group 21, Epoch 1: G loss: 0.39472566587584357 vs. C loss: 0.2068388437231382\n",
      "Group 21, Epoch 2: G loss: 0.30836590954235626 vs. C loss: 0.16382702853944567\n",
      "Group 21, Epoch 3: G loss: 0.21339769001517975 vs. C loss: 0.1772715432776345\n",
      "Group 21, Epoch 4: G loss: 0.2247825169137546 vs. C loss: 0.1790651968783802\n",
      "Group 21, Epoch 5: G loss: 0.28222116423504695 vs. C loss: 0.23662460181448194\n",
      "Group 21, Epoch 6: G loss: 0.06715433427265713 vs. C loss: 0.20944062372048697\n",
      "Group 22, Epoch 1: G loss: 0.4562206293855394 vs. C loss: 0.2219120396508111\n",
      "Group 22, Epoch 2: G loss: 0.36197716167994903 vs. C loss: 0.1191140678193834\n",
      "Group 22, Epoch 3: G loss: 0.7578103508268085 vs. C loss: 0.038150709950261645\n",
      "Group 22, Epoch 4: G loss: 0.8651068993977138 vs. C loss: 0.018132757498986192\n",
      "Group 22, Epoch 5: G loss: 0.9585060204778398 vs. C loss: 0.04598471687899695\n",
      "Group 22, Epoch 6: G loss: 0.5518496344132083 vs. C loss: 0.21451535324255624\n",
      "Group 23, Epoch 1: G loss: 0.3581814433847155 vs. C loss: 0.23058467689487672\n",
      "Group 23, Epoch 2: G loss: 0.41226736818041115 vs. C loss: 0.14436489840348563\n",
      "Group 23, Epoch 3: G loss: 0.3508841429437909 vs. C loss: 0.12525037262174818\n"
     ]
    }
   ],
   "source": [
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer, add the start pos\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.002)\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "class PrintCross(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('x', end='')\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, GAN_PARAMS[\"g_input_size\"]));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    \n",
    "    classifier_model = build_classifier_model()\n",
    "    extvar[\"classifier_model\"] = classifier_model;\n",
    "    \n",
    "    def loss_function_for_generative_model(y_true, y_pred):\n",
    "        return construct_map_and_calc_loss(y_pred, extvar);\n",
    "    \n",
    "#     classifier_true_set_group = special_train_data[np.random.randint(0, special_train_data.shape[0], (500,))];\n",
    "    \n",
    "#     classifier_model.summary()\n",
    "#     gmodel.summary()\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    max_epoch = GAN_PARAMS[\"max_epoch\"];\n",
    "    good_epoch = GAN_PARAMS[\"good_epoch\"] - 1;\n",
    "    g_multiplier = GAN_PARAMS[\"g_epochs\"];\n",
    "    c_multiplier = GAN_PARAMS[\"c_epochs\"];\n",
    "    g_batch = GAN_PARAMS[\"g_batch\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    c_true_batch = GAN_PARAMS[\"c_true_batch\"]\n",
    "    c_false_batch = GAN_PARAMS[\"c_false_batch\"]\n",
    "    \n",
    "    gmodel = generative_model(g_input_size, note_group_size * 4, loss_function_for_generative_model);\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        gnoise = np.random.random((g_batch, g_input_size));\n",
    "        glabel = np.ones((g_batch,))\n",
    "\n",
    "        history = gmodel.fit(gnoise, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintDot()\n",
    "        \n",
    "        predicted_maps_data = gmodel.predict(np.random.random((c_false_batch, g_input_size)));\n",
    "        new_false_maps = construct_map_with_sliders(tf.convert_to_tensor(predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "        new_false_labels = np.zeros(c_false_batch);\n",
    "        \n",
    "#         not_predicted_maps_data = np.random.random((10, 40));\n",
    "    #     new_false_maps2 = construct_map_with_sliders(tf.convert_to_tensor(not_predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "    #     new_false_labels2 = np.zeros(10);\n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (c_true_batch,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "    #     actual_train_data = special_train_data[st:se];\n",
    "    #     actual_train_labels = special_train_labels[st:se];\n",
    "\n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintCross()\n",
    "\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        res = gmodel.predict(plot_noise);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        if i >= good_epoch:\n",
    "            current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # it's the same anyways\n",
    "            res = gmodel.predict(np.random.random((1, g_input_size)));\n",
    "#         print(construct_map_with_sliders(tf.convert_to_tensor(res), extvar=extvar).numpy().squeeze());\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "\n",
    "    onoise = np.random.random((1, g_input_size));\n",
    "    \n",
    "    return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    for i in range(timestamps.shape[0] // note_group_size):\n",
    "        z = generate_set(begin = i * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    generate_set(begin = 3 * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@kotri_lv204 / ar3sgice, 2018/8/16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
