{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2018/8/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://ar3.moe/files/sophie.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "GAN is, kind of, hard to train; and I personally felt the pain when I downloaded 10+ github repos where no one worked. While all of those code are based on MNIST, they either do not train any image that looks good, or simply fail to run.\n",
    "\n",
    "As a consequence, I coded this notebook myself using tf.contrib.eager and tf.keras - took a while to find out how to modify that loss function.\n",
    "\n",
    "tf.contrib.eager is also said to be a pretty new feature in Tensorflow. ~~Idk how to write the other style of code using sessions without getting error anyways,~~ so make sure Tensorflow has the right version. My env is tensorflow v1.9.0. On Win10, python3.5, no cuda.\n",
    "\n",
    "UPDATE: updated tensorflow to 1.10.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "3LXMVuV0VhDr"
   },
   "source": [
    "Import the wheels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# this line below can only run once in the notebook! otherwise it will cause errors\n",
    "tf.enable_eager_execution();\n",
    "tfe = tf.contrib.eager;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some parameters related to GAN training.\n",
    "\n",
    "May or may not affect the result; haven't experimented a lot about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GAN_PARAMS = {\n",
    "    \"good_epoch\" : 6,\n",
    "    \"max_epoch\" : 25,\n",
    "    \"note_group_size\" : 10,\n",
    "    \"g_epochs\" : 7,\n",
    "    \"c_epochs\" : 3,\n",
    "    \"g_batch\" : 50,\n",
    "    \"g_input_size\" : 50,\n",
    "    \"c_true_batch\" : 50,\n",
    "    \"c_false_batch\" : 10\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, unfiltered_sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "first_step_sv =          unfiltered_sv[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + 5: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "sv =          first_step_sv[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // 4;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 3, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "timestamps_after = timestamps_plus_1 - timestamps;\n",
    "timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "\n",
    "note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plotting functions found from stackoverflow. Probably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib.text as mtext\n",
    "\n",
    "\n",
    "class MyLine(lines.Line2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # we'll update the position when the line data is set\n",
    "        self.text = mtext.Text(0, 0, '')\n",
    "        lines.Line2D.__init__(self, *args, **kwargs)\n",
    "\n",
    "        # we can't access the label attr until *after* the line is\n",
    "        # inited\n",
    "        self.text.set_text(self.get_label())\n",
    "\n",
    "    def set_figure(self, figure):\n",
    "        self.text.set_figure(figure)\n",
    "        lines.Line2D.set_figure(self, figure)\n",
    "\n",
    "    def set_axes(self, axes):\n",
    "        self.text.set_axes(axes)\n",
    "        lines.Line2D.set_axes(self, axes)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        # 2 pixel offset\n",
    "        texttrans = transform + mtransforms.Affine2D().translate(2, 2)\n",
    "        self.text.set_transform(texttrans)\n",
    "        lines.Line2D.set_transform(self, transform)\n",
    "\n",
    "    def set_data(self, x, y):\n",
    "        if len(x):\n",
    "            self.text.set_position((x[-1], y[-1]))\n",
    "\n",
    "        lines.Line2D.set_data(self, x, y)\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        # draw my label at the end of the line with 2 pixel offset\n",
    "        lines.Line2D.draw(self, renderer)\n",
    "        self.text.draw(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import numpy as np;\n",
    "\n",
    "root = \"mapdata/\";\n",
    "\n",
    "chunk_size = GAN_PARAMS[\"note_group_size\"];\n",
    "step_size = 5;\n",
    "divisor = 4;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define some sort of plotting functions to show the generator and discriminator losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Limitless]')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# actual_train_data = np.concatenate((special_train_data[0:1], special_false_data[0:1]), axis=0);\n",
    "# actual_train_labels = np.concatenate((special_train_labels[0:1], special_false_labels[0:1]), axis=0);\n",
    "\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0.2)) * tf.square(0.3 - vg);\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 0.8)) * tf.square(vg - 0.7);\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0));\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 1));\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "    var_tensor = tf.to_float(var_tensor);\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_tensor.shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    batch_size = var_tensor.shape[0];\n",
    "    note_distances_now = length_multiplier * np.tile(np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "    note_angles_now = np.tile(np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "    \n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "    \n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.to_float(tf.greater(l, y_max / 2));\n",
    "    not_rerand = tf.to_float(tf.less_equal(l, y_max / 2));\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _px, _py = extvar[\"start_pos\"];\n",
    "    else:\n",
    "        _px = 256;\n",
    "        _py = 192;\n",
    "    _pa = 0;\n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = 256;\n",
    "    _y = 192;\n",
    "    for k in range(half_tensor):\n",
    "        note_index = begin_offset + k;\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l = tf.to_float(tf.less(_px, wall_l[:, k]));\n",
    "        wall_value_r = tf.to_float(tf.greater(_px, wall_r[:, k]));\n",
    "        wall_value_xmid = tf.to_float(tf.greater(_px, wall_l[:, k])) * tf.to_float(tf.less(_px, wall_r[:, k]));\n",
    "        wall_value_t = tf.to_float(tf.less(_py, wall_t[:, k]));\n",
    "        wall_value_b = tf.to_float(tf.greater(_py, wall_b[:, k]));\n",
    "        wall_value_ymid = tf.to_float(tf.greater(_py, wall_t[:, k])) * tf.to_float(tf.less(_py, wall_b[:, k]));\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        if is_slider[note_index]:\n",
    "            sln = slider_lengths[note_index];\n",
    "            slider_type = slider_types[note_index];\n",
    "            scos = slider_cos[slider_type];\n",
    "            ssin = slider_sin[slider_type];\n",
    "            _a = cos_list[:, k + half_tensor];\n",
    "            _b = sin_list[:, k + half_tensor];\n",
    "            # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "            # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "            _oa = _a * scos - _b * ssin;\n",
    "            _ob = _a * ssin + _b * scos;\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x + _a * sln;\n",
    "            _py = _y + _b * sln;\n",
    "        else:\n",
    "            _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "            _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x;\n",
    "            _py = _y;\n",
    "        _pa = tf.mod(_pa + 0, 6.283);\n",
    "    return tf.transpose(tf.stack(out, axis=0), [1, 0, 2]);\n",
    "\n",
    "def stack_loss(tensor):\n",
    "    complex_list = tf.complex(tensor[:, :, 0] * 512, tensor[:, :, 1] * 384);\n",
    "    stack_limit = 30;\n",
    "    precise_limit = 1;\n",
    "    a = [];\n",
    "    for k in range(tensor.shape[1]):\n",
    "        w = tf.tile(tf.expand_dims(complex_list[:, k], axis=1), [1, tensor.shape[1]]);\n",
    "        r = tf.abs(w - complex_list);\n",
    "        rless = tf.to_float(tf.less(r, stack_limit)) * tf.to_float(tf.greater(r, precise_limit));\n",
    "        rmean = tf.reduce_mean(rless * (stack_limit - r) / stack_limit);\n",
    "        a.append(rmean);\n",
    "    b = tf.reduce_sum(a);\n",
    "#         print(tf.tile(w, [1, tensor.shape[1]]));\n",
    "#         print(complex_list);\n",
    "    return b;\n",
    "\n",
    "# This polygon loss was an attempt to make the map less likely to overlap each other.\n",
    "# The idea is: calculate the area of polygon formed from the note positions;\n",
    "# If it is big, then it is good - they form a convex shape, no overlap.\n",
    "# ... of course it totally doesn't work like that.\n",
    "def polygon_loss(tensor):\n",
    "    tensor_this = tensor[:, :, 0:2];\n",
    "    tensor_next = tf.concat([tensor[:, 1:, 0:2], tensor[:, 0:1, 0:2]], axis=1);\n",
    "    sa = (tensor_this[:, :, 0] + tensor_next[:, :, 0]) * (tensor_next[:, :, 1] - tensor_this[:, :, 0]);\n",
    "    surface = tf.abs(tf.reduce_sum(sa, axis=1))/2;\n",
    "    return surface;\n",
    "\n",
    "def construct_map_and_calc_loss(var_tensor, extvar):\n",
    "    # first make a map from the outputs of generator, then ask the classifier (discriminator) to classify it\n",
    "    classifier_model = extvar[\"classifier_model\"]\n",
    "    out = construct_map_with_sliders(var_tensor, extvar=extvar);\n",
    "    cm = classifier_model(out);\n",
    "    predmean = 1 - tf.reduce_mean(cm, axis=1);\n",
    "#    regulator = tf.reduce_mean(tf.reduce_mean(- 0.1 * tf.square(out[:, :, 1:2] - 0.5), axis=2), axis=1); # * tf.square(out[:, :, 1:2] - 2)\n",
    "    box_loss = inblock_loss(out[:, :, 0:2]);\n",
    "    box_loss2 = inblock_loss(out[:, :, 4:6]);\n",
    "#     polygon = polygon_loss(out);\n",
    "    # print(out.shape); shape is (10, X, 4)\n",
    "    #return predmean + box_loss*100; <-- this was too harsh, in fact breaking SGD and creating waveforms on the borderline\n",
    "    return predmean + box_loss + box_loss2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "TODO: add more info here\n",
    "\n",
    "TODO: strip variables group_note_count, good_maps_each_epoch, bad_maps_each_epoch, epoch_g, epoch_c, epoch_both_good, epoch_both_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "XdfkR223D9dW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0, Epoch 1: G loss: 0.36413068260465353 vs. C loss: 0.18609274758232966\n",
      "Group 0, Epoch 2: G loss: 0.22919845581054685 vs. C loss: 0.15591300858391657\n",
      "Group 0, Epoch 3: G loss: 0.33102192538125175 vs. C loss: 0.18876740253633925\n",
      "Group 0, Epoch 4: G loss: 0.2697403394750187 vs. C loss: 0.21144216342104805\n",
      "Group 0, Epoch 5: G loss: 0.08168096425277847 vs. C loss: 0.20060642643107307\n",
      "Group 0, Epoch 6: G loss: 0.04881691634654999 vs. C loss: 0.1955159306526184\n",
      "Group 1, Epoch 1: G loss: 0.37708807672773087 vs. C loss: 0.20029182318184113\n",
      "Group 1, Epoch 2: G loss: 0.34723188791956217 vs. C loss: 0.13342115324404505\n",
      "Group 1, Epoch 3: G loss: 0.681088309628623 vs. C loss: 0.045030515640974045\n",
      "Group 1, Epoch 4: G loss: 0.6366004262651718 vs. C loss: 0.11832276441984706\n",
      "Group 1, Epoch 5: G loss: 0.7912474240575519 vs. C loss: 0.07329841454823811\n",
      "Group 1, Epoch 6: G loss: 0.37370763836162435 vs. C loss: 0.22580249276426104\n",
      "Group 1, Epoch 7: G loss: 0.0219095557395901 vs. C loss: 0.20714540448453692\n",
      "Group 2, Epoch 1: G loss: 0.42239894866943356 vs. C loss: 0.25720742179287803\n",
      "Group 2, Epoch 2: G loss: 0.3137503440890993 vs. C loss: 0.17963318361176384\n",
      "Group 2, Epoch 3: G loss: 0.2661952342305865 vs. C loss: 0.1382829464144177\n",
      "Group 2, Epoch 4: G loss: 0.47708483082907543 vs. C loss: 0.20267084447873965\n",
      "Group 2, Epoch 5: G loss: 0.0908699501838003 vs. C loss: 0.20928069204092026\n",
      "Group 2, Epoch 6: G loss: 0.017273435182869435 vs. C loss: 0.20552071597841048\n",
      "Group 3, Epoch 1: G loss: 0.4249957067625863 vs. C loss: 0.22115447951687708\n",
      "Group 3, Epoch 2: G loss: 0.1442178093961307 vs. C loss: 0.19095998588535523\n",
      "Group 3, Epoch 3: G loss: 0.0794860942023141 vs. C loss: 0.17951725588904485\n",
      "Group 3, Epoch 4: G loss: 0.1851164025919778 vs. C loss: 0.16594909297095406\n",
      "Group 3, Epoch 5: G loss: 0.24442478418350216 vs. C loss: 0.17279703749550712\n",
      "Group 3, Epoch 6: G loss: 0.23558100461959838 vs. C loss: 0.20694185131125983\n",
      "Group 4, Epoch 1: G loss: 0.3832848991666521 vs. C loss: 0.21120763818422952\n",
      "Group 4, Epoch 2: G loss: 0.3014239149434226 vs. C loss: 0.15705527365207672\n",
      "Group 4, Epoch 3: G loss: 0.27123266075338637 vs. C loss: 0.176812659121222\n",
      "Group 4, Epoch 4: G loss: 0.15088611330304827 vs. C loss: 0.21719412091705534\n",
      "Group 4, Epoch 5: G loss: 0.06126384937337466 vs. C loss: 0.20208310087521872\n",
      "Group 4, Epoch 6: G loss: 0.04396031317966325 vs. C loss: 0.18228096597724489\n",
      "Group 5, Epoch 1: G loss: 0.2217615067958832 vs. C loss: 0.2470551646418042\n",
      "Group 5, Epoch 2: G loss: 0.20156680643558503 vs. C loss: 0.18119517134295568\n",
      "Group 5, Epoch 3: G loss: 0.22114757299423218 vs. C loss: 0.1755543483628167\n",
      "Group 5, Epoch 4: G loss: 0.24389033530439647 vs. C loss: 0.2103476342227724\n",
      "Group 5, Epoch 5: G loss: 0.0934242361358234 vs. C loss: 0.18991821010907492\n",
      "Group 5, Epoch 6: G loss: 0.13817120492458343 vs. C loss: 0.15992022222942773\n",
      "Group 6, Epoch 1: G loss: 0.24849621568407332 vs. C loss: 0.1553170308470726\n",
      "Group 6, Epoch 2: G loss: 0.5507036319800785 vs. C loss: 0.0834622501085202\n",
      "Group 6, Epoch 3: G loss: 0.5577868103981019 vs. C loss: 0.049517926656537585\n",
      "Group 6, Epoch 4: G loss: 0.4943982394678252 vs. C loss: 0.14032777316040462\n",
      "Group 6, Epoch 5: G loss: 0.3451376810669899 vs. C loss: 0.18692691872517267\n",
      "Group 6, Epoch 6: G loss: 0.42062276303768165 vs. C loss: 0.09334546224110657\n",
      "Group 7, Epoch 1: G loss: 0.21655412444046565 vs. C loss: 0.16199749377038744\n",
      "Group 7, Epoch 2: G loss: 0.4504711704594749 vs. C loss: 0.12632560398843554\n",
      "Group 7, Epoch 3: G loss: 0.407149944135121 vs. C loss: 0.17217279887861672\n",
      "Group 7, Epoch 4: G loss: 0.4088353020804269 vs. C loss: 0.09337705249587695\n",
      "Group 7, Epoch 5: G loss: 0.6784133740833828 vs. C loss: 0.09636920959585243\n",
      "Group 7, Epoch 6: G loss: 0.723882143838065 vs. C loss: 0.08501785745223363\n",
      "Group 7, Epoch 7: G loss: 0.4784065255096981 vs. C loss: 0.21012750930256316\n",
      "Group 7, Epoch 8: G loss: 0.04652729630470275 vs. C loss: 0.2076003220346239\n",
      "Group 8, Epoch 1: G loss: 0.3293055874960763 vs. C loss: 0.18890505532423654\n",
      "Group 8, Epoch 2: G loss: 0.36800365958895 vs. C loss: 0.08387696618835132\n",
      "Group 8, Epoch 3: G loss: 0.7288474125521524 vs. C loss: 0.10892563271853657\n",
      "Group 8, Epoch 4: G loss: 0.801840090751648 vs. C loss: 0.05937237540880839\n",
      "Group 8, Epoch 5: G loss: 0.26743439914924755 vs. C loss: 0.21753274235460493\n",
      "Group 8, Epoch 6: G loss: 0.012823697232774326 vs. C loss: 0.21196671823660532\n",
      "Group 9, Epoch 1: G loss: 0.32081972360610955 vs. C loss: 0.20592123932308623\n",
      "Group 9, Epoch 2: G loss: 0.47934748189789905 vs. C loss: 0.11351597350504665\n",
      "Group 9, Epoch 3: G loss: 0.33079378392015185 vs. C loss: 0.14876485367616016\n",
      "Group 9, Epoch 4: G loss: 0.461176073551178 vs. C loss: 0.21702392399311066\n",
      "Group 9, Epoch 5: G loss: 0.061049313843250276 vs. C loss: 0.2340011265542772\n",
      "Group 9, Epoch 6: G loss: 0.0214918420783111 vs. C loss: 0.20405721829997167\n",
      "Group 10, Epoch 1: G loss: 0.339917916910989 vs. C loss: 0.1927379361457295\n",
      "Group 10, Epoch 2: G loss: 0.3250231070177896 vs. C loss: 0.127894572292765\n",
      "Group 10, Epoch 3: G loss: 0.3815709669675146 vs. C loss: 0.2000287820895513\n",
      "Group 10, Epoch 4: G loss: 0.1336465582251549 vs. C loss: 0.17517766025331286\n",
      "Group 10, Epoch 5: G loss: 0.2356657255973135 vs. C loss: 0.19954042798942992\n",
      "Group 10, Epoch 6: G loss: 0.16045506788151603 vs. C loss: 0.1868161012729009\n",
      "Group 11, Epoch 1: G loss: 0.27576141144548144 vs. C loss: 0.13485807842678493\n",
      "Group 11, Epoch 2: G loss: 0.563710184608187 vs. C loss: 0.12045679820908439\n",
      "Group 11, Epoch 3: G loss: 0.3424223496445588 vs. C loss: 0.218257165617413\n",
      "Group 11, Epoch 4: G loss: 0.03781982426132475 vs. C loss: 0.20627925462192961\n",
      "Group 11, Epoch 5: G loss: 0.025833124348095485 vs. C loss: 0.20570708645714653\n",
      "Group 11, Epoch 6: G loss: 0.027655615125383646 vs. C loss: 0.19605415397220186\n",
      "Group 12, Epoch 1: G loss: 0.31390889627592905 vs. C loss: 0.1947742468780941\n",
      "Group 12, Epoch 2: G loss: 0.19981426170894082 vs. C loss: 0.18193895866473517\n",
      "Group 12, Epoch 3: G loss: 0.24882770010403224 vs. C loss: 0.15005557901329467\n",
      "Group 12, Epoch 4: G loss: 0.29819687860352656 vs. C loss: 0.18437950478659737\n",
      "Group 12, Epoch 5: G loss: 0.13523536856685367 vs. C loss: 0.19313742882675597\n",
      "Group 12, Epoch 6: G loss: 0.12846534379890986 vs. C loss: 0.20790383054150474\n",
      "Group 13, Epoch 1: G loss: 0.3250224705253328 vs. C loss: 0.19641774561670092\n",
      "Group 13, Epoch 2: G loss: 0.2472242376634053 vs. C loss: 0.17256494197580552\n",
      "Group 13, Epoch 3: G loss: 0.16568216255732945 vs. C loss: 0.19604635735352835\n",
      "Group 13, Epoch 4: G loss: 0.15129318130867822 vs. C loss: 0.1952342920833164\n",
      "Group 13, Epoch 5: G loss: 0.1274286402123315 vs. C loss: 0.20111922257476386\n",
      "Group 13, Epoch 6: G loss: 0.0942219438297408 vs. C loss: 0.18305893407927618\n",
      "Group 14, Epoch 1: G loss: 0.3360704004764557 vs. C loss: 0.2339691660470433\n",
      "Group 14, Epoch 2: G loss: 0.3943857073783875 vs. C loss: 0.13114107110434106\n",
      "Group 14, Epoch 3: G loss: 0.3595151158315795 vs. C loss: 0.19811617996957567\n",
      "Group 14, Epoch 4: G loss: 0.05428897781031473 vs. C loss: 0.20824399921629164\n",
      "Group 14, Epoch 5: G loss: 0.02132015882858208 vs. C loss: 0.20250872439808318\n",
      "Group 14, Epoch 6: G loss: 0.02339909901576383 vs. C loss: 0.197661392390728\n",
      "Group 15, Epoch 1: G loss: 0.34776208656174795 vs. C loss: 0.16794305087791547\n",
      "Group 15, Epoch 2: G loss: 0.4026827011789595 vs. C loss: 0.10412192882763016\n",
      "Group 15, Epoch 3: G loss: 0.5510473225797926 vs. C loss: 0.12021252554323937\n",
      "Group 15, Epoch 4: G loss: 0.4814073592424392 vs. C loss: 0.15799257242017323\n",
      "Group 15, Epoch 5: G loss: 0.5738770140068871 vs. C loss: 0.22225962579250336\n",
      "Group 15, Epoch 6: G loss: 0.04568806024534362 vs. C loss: 0.20385804937945473\n",
      "Group 16, Epoch 1: G loss: 0.37664437634604314 vs. C loss: 0.2543392926454544\n",
      "Group 16, Epoch 2: G loss: 0.296688403402056 vs. C loss: 0.17174330684873793\n",
      "Group 16, Epoch 3: G loss: 0.4183503721441541 vs. C loss: 0.12816317876180014\n",
      "Group 16, Epoch 4: G loss: 0.570789828470775 vs. C loss: 0.2253550009595023\n",
      "Group 16, Epoch 5: G loss: 0.06356946996280126 vs. C loss: 0.20583310392167833\n",
      "Group 16, Epoch 6: G loss: 0.03416815253772906 vs. C loss: 0.20240271546774438\n",
      "Group 17, Epoch 1: G loss: 0.33178621871130803 vs. C loss: 0.16956868436601427\n",
      "Group 17, Epoch 2: G loss: 0.29298299082687923 vs. C loss: 0.19204086561997732\n",
      "Group 17, Epoch 3: G loss: 0.09502269221203667 vs. C loss: 0.18114397012525132\n",
      "Group 17, Epoch 4: G loss: 0.13780955608401982 vs. C loss: 0.18864823049969145\n",
      "Group 17, Epoch 5: G loss: 0.18716931087630134 vs. C loss: 0.1714214707414309\n",
      "Group 17, Epoch 6: G loss: 0.16455564371177128 vs. C loss: 0.16422676708963183\n",
      "Group 18, Epoch 1: G loss: 0.2915789646761758 vs. C loss: 0.1942521846956677\n",
      "Group 18, Epoch 2: G loss: 0.4219707821096693 vs. C loss: 0.10390412310759227\n",
      "Group 18, Epoch 3: G loss: 0.3462653436831066 vs. C loss: 0.1879757857984967\n",
      "Group 18, Epoch 4: G loss: 0.32010657574449264 vs. C loss: 0.14050378650426867\n",
      "Group 18, Epoch 5: G loss: 0.45008600481918887 vs. C loss: 0.18472043424844742\n",
      "Group 18, Epoch 6: G loss: 0.15495172142982483 vs. C loss: 0.20645091765456727\n",
      "Group 18, Epoch 7: G loss: 0.11373707566942488 vs. C loss: 0.165511483947436\n",
      "Group 18, Epoch 8: G loss: 0.43660796540124075 vs. C loss: 0.12754252801338833\n",
      "Group 18, Epoch 9: G loss: 0.5283512149538312 vs. C loss: 0.11791989869541591\n",
      "Group 18, Epoch 10: G loss: 0.33021189187254224 vs. C loss: 0.21386957416931787\n",
      "Group 18, Epoch 11: G loss: 0.04931713502321924 vs. C loss: 0.20607915769020715\n",
      "Group 18, Epoch 12: G loss: 0.047726171995912274 vs. C loss: 0.20605347719457415\n",
      "Group 18, Epoch 13: G loss: 0.02840500686849867 vs. C loss: 0.20658662501308653\n",
      "Group 19, Epoch 1: G loss: 0.25851601404803143 vs. C loss: 0.20279773241943785\n",
      "Group 19, Epoch 2: G loss: 0.2555486364024026 vs. C loss: 0.196745913889673\n",
      "Group 19, Epoch 3: G loss: 0.1140311902122838 vs. C loss: 0.197077085574468\n",
      "Group 19, Epoch 4: G loss: 0.08900799474545887 vs. C loss: 0.20407323704825509\n",
      "Group 19, Epoch 5: G loss: 0.10460205269711358 vs. C loss: 0.17855783717499837\n",
      "Group 19, Epoch 6: G loss: 0.16820911169052125 vs. C loss: 0.19246394435564676\n",
      "Group 19, Epoch 7: G loss: 0.1364885687828064 vs. C loss: 0.1769535036550628\n",
      "Group 19, Epoch 8: G loss: 0.22843179021562848 vs. C loss: 0.18162261363532808\n",
      "Group 20, Epoch 1: G loss: 0.31532266693455835 vs. C loss: 0.17958137517174086\n",
      "Group 20, Epoch 2: G loss: 0.3174468981368201 vs. C loss: 0.1716691686047448\n",
      "Group 20, Epoch 3: G loss: 0.15534853999103818 vs. C loss: 0.21648680004808638\n",
      "Group 20, Epoch 4: G loss: 0.06204520825828824 vs. C loss: 0.18968885309166375\n",
      "Group 20, Epoch 5: G loss: 0.09491114446095059 vs. C loss: 0.1601509526371956\n",
      "Group 20, Epoch 6: G loss: 0.3752177408763341 vs. C loss: 0.14791060735781988\n",
      "Group 21, Epoch 1: G loss: 0.39882842046873906 vs. C loss: 0.1983322236273024\n",
      "Group 21, Epoch 2: G loss: 0.5145362147263118 vs. C loss: 0.09032247960567474\n",
      "Group 21, Epoch 3: G loss: 0.7587843911988396 vs. C loss: 0.07176698382116027\n",
      "Group 21, Epoch 4: G loss: 0.834666348355157 vs. C loss: 0.11865707341995503\n",
      "Group 21, Epoch 5: G loss: 0.28063417598605156 vs. C loss: 0.21847948763105604\n",
      "Group 21, Epoch 6: G loss: 0.04572671332529613 vs. C loss: 0.2091070719891124\n",
      "Group 22, Epoch 1: G loss: 0.29834669572966444 vs. C loss: 0.20349188148975372\n",
      "Group 22, Epoch 2: G loss: 0.26201019372258866 vs. C loss: 0.17351833565367591\n",
      "Group 22, Epoch 3: G loss: 0.19785872719117573 vs. C loss: 0.1954388419787089\n",
      "Group 22, Epoch 4: G loss: 0.10532217941113882 vs. C loss: 0.214818752474255\n",
      "Group 22, Epoch 5: G loss: 0.02521606981754303 vs. C loss: 0.20218018276823893\n",
      "Group 22, Epoch 6: G loss: 0.0336351851267474 vs. C loss: 0.18781286146905685\n",
      "Group 23, Epoch 1: G loss: 0.3214794754981995 vs. C loss: 0.2210697539978557\n",
      "Group 23, Epoch 2: G loss: 0.3456695181982858 vs. C loss: 0.1292110656698545\n",
      "Group 23, Epoch 3: G loss: 0.5487034606082098 vs. C loss: 0.22179034021165636\n",
      "Group 23, Epoch 4: G loss: 0.12187033402068273 vs. C loss: 0.20915005107720694\n",
      "Group 23, Epoch 5: G loss: 0.11120235111032215 vs. C loss: 0.16462406681643593\n",
      "Group 23, Epoch 6: G loss: 0.2687958649226597 vs. C loss: 0.16829969568385017\n",
      "Group 24, Epoch 1: G loss: 0.276303836277553 vs. C loss: 0.20696686870521971\n",
      "Group 24, Epoch 2: G loss: 0.39570774393422264 vs. C loss: 0.16460936433739132\n",
      "Group 24, Epoch 3: G loss: 0.25245148028646197 vs. C loss: 0.21469041208426157\n",
      "Group 24, Epoch 4: G loss: 0.02847701986985548 vs. C loss: 0.20513821144898733\n",
      "Group 24, Epoch 5: G loss: 0.010254338490111486 vs. C loss: 0.20531852708922493\n",
      "Group 24, Epoch 6: G loss: 0.010964520834386348 vs. C loss: 0.20375675790839723\n",
      "Group 25, Epoch 1: G loss: 0.39279989600181586 vs. C loss: 0.2276430543926027\n",
      "Group 25, Epoch 2: G loss: 0.29226534622056144 vs. C loss: 0.17586505247486964\n",
      "Group 25, Epoch 3: G loss: 0.1702411289725985 vs. C loss: 0.17378009690178764\n",
      "Group 25, Epoch 4: G loss: 0.24251713071550637 vs. C loss: 0.19975258906682333\n",
      "Group 25, Epoch 5: G loss: 0.09327873587608337 vs. C loss: 0.20783922904067567\n",
      "Group 25, Epoch 6: G loss: 0.03087985265467848 vs. C loss: 0.20271151016155878\n",
      "Group 26, Epoch 1: G loss: 0.3147403146539416 vs. C loss: 0.1780682123369641\n",
      "Group 26, Epoch 2: G loss: 0.250846625651632 vs. C loss: 0.22053894897301993\n",
      "Group 26, Epoch 3: G loss: 0.04600392963205065 vs. C loss: 0.19411412874857584\n",
      "Group 26, Epoch 4: G loss: 0.08890193349548747 vs. C loss: 0.18290193710062239\n",
      "Group 26, Epoch 5: G loss: 0.2557319513389042 vs. C loss: 0.19395277152458826\n",
      "Group 26, Epoch 6: G loss: 0.09105676519019264 vs. C loss: 0.17609830697377524\n",
      "Group 27, Epoch 1: G loss: 0.246219755922045 vs. C loss: 0.15836440606249705\n",
      "Group 27, Epoch 2: G loss: 0.2848642034190042 vs. C loss: 0.20266927778720856\n",
      "Group 27, Epoch 3: G loss: 0.05086507382137435 vs. C loss: 0.2067424886756473\n",
      "Group 27, Epoch 4: G loss: 0.036860055848956116 vs. C loss: 0.19420727921856773\n",
      "Group 27, Epoch 5: G loss: 0.12288423478603362 vs. C loss: 0.16323731922441057\n",
      "Group 27, Epoch 6: G loss: 0.1824770893369402 vs. C loss: 0.2097275315059556\n",
      "Group 28, Epoch 1: G loss: 0.2115777658564704 vs. C loss: 0.17302193120121956\n",
      "Group 28, Epoch 2: G loss: 0.3842595351593835 vs. C loss: 0.1884254515171051\n",
      "Group 28, Epoch 3: G loss: 0.23417983566011702 vs. C loss: 0.17449296762545904\n",
      "Group 28, Epoch 4: G loss: 0.30457020359379905 vs. C loss: 0.18577519141965446\n",
      "Group 28, Epoch 5: G loss: 0.2777545349938529 vs. C loss: 0.2136489748954773\n",
      "Group 28, Epoch 6: G loss: 0.07840488808495658 vs. C loss: 0.19854266113705107\n",
      "Group 29, Epoch 1: G loss: 0.21495825690882547 vs. C loss: 0.1638274441162745\n",
      "Group 29, Epoch 2: G loss: 0.23184755018779213 vs. C loss: 0.17285608831379148\n",
      "Group 29, Epoch 3: G loss: 0.2372110622269767 vs. C loss: 0.2119709187083774\n",
      "Group 29, Epoch 4: G loss: 0.041122595380459516 vs. C loss: 0.20653600494066873\n",
      "Group 29, Epoch 5: G loss: 0.024956334009766577 vs. C loss: 0.19914412912395263\n",
      "Group 29, Epoch 6: G loss: 0.04700771123170853 vs. C loss: 0.1745324846771028\n",
      "Group 30, Epoch 1: G loss: 0.321799749987466 vs. C loss: 0.19403518570793998\n",
      "Group 30, Epoch 2: G loss: 0.20195943606751304 vs. C loss: 0.20751765370368958\n",
      "Group 30, Epoch 3: G loss: 0.03639310507902078 vs. C loss: 0.2009259205725458\n",
      "Group 30, Epoch 4: G loss: 0.025502798972385273 vs. C loss: 0.20297917889224157\n",
      "Group 30, Epoch 5: G loss: 0.032273397435035024 vs. C loss: 0.1981530181235737\n",
      "Group 30, Epoch 6: G loss: 0.05646203522171294 vs. C loss: 0.18689136538240644\n",
      "Group 31, Epoch 1: G loss: 0.4043821854250772 vs. C loss: 0.21264067954487273\n",
      "Group 31, Epoch 2: G loss: 0.30435247038091934 vs. C loss: 0.13389950535363623\n",
      "Group 31, Epoch 3: G loss: 0.587770539522171 vs. C loss: 0.11799776554107667\n",
      "Group 31, Epoch 4: G loss: 0.36303014882973267 vs. C loss: 0.21232027808825174\n",
      "Group 31, Epoch 5: G loss: 0.05398765855601855 vs. C loss: 0.20777896755478445\n",
      "Group 31, Epoch 6: G loss: 0.01413061585543411 vs. C loss: 0.2078317834271325\n",
      "Group 32, Epoch 1: G loss: 0.35589812823704314 vs. C loss: 0.23156371381547716\n",
      "Group 32, Epoch 2: G loss: 0.33522374502250135 vs. C loss: 0.1990148334039582\n",
      "Group 32, Epoch 3: G loss: 0.08528786195175989 vs. C loss: 0.19883401691913605\n",
      "Group 32, Epoch 4: G loss: 0.055397013681275496 vs. C loss: 0.1833777411116494\n",
      "Group 32, Epoch 5: G loss: 0.19129247197083063 vs. C loss: 0.17304958237542045\n",
      "Group 32, Epoch 6: G loss: 0.28602008904729564 vs. C loss: 0.18344627486334908\n",
      "Group 33, Epoch 1: G loss: 0.3073134171111243 vs. C loss: 0.16478640089432398\n",
      "Group 33, Epoch 2: G loss: 0.3625558614730835 vs. C loss: 0.18809468547503153\n",
      "Group 33, Epoch 3: G loss: 0.13006634776081358 vs. C loss: 0.17051316218243706\n",
      "Group 33, Epoch 4: G loss: 0.2427200734615326 vs. C loss: 0.22862250109513602\n",
      "Group 33, Epoch 5: G loss: 0.04554692389709609 vs. C loss: 0.21022421783871123\n",
      "Group 33, Epoch 6: G loss: 0.016916746300246036 vs. C loss: 0.20260747687684166\n",
      "Group 34, Epoch 1: G loss: 0.2607965724808829 vs. C loss: 0.18544834107160568\n",
      "Group 34, Epoch 2: G loss: 0.42340032628604346 vs. C loss: 0.10525320553117327\n",
      "Group 34, Epoch 3: G loss: 0.4174203506537846 vs. C loss: 0.20670785796311164\n",
      "Group 34, Epoch 4: G loss: 0.06523159561412675 vs. C loss: 0.20332151485813987\n",
      "Group 34, Epoch 5: G loss: 0.017764699991260253 vs. C loss: 0.20452878461542745\n",
      "Group 34, Epoch 6: G loss: 0.01400242019444704 vs. C loss: 0.20173884762658012\n",
      "Group 35, Epoch 1: G loss: 0.4678891692842756 vs. C loss: 0.2576099783182144\n",
      "Group 35, Epoch 2: G loss: 0.39799578658172063 vs. C loss: 0.202935548292266\n",
      "Group 35, Epoch 3: G loss: 0.11036470234394073 vs. C loss: 0.19074930250644684\n",
      "Group 35, Epoch 4: G loss: 0.10804391865219388 vs. C loss: 0.1686957279841105\n",
      "Group 35, Epoch 5: G loss: 0.15140913043703352 vs. C loss: 0.2075666586558024\n",
      "Group 35, Epoch 6: G loss: 0.0889216369816235 vs. C loss: 0.19171673556168875\n",
      "Group 36, Epoch 1: G loss: 0.44209155866077976 vs. C loss: 0.2294776174757216\n",
      "Group 36, Epoch 2: G loss: 0.350418758392334 vs. C loss: 0.18798352777957916\n",
      "Group 36, Epoch 3: G loss: 0.10516035471643723 vs. C loss: 0.18728528999620017\n",
      "Group 36, Epoch 4: G loss: 0.11973553640501841 vs. C loss: 0.17644723173644808\n",
      "Group 36, Epoch 5: G loss: 0.23307767723287856 vs. C loss: 0.14029661441842717\n",
      "Group 36, Epoch 6: G loss: 0.44443014604704717 vs. C loss: 0.10538479934136073\n",
      "Group 37, Epoch 1: G loss: 0.3439137399196625 vs. C loss: 0.1836902913120058\n",
      "Group 37, Epoch 2: G loss: 0.3192986960921969 vs. C loss: 0.14582759473058912\n",
      "Group 37, Epoch 3: G loss: 0.4971359848976136 vs. C loss: 0.10568319840563668\n",
      "Group 37, Epoch 4: G loss: 0.3734507669295584 vs. C loss: 0.21842237975862291\n",
      "Group 37, Epoch 5: G loss: 0.04616911262273788 vs. C loss: 0.2049964750411972\n",
      "Group 37, Epoch 6: G loss: 0.03418786972761154 vs. C loss: 0.20008522437678444\n",
      "Group 38, Epoch 1: G loss: 0.45662591883114406 vs. C loss: 0.24617708557181892\n",
      "Group 38, Epoch 2: G loss: 0.4653651126793452 vs. C loss: 0.16015600661436716\n",
      "Group 38, Epoch 3: G loss: 0.23151717845882688 vs. C loss: 0.19264439907338884\n",
      "Group 38, Epoch 4: G loss: 0.10130331643990108 vs. C loss: 0.1892891377210617\n",
      "Group 38, Epoch 5: G loss: 0.09153525935752052 vs. C loss: 0.20487571259339651\n",
      "Group 38, Epoch 6: G loss: 0.06700069499867303 vs. C loss: 0.1998627483844757\n",
      "Group 39, Epoch 1: G loss: 0.25198114131178173 vs. C loss: 0.1801707074046135\n",
      "Group 39, Epoch 2: G loss: 0.3459300190210342 vs. C loss: 0.10965378748046027\n",
      "Group 39, Epoch 3: G loss: 0.4821090928145817 vs. C loss: 0.20566801561249626\n",
      "Group 39, Epoch 4: G loss: 0.0740587489945548 vs. C loss: 0.20090543064806196\n",
      "Group 39, Epoch 5: G loss: 0.04303064559187208 vs. C loss: 0.19201114980710876\n",
      "Group 39, Epoch 6: G loss: 0.11048905083111356 vs. C loss: 0.17707093556722006\n",
      "Group 40, Epoch 1: G loss: 0.1627640698637281 vs. C loss: 0.15343000325891706\n",
      "Group 40, Epoch 2: G loss: 0.26695705269064224 vs. C loss: 0.21341743071873984\n",
      "Group 40, Epoch 3: G loss: 0.10406889106546129 vs. C loss: 0.16107514748970667\n",
      "Group 40, Epoch 4: G loss: 0.3479945182800293 vs. C loss: 0.13300676974985334\n",
      "Group 40, Epoch 5: G loss: 0.3205522273268019 vs. C loss: 0.2005288733376397\n",
      "Group 40, Epoch 6: G loss: 0.07911227877650942 vs. C loss: 0.199484184384346\n",
      "Group 41, Epoch 1: G loss: 0.43072999545506063 vs. C loss: 0.1874192539188597\n",
      "Group 41, Epoch 2: G loss: 0.2506140031984874 vs. C loss: 0.1857792420519723\n",
      "Group 41, Epoch 3: G loss: 0.10039781651326588 vs. C loss: 0.16547112001313102\n",
      "Group 41, Epoch 4: G loss: 0.2358336195349693 vs. C loss: 0.183677034245597\n",
      "Group 41, Epoch 5: G loss: 0.1556003457733563 vs. C loss: 0.20415663719177246\n",
      "Group 41, Epoch 6: G loss: 0.04299323175634656 vs. C loss: 0.1929563449488746\n",
      "Group 42, Epoch 1: G loss: 0.35937570674078806 vs. C loss: 0.21747352017296687\n",
      "Group 42, Epoch 2: G loss: 0.2548021465539932 vs. C loss: 0.1384304952290323\n",
      "Group 42, Epoch 3: G loss: 0.27467648748840606 vs. C loss: 0.17375152640872535\n",
      "Group 42, Epoch 4: G loss: 0.26772727412836894 vs. C loss: 0.18645632598135206\n",
      "Group 42, Epoch 5: G loss: 0.20354835731642587 vs. C loss: 0.1491117775440216\n",
      "Group 42, Epoch 6: G loss: 0.35193110278674533 vs. C loss: 0.1547639270623525\n",
      "Group 42, Epoch 7: G loss: 0.3025205756936754 vs. C loss: 0.21460341744952735\n",
      "Group 43, Epoch 1: G loss: 0.3052933233124869 vs. C loss: 0.18386988590161005\n",
      "Group 43, Epoch 2: G loss: 0.3069480099848339 vs. C loss: 0.15064746058649486\n",
      "Group 43, Epoch 3: G loss: 0.364584595816476 vs. C loss: 0.15734614514642292\n",
      "Group 43, Epoch 4: G loss: 0.3657204866409302 vs. C loss: 0.18426845636632705\n",
      "Group 43, Epoch 5: G loss: 0.19850940959794183 vs. C loss: 0.22392697797881234\n",
      "Group 43, Epoch 6: G loss: 0.04759959227272443 vs. C loss: 0.20442267921235824\n",
      "Group 44, Epoch 1: G loss: 0.27042791162218366 vs. C loss: 0.16155755602651173\n",
      "Group 44, Epoch 2: G loss: 0.2868975409439632 vs. C loss: 0.1470003906223509\n",
      "Group 44, Epoch 3: G loss: 0.3091556025402887 vs. C loss: 0.12057014885875915\n",
      "Group 44, Epoch 4: G loss: 0.5813884411539351 vs. C loss: 0.10926078177160686\n",
      "Group 44, Epoch 5: G loss: 0.7813470482826234 vs. C loss: 0.045307169978817306\n",
      "Group 44, Epoch 6: G loss: 0.5385620521647589 vs. C loss: 0.19627755797571608\n",
      "Group 45, Epoch 1: G loss: 0.37491580077580045 vs. C loss: 0.17428214930825767\n",
      "Group 45, Epoch 2: G loss: 0.31926758459636145 vs. C loss: 0.17411207656065622\n",
      "Group 45, Epoch 3: G loss: 0.27446612800870623 vs. C loss: 0.09967102027601665\n",
      "Group 45, Epoch 4: G loss: 0.7588715740612575 vs. C loss: 0.054684646841552526\n",
      "Group 45, Epoch 5: G loss: 0.3117168441414833 vs. C loss: 0.2035391612185372\n",
      "Group 45, Epoch 6: G loss: 0.02492231295577117 vs. C loss: 0.20756875971953073\n",
      "Group 46, Epoch 1: G loss: 0.24374856097357614 vs. C loss: 0.15346209870444405\n",
      "Group 46, Epoch 2: G loss: 0.2685101385627474 vs. C loss: 0.12633540315760505\n",
      "Group 46, Epoch 3: G loss: 0.5681962515626634 vs. C loss: 0.07447355406151877\n",
      "Group 46, Epoch 4: G loss: 0.8483305249895369 vs. C loss: 0.015039518294442033\n",
      "Group 46, Epoch 5: G loss: 0.3851508059671947 vs. C loss: 0.22744876560237673\n",
      "Group 46, Epoch 6: G loss: 0.015461933559605056 vs. C loss: 0.20836548921134737\n",
      "Group 47, Epoch 1: G loss: 0.3004735197339739 vs. C loss: 0.1673789181643062\n",
      "Group 47, Epoch 2: G loss: 0.26776775802884784 vs. C loss: 0.12611557005180252\n",
      "Group 47, Epoch 3: G loss: 0.5650373654706137 vs. C loss: 0.1606957432296541\n",
      "Group 47, Epoch 4: G loss: 0.27327285196099965 vs. C loss: 0.16887477950917348\n",
      "Group 47, Epoch 5: G loss: 0.16471383135233605 vs. C loss: 0.2118797136677636\n",
      "Group 47, Epoch 6: G loss: 0.04967471254723412 vs. C loss: 0.20305661857128143\n",
      "Group 48, Epoch 1: G loss: 0.3095869549683163 vs. C loss: 0.18465247088008455\n",
      "Group 48, Epoch 2: G loss: 0.26555938380105154 vs. C loss: 0.16383165452215406\n",
      "Group 48, Epoch 3: G loss: 0.2766829226698195 vs. C loss: 0.11425576069288784\n",
      "Group 48, Epoch 4: G loss: 0.5346352781568254 vs. C loss: 0.06494549910227458\n",
      "Group 48, Epoch 5: G loss: 0.4818342245050839 vs. C loss: 0.19540374974409738\n",
      "Group 48, Epoch 6: G loss: 0.07417625662471565 vs. C loss: 0.21632693873511422\n",
      "Group 49, Epoch 1: G loss: 0.3193576182637896 vs. C loss: 0.22122446116473937\n",
      "Group 49, Epoch 2: G loss: 0.38887505871909006 vs. C loss: 0.08385967628823386\n",
      "Group 49, Epoch 3: G loss: 0.7736405917576381 vs. C loss: 0.040336934435698725\n",
      "Group 49, Epoch 4: G loss: 0.4324783772230148 vs. C loss: 0.14162149963279566\n",
      "Group 49, Epoch 5: G loss: 0.39604037143290044 vs. C loss: 0.2174951434135437\n",
      "Group 49, Epoch 6: G loss: 0.019143701451165333 vs. C loss: 0.20413236320018768\n",
      "Group 50, Epoch 1: G loss: 0.24736848558698382 vs. C loss: 0.19858038756582472\n",
      "Group 50, Epoch 2: G loss: 0.2190541326999664 vs. C loss: 0.14866647703780067\n",
      "Group 50, Epoch 3: G loss: 0.4091304651328495 vs. C loss: 0.12804370125134787\n",
      "Group 50, Epoch 4: G loss: 0.25282794364861083 vs. C loss: 0.20567538672023347\n",
      "Group 50, Epoch 5: G loss: 0.06253397092223169 vs. C loss: 0.1969958245754242\n",
      "Group 50, Epoch 6: G loss: 0.07780601478048733 vs. C loss: 0.18825801958640417\n",
      "Group 51, Epoch 1: G loss: 0.4149900342736926 vs. C loss: 0.23978549904293486\n",
      "Group 51, Epoch 2: G loss: 0.25165104993752074 vs. C loss: 0.17763235833909777\n",
      "Group 51, Epoch 3: G loss: 0.16747026102883478 vs. C loss: 0.1907108724117279\n",
      "Group 51, Epoch 4: G loss: 0.11275099239179065 vs. C loss: 0.19574851625495485\n",
      "Group 51, Epoch 5: G loss: 0.08946480814899717 vs. C loss: 0.18893156117863127\n",
      "Group 51, Epoch 6: G loss: 0.1176925552742822 vs. C loss: 0.16996738149060142\n",
      "Group 52, Epoch 1: G loss: 0.32897651961871555 vs. C loss: 0.16871623529328242\n",
      "Group 52, Epoch 2: G loss: 0.3226458498409816 vs. C loss: 0.1299566767281956\n",
      "Group 52, Epoch 3: G loss: 0.41518998358930853 vs. C loss: 0.16401312003533047\n",
      "Group 52, Epoch 4: G loss: 0.2805836226258959 vs. C loss: 0.16905081272125244\n",
      "Group 52, Epoch 5: G loss: 0.2553449170930045 vs. C loss: 0.18910792221625647\n",
      "Group 52, Epoch 6: G loss: 0.22373156547546388 vs. C loss: 0.11800757630003823\n",
      "Group 53, Epoch 1: G loss: 0.47163139666829795 vs. C loss: 0.265319084127744\n",
      "Group 53, Epoch 2: G loss: 0.3164981995310102 vs. C loss: 0.18445161978403726\n",
      "Group 53, Epoch 3: G loss: 0.15577306768723895 vs. C loss: 0.2064966062704722\n",
      "Group 53, Epoch 4: G loss: 0.030789095961621833 vs. C loss: 0.2035764435099231\n",
      "Group 53, Epoch 5: G loss: 0.020186634893928253 vs. C loss: 0.2024743060270945\n",
      "Group 53, Epoch 6: G loss: 0.03159692021352904 vs. C loss: 0.1956738473640548\n",
      "Group 54, Epoch 1: G loss: 0.19784687502043588 vs. C loss: 0.19020304083824158\n",
      "Group 54, Epoch 2: G loss: 0.21355324174676624 vs. C loss: 0.14777902099821302\n",
      "Group 54, Epoch 3: G loss: 0.2729709506034851 vs. C loss: 0.20003592636850143\n",
      "Group 54, Epoch 4: G loss: 0.15933403968811036 vs. C loss: 0.20132044951121011\n",
      "Group 54, Epoch 5: G loss: 0.08497220863189014 vs. C loss: 0.20544284664922288\n",
      "Group 54, Epoch 6: G loss: 0.054694456713540215 vs. C loss: 0.1844551165898641\n",
      "Group 55, Epoch 1: G loss: 0.40694958652768815 vs. C loss: 0.19735500547620988\n",
      "Group 55, Epoch 2: G loss: 0.2840726447956903 vs. C loss: 0.17768854896227518\n",
      "Group 55, Epoch 3: G loss: 0.1957974983113153 vs. C loss: 0.20258812606334686\n",
      "Group 55, Epoch 4: G loss: 0.0642727817807879 vs. C loss: 0.20921843581729463\n",
      "Group 55, Epoch 5: G loss: 0.02893505378493241 vs. C loss: 0.19977635476324293\n",
      "Group 55, Epoch 6: G loss: 0.03618897774389812 vs. C loss: 0.19489105708069274\n",
      "Group 56, Epoch 1: G loss: 0.3440069854259491 vs. C loss: 0.17321185188161003\n",
      "Group 56, Epoch 2: G loss: 0.33834901579788756 vs. C loss: 0.16810559398598143\n",
      "Group 56, Epoch 3: G loss: 0.19833163470029833 vs. C loss: 0.23570980297194588\n",
      "Group 56, Epoch 4: G loss: 0.06352196271930423 vs. C loss: 0.20361541542741987\n",
      "Group 56, Epoch 5: G loss: 0.04785661016191756 vs. C loss: 0.1931831571790907\n",
      "Group 56, Epoch 6: G loss: 0.10381489715405874 vs. C loss: 0.15649487409326765\n",
      "Group 57, Epoch 1: G loss: 0.4181681045464107 vs. C loss: 0.2575933428274261\n",
      "Group 57, Epoch 2: G loss: 0.2996252775192261 vs. C loss: 0.19395950933297476\n",
      "Group 57, Epoch 3: G loss: 0.08940550770078388 vs. C loss: 0.179569638437695\n",
      "Group 57, Epoch 4: G loss: 0.14927144199609757 vs. C loss: 0.19355365137259165\n",
      "Group 57, Epoch 5: G loss: 0.11529228623424259 vs. C loss: 0.20465341541502213\n",
      "Group 57, Epoch 6: G loss: 0.06716096624732018 vs. C loss: 0.18918958471881017\n",
      "Group 58, Epoch 1: G loss: 0.2540582231112889 vs. C loss: 0.23454206519656715\n",
      "Group 58, Epoch 2: G loss: 0.3368002661636897 vs. C loss: 0.12656211604674658\n",
      "Group 58, Epoch 3: G loss: 0.3360037454536983 vs. C loss: 0.18174226706226668\n",
      "Group 58, Epoch 4: G loss: 0.06323151215910912 vs. C loss: 0.21039393544197083\n",
      "Group 58, Epoch 5: G loss: 0.008901060532246317 vs. C loss: 0.20754841218392053\n",
      "Group 58, Epoch 6: G loss: 0.0035701210344476356 vs. C loss: 0.20745299590958488\n",
      "Group 59, Epoch 1: G loss: 0.26180945975439884 vs. C loss: 0.18778028960029283\n",
      "Group 59, Epoch 2: G loss: 0.24944873026439116 vs. C loss: 0.18720714582337272\n",
      "Group 59, Epoch 3: G loss: 0.14286998957395552 vs. C loss: 0.2034387853410509\n",
      "Group 59, Epoch 4: G loss: 0.12468480723244803 vs. C loss: 0.17580370439423454\n",
      "Group 59, Epoch 5: G loss: 0.16816112122365404 vs. C loss: 0.22302374574873185\n",
      "Group 59, Epoch 6: G loss: 0.008498347058360067 vs. C loss: 0.2089280469550027\n",
      "Group 60, Epoch 1: G loss: 0.42948465517589024 vs. C loss: 0.1952700350019667\n",
      "Group 60, Epoch 2: G loss: 0.2738233936684472 vs. C loss: 0.17831266257498\n",
      "Group 60, Epoch 3: G loss: 0.23897436133452826 vs. C loss: 0.20614537762271035\n",
      "Group 60, Epoch 4: G loss: 0.06953965201973915 vs. C loss: 0.18867465191417268\n",
      "Group 60, Epoch 5: G loss: 0.07012569776603154 vs. C loss: 0.19879760675960115\n",
      "Group 60, Epoch 6: G loss: 0.09221135697194507 vs. C loss: 0.1885400927729077\n",
      "Group 61, Epoch 1: G loss: 0.39816786646842955 vs. C loss: 0.25650291641553247\n",
      "Group 61, Epoch 2: G loss: 0.3095443397760391 vs. C loss: 0.18742366962962678\n",
      "Group 61, Epoch 3: G loss: 0.11970756245510918 vs. C loss: 0.17157630870739618\n",
      "Group 61, Epoch 4: G loss: 0.15222349975790297 vs. C loss: 0.1671137130922741\n",
      "Group 61, Epoch 5: G loss: 0.333635481766292 vs. C loss: 0.15577770355674955\n",
      "Group 61, Epoch 6: G loss: 0.2293373818908419 vs. C loss: 0.18620670007334816\n",
      "Group 62, Epoch 1: G loss: 0.4403675539152963 vs. C loss: 0.22473678572310343\n",
      "Group 62, Epoch 2: G loss: 0.39845048955508633 vs. C loss: 0.17163376179006365\n",
      "Group 62, Epoch 3: G loss: 0.38990848192146854 vs. C loss: 0.14904706842369503\n",
      "Group 62, Epoch 4: G loss: 0.3488231837749481 vs. C loss: 0.21403337187237212\n",
      "Group 62, Epoch 5: G loss: 0.07592422302280154 vs. C loss: 0.19884981711705527\n",
      "Group 62, Epoch 6: G loss: 0.05912804113967078 vs. C loss: 0.19089985887209573\n",
      "Group 63, Epoch 1: G loss: 0.2351447182042258 vs. C loss: 0.20637325280242494\n",
      "Group 63, Epoch 2: G loss: 0.401469515476908 vs. C loss: 0.20207808083958098\n",
      "Group 63, Epoch 3: G loss: 0.17070222922733852 vs. C loss: 0.13738103873199886\n",
      "Group 63, Epoch 4: G loss: 0.44544953533581333 vs. C loss: 0.20353260139624277\n",
      "Group 63, Epoch 5: G loss: 0.08439589898501124 vs. C loss: 0.21730983091725245\n",
      "Group 63, Epoch 6: G loss: 0.020339049744818892 vs. C loss: 0.21093067195680404\n",
      "Group 64, Epoch 1: G loss: 0.2955408232552664 vs. C loss: 0.19665988617473176\n",
      "Group 64, Epoch 2: G loss: 0.38802801455770214 vs. C loss: 0.10337698376841016\n",
      "Group 64, Epoch 3: G loss: 0.5351268721478326 vs. C loss: 0.14333283570077684\n",
      "Group 64, Epoch 4: G loss: 0.680927214877946 vs. C loss: 0.17660556940568817\n",
      "Group 64, Epoch 5: G loss: 0.38914049714803695 vs. C loss: 0.21873547964625892\n",
      "Group 64, Epoch 6: G loss: 0.08084119228380064 vs. C loss: 0.2098350856039259\n",
      "Group 65, Epoch 1: G loss: 0.44717969468661717 vs. C loss: 0.24380692839622498\n",
      "Group 65, Epoch 2: G loss: 0.32933706981795174 vs. C loss: 0.13957523388995063\n",
      "Group 65, Epoch 3: G loss: 0.24052813266004835 vs. C loss: 0.19853946566581726\n",
      "Group 65, Epoch 4: G loss: 0.0650152040379388 vs. C loss: 0.20015255941285026\n",
      "Group 65, Epoch 5: G loss: 0.04988826236554555 vs. C loss: 0.1871413712700208\n",
      "Group 65, Epoch 6: G loss: 0.15305308401584625 vs. C loss: 0.15870165824890137\n",
      "Group 66, Epoch 1: G loss: 0.3713367513247898 vs. C loss: 0.21667347931199601\n",
      "Group 66, Epoch 2: G loss: 0.23901376894542148 vs. C loss: 0.20567210639516512\n",
      "Group 66, Epoch 3: G loss: 0.060022798180580136 vs. C loss: 0.1984934988949034\n",
      "Group 66, Epoch 4: G loss: 0.05551161648971694 vs. C loss: 0.18805054078499475\n",
      "Group 66, Epoch 5: G loss: 0.12180765228612081 vs. C loss: 0.14372744494014314\n",
      "Group 66, Epoch 6: G loss: 0.42564351473535805 vs. C loss: 0.1706568251053492\n",
      "Group 67, Epoch 1: G loss: 0.4288297372204917 vs. C loss: 0.22315588179561827\n",
      "Group 67, Epoch 2: G loss: 0.2482027786118644 vs. C loss: 0.18233825597498152\n",
      "Group 67, Epoch 3: G loss: 0.1667409564767565 vs. C loss: 0.1716714459988806\n",
      "Group 67, Epoch 4: G loss: 0.1599905482360295 vs. C loss: 0.22027925650278726\n",
      "Group 67, Epoch 5: G loss: 0.018829550806965144 vs. C loss: 0.2056591139076368\n",
      "Group 67, Epoch 6: G loss: 0.010331905713038785 vs. C loss: 0.20596572591198817\n",
      "Group 68, Epoch 1: G loss: 0.44944302099091665 vs. C loss: 0.197808268169562\n",
      "Group 68, Epoch 2: G loss: 0.3909959060805185 vs. C loss: 0.10264699906110764\n",
      "Group 68, Epoch 3: G loss: 0.8082839335714068 vs. C loss: 0.04182270314130518\n",
      "Group 68, Epoch 4: G loss: 0.7457912547247751 vs. C loss: 0.014927637970281972\n",
      "Group 68, Epoch 5: G loss: 0.8832514422280449 vs. C loss: 0.019763439376321103\n",
      "Group 68, Epoch 6: G loss: 0.9410210626465932 vs. C loss: 0.061397154505054154\n",
      "Group 69, Epoch 1: G loss: 0.2932970851659774 vs. C loss: 0.1971097820334964\n",
      "Group 69, Epoch 2: G loss: 0.2571266608578818 vs. C loss: 0.1526344037718243\n",
      "Group 69, Epoch 3: G loss: 0.29571601578167506 vs. C loss: 0.19191947248246935\n",
      "Group 69, Epoch 4: G loss: 0.18156531431845255 vs. C loss: 0.15534976538684633\n",
      "Group 69, Epoch 5: G loss: 0.3499982067516872 vs. C loss: 0.21023978955215875\n",
      "Group 69, Epoch 6: G loss: 0.1006877200944083 vs. C loss: 0.1897602569725778\n",
      "Group 70, Epoch 1: G loss: 0.31722527657236366 vs. C loss: 0.2121036135488086\n",
      "Group 70, Epoch 2: G loss: 0.4025721643652235 vs. C loss: 0.13648641523387697\n",
      "Group 70, Epoch 3: G loss: 0.339298038823264 vs. C loss: 0.16934741164247194\n",
      "Group 70, Epoch 4: G loss: 0.22810910216399602 vs. C loss: 0.19392916725741494\n",
      "Group 70, Epoch 5: G loss: 0.1837621905973979 vs. C loss: 0.20294268429279327\n",
      "Group 70, Epoch 6: G loss: 0.12171082645654677 vs. C loss: 0.15577595763736302\n",
      "Group 71, Epoch 1: G loss: 0.339960389477866 vs. C loss: 0.2226781381501092\n",
      "Group 71, Epoch 2: G loss: 0.25457787684031896 vs. C loss: 0.16192559649546942\n",
      "Group 71, Epoch 3: G loss: 0.24120201127869742 vs. C loss: 0.14944374561309812\n",
      "Group 71, Epoch 4: G loss: 0.5069487546171461 vs. C loss: 0.07884498892558946\n",
      "Group 71, Epoch 5: G loss: 0.3406164770679814 vs. C loss: 0.208027054866155\n",
      "Group 71, Epoch 6: G loss: 0.026447189918586182 vs. C loss: 0.20877549714512297\n",
      "Group 72, Epoch 1: G loss: 0.44148753966603954 vs. C loss: 0.17295944856272805\n",
      "Group 72, Epoch 2: G loss: 0.4516137089048113 vs. C loss: 0.1213443817363845\n",
      "Group 72, Epoch 3: G loss: 0.5288160715784345 vs. C loss: 0.06746164709329605\n",
      "Group 72, Epoch 4: G loss: 0.7819119504519871 vs. C loss: 0.10476020723581314\n",
      "Group 72, Epoch 5: G loss: 0.7113377745662417 vs. C loss: 0.23720172544320425\n",
      "Group 72, Epoch 6: G loss: 0.04552070647478103 vs. C loss: 0.20862366093529597\n",
      "Group 73, Epoch 1: G loss: 0.37585514443261286 vs. C loss: 0.19421026027864877\n",
      "Group 73, Epoch 2: G loss: 0.431302798645837 vs. C loss: 0.12261500209569931\n",
      "Group 73, Epoch 3: G loss: 0.3577109758343015 vs. C loss: 0.19723306596279144\n",
      "Group 73, Epoch 4: G loss: 0.07264185398817062 vs. C loss: 0.20110844737953612\n",
      "Group 73, Epoch 5: G loss: 0.0746418976358005 vs. C loss: 0.19725637717379463\n",
      "Group 73, Epoch 6: G loss: 0.16241966954299383 vs. C loss: 0.19036306606398687\n",
      "Group 73, Epoch 7: G loss: 0.33239231109619144 vs. C loss: 0.16150439116689894\n",
      "Group 73, Epoch 8: G loss: 0.26482058082308085 vs. C loss: 0.19006513059139252\n",
      "Group 73, Epoch 9: G loss: 0.12359746598771638 vs. C loss: 0.21002831061681113\n",
      "Group 74, Epoch 1: G loss: 0.3983804447310312 vs. C loss: 0.22367300424310899\n",
      "Group 74, Epoch 2: G loss: 0.4301463876451765 vs. C loss: 0.14369454897112316\n",
      "Group 74, Epoch 3: G loss: 0.214104337138789 vs. C loss: 0.20312932961516908\n",
      "Group 74, Epoch 4: G loss: 0.10732587959085192 vs. C loss: 0.18576910760667587\n",
      "Group 74, Epoch 5: G loss: 0.13037200484957015 vs. C loss: 0.14729731695519555\n",
      "Group 74, Epoch 6: G loss: 0.44505874259131295 vs. C loss: 0.11211845030387242\n",
      "Group 75, Epoch 1: G loss: 0.26280748077801297 vs. C loss: 0.17950642771191064\n",
      "Group 75, Epoch 2: G loss: 0.3303412816354206 vs. C loss: 0.10664357203576301\n",
      "Group 75, Epoch 3: G loss: 0.656661708014352 vs. C loss: 0.17816799382368723\n",
      "Group 75, Epoch 4: G loss: 0.3011886741433825 vs. C loss: 0.10383189552360111\n",
      "Group 75, Epoch 5: G loss: 0.8503189325332642 vs. C loss: 0.03828582622938686\n",
      "Group 75, Epoch 6: G loss: 0.6373220128672463 vs. C loss: 0.06822176339725654\n",
      "Group 76, Epoch 1: G loss: 0.5792642559323992 vs. C loss: 0.23165958788659838\n",
      "Group 76, Epoch 2: G loss: 0.3516812307494027 vs. C loss: 0.1186927639775806\n",
      "Group 76, Epoch 3: G loss: 0.6780890515872411 vs. C loss: 0.047350314963195056\n",
      "Group 76, Epoch 4: G loss: 0.4514974110892841 vs. C loss: 0.21135628140634963\n",
      "Group 76, Epoch 5: G loss: 0.03459142249609743 vs. C loss: 0.20613313714663187\n",
      "Group 76, Epoch 6: G loss: 0.02151898281382663 vs. C loss: 0.20660213629404703\n",
      "Group 77, Epoch 1: G loss: 0.15348238732133593 vs. C loss: 0.168649145298534\n",
      "Group 77, Epoch 2: G loss: 0.3555083960294723 vs. C loss: 0.1735422131088045\n",
      "Group 77, Epoch 3: G loss: 0.2269358043159757 vs. C loss: 0.2123066518041823\n",
      "Group 77, Epoch 4: G loss: 0.03553216872470719 vs. C loss: 0.20231238669819304\n",
      "Group 77, Epoch 5: G loss: 0.02963497612093176 vs. C loss: 0.1852600872516632\n",
      "Group 77, Epoch 6: G loss: 0.24168138929775781 vs. C loss: 0.14876484705342188\n",
      "Group 78, Epoch 1: G loss: 0.1959961380277361 vs. C loss: 0.21293594357040194\n",
      "Group 78, Epoch 2: G loss: 0.22538566057171142 vs. C loss: 0.18428375654750398\n",
      "Group 78, Epoch 3: G loss: 0.2012689747980663 vs. C loss: 0.18133319914340973\n",
      "Group 78, Epoch 4: G loss: 0.14851670414209367 vs. C loss: 0.21382571260134378\n",
      "Group 78, Epoch 5: G loss: 0.029501110687851904 vs. C loss: 0.1994874676068624\n",
      "Group 78, Epoch 6: G loss: 0.03363393130046981 vs. C loss: 0.19262759718630051\n",
      "Group 79, Epoch 1: G loss: 0.3877586262566703 vs. C loss: 0.21894583602746323\n",
      "Group 79, Epoch 2: G loss: 0.27661109992436 vs. C loss: 0.1965238120820787\n",
      "Group 79, Epoch 3: G loss: 0.08536447542054312 vs. C loss: 0.1825729873445299\n",
      "Group 79, Epoch 4: G loss: 0.1510239256279809 vs. C loss: 0.16415477461285063\n",
      "Group 79, Epoch 5: G loss: 0.3454730987548828 vs. C loss: 0.1766415536403656\n",
      "Group 79, Epoch 6: G loss: 0.18772941040141242 vs. C loss: 0.1949262155426873\n",
      "Group 80, Epoch 1: G loss: 0.3187520103795188 vs. C loss: 0.2329638045695093\n",
      "Group 80, Epoch 2: G loss: 0.27310136088303155 vs. C loss: 0.16624703341060212\n",
      "Group 80, Epoch 3: G loss: 0.3052029013633728 vs. C loss: 0.18552052477995554\n",
      "Group 80, Epoch 4: G loss: 0.17454869662012373 vs. C loss: 0.18214023609956106\n",
      "Group 80, Epoch 5: G loss: 0.16942661106586457 vs. C loss: 0.19079004724820456\n",
      "Group 80, Epoch 6: G loss: 0.15769154770033703 vs. C loss: 0.15204042692979178\n",
      "Group 81, Epoch 1: G loss: 0.3086677134037018 vs. C loss: 0.233861419889662\n",
      "Group 81, Epoch 2: G loss: 0.17797163746186667 vs. C loss: 0.17421721915404\n",
      "Group 81, Epoch 3: G loss: 0.15702906421252658 vs. C loss: 0.21144349210792115\n",
      "Group 81, Epoch 4: G loss: 0.09147474254880632 vs. C loss: 0.19739738023943373\n",
      "Group 81, Epoch 5: G loss: 0.057637575907366616 vs. C loss: 0.20122512347168395\n",
      "Group 81, Epoch 6: G loss: 0.02733393907546997 vs. C loss: 0.20319794449541306\n",
      "Group 82, Epoch 1: G loss: 0.2813024887016841 vs. C loss: 0.19240654011567435\n",
      "Group 82, Epoch 2: G loss: 0.21056188132081713 vs. C loss: 0.1914533757501178\n",
      "Group 82, Epoch 3: G loss: 0.120459210979087 vs. C loss: 0.20595232976807487\n",
      "Group 82, Epoch 4: G loss: 0.05485367881400244 vs. C loss: 0.19246747841437659\n",
      "Group 82, Epoch 5: G loss: 0.10137585635696138 vs. C loss: 0.18870404693815446\n",
      "Group 82, Epoch 6: G loss: 0.1705619264926229 vs. C loss: 0.19382784515619278\n",
      "Group 82, Epoch 7: G loss: 0.08984275885990688 vs. C loss: 0.18020100063747832\n",
      "Group 83, Epoch 1: G loss: 0.46100871818406247 vs. C loss: 0.25643059114615124\n",
      "Group 83, Epoch 2: G loss: 0.41962965812001907 vs. C loss: 0.17521356791257858\n",
      "Group 83, Epoch 3: G loss: 0.18248757421970369 vs. C loss: 0.17826983167065513\n",
      "Group 83, Epoch 4: G loss: 0.16491673482315883 vs. C loss: 0.19475871738460326\n",
      "Group 83, Epoch 5: G loss: 0.08340902690376553 vs. C loss: 0.19311451249652442\n",
      "Group 83, Epoch 6: G loss: 0.0965665693793978 vs. C loss: 0.18640938111477431\n",
      "Group 84, Epoch 1: G loss: 0.28214385722364704 vs. C loss: 0.2008144934144285\n",
      "Group 84, Epoch 2: G loss: 0.21195773759058542 vs. C loss: 0.19978017691108915\n",
      "Group 84, Epoch 3: G loss: 0.05788241443889481 vs. C loss: 0.19063375062412682\n",
      "Group 84, Epoch 4: G loss: 0.07270228064485958 vs. C loss: 0.1887898540331258\n",
      "Group 84, Epoch 5: G loss: 0.13834693282842636 vs. C loss: 0.18477784428331587\n",
      "Group 84, Epoch 6: G loss: 0.1549596418227468 vs. C loss: 0.18013611932595572\n",
      "Group 85, Epoch 1: G loss: 0.28021585004670285 vs. C loss: 0.16417139106326634\n",
      "Group 85, Epoch 2: G loss: 0.41631902796881537 vs. C loss: 0.1420722645190027\n",
      "Group 85, Epoch 3: G loss: 0.3319324144295283 vs. C loss: 0.21620418959193757\n",
      "Group 85, Epoch 4: G loss: 0.03798015979783875 vs. C loss: 0.20146352466609743\n",
      "Group 85, Epoch 5: G loss: 0.027631042578390667 vs. C loss: 0.19259109099706015\n",
      "Group 85, Epoch 6: G loss: 0.12343310373170036 vs. C loss: 0.16880333423614502\n",
      "Group 85, Epoch 7: G loss: 0.28865878837449216 vs. C loss: 0.18199280235502455\n",
      "Group 86, Epoch 1: G loss: 0.33215121712003437 vs. C loss: 0.19648663534058464\n",
      "Group 86, Epoch 2: G loss: 0.3262892127037048 vs. C loss: 0.19870262013541326\n",
      "Group 86, Epoch 3: G loss: 0.1083578766456672 vs. C loss: 0.1982261041800181\n",
      "Group 86, Epoch 4: G loss: 0.06199891684310777 vs. C loss: 0.18632885813713074\n",
      "Group 86, Epoch 5: G loss: 0.16366469732352668 vs. C loss: 0.1760149614678489\n",
      "Group 86, Epoch 6: G loss: 0.17166178311620442 vs. C loss: 0.17782682842678496\n",
      "Group 87, Epoch 1: G loss: 0.35078532184873307 vs. C loss: 0.24912655684683058\n",
      "Group 87, Epoch 2: G loss: 0.23787278958729335 vs. C loss: 0.1651177762283219\n",
      "Group 87, Epoch 3: G loss: 0.20618529511349545 vs. C loss: 0.20951365927855173\n",
      "Group 87, Epoch 4: G loss: 0.045223576469080796 vs. C loss: 0.20101934836970436\n",
      "Group 87, Epoch 5: G loss: 0.038867633257593424 vs. C loss: 0.18846685025427076\n",
      "Group 87, Epoch 6: G loss: 0.13874252992016928 vs. C loss: 0.16318043073018393\n",
      "Group 88, Epoch 1: G loss: 0.22163862415722435 vs. C loss: 0.1999121457338333\n",
      "Group 88, Epoch 2: G loss: 0.25148190345082966 vs. C loss: 0.17601508067713842\n",
      "Group 88, Epoch 3: G loss: 0.23749732119696482 vs. C loss: 0.19609586232238344\n",
      "Group 88, Epoch 4: G loss: 0.09049587228468488 vs. C loss: 0.18927850325902304\n",
      "Group 88, Epoch 5: G loss: 0.12323479098933085 vs. C loss: 0.20630283488167656\n",
      "Group 88, Epoch 6: G loss: 0.06401939040848187 vs. C loss: 0.19563794632752737\n",
      "Group 89, Epoch 1: G loss: 0.24531930344445366 vs. C loss: 0.22788468831115302\n",
      "Group 89, Epoch 2: G loss: 0.248576518041747 vs. C loss: 0.1903859939840105\n",
      "Group 89, Epoch 3: G loss: 0.12256761022976467 vs. C loss: 0.2158518781264623\n",
      "Group 89, Epoch 4: G loss: 0.03260846153966018 vs. C loss: 0.20489594837029776\n",
      "Group 89, Epoch 5: G loss: 0.015699013588683943 vs. C loss: 0.20199806160396996\n",
      "Group 89, Epoch 6: G loss: 0.03053285059119974 vs. C loss: 0.18902758094999528\n",
      "Group 90, Epoch 1: G loss: 0.3298255886350359 vs. C loss: 0.28276874456140727\n",
      "Group 90, Epoch 2: G loss: 0.3569985789912088 vs. C loss: 0.17001034650537702\n",
      "Group 90, Epoch 3: G loss: 0.259844959633691 vs. C loss: 0.1901806584662861\n",
      "Group 90, Epoch 4: G loss: 0.1364017354590552 vs. C loss: 0.21118046343326569\n",
      "Group 90, Epoch 5: G loss: 0.061513119616678776 vs. C loss: 0.19444560135404268\n",
      "Group 90, Epoch 6: G loss: 0.05943933012230056 vs. C loss: 0.18384685615698496\n",
      "Group 91, Epoch 1: G loss: 0.2540281576769693 vs. C loss: 0.23108942475583819\n",
      "Group 91, Epoch 2: G loss: 0.2649891265801021 vs. C loss: 0.1733445856306288\n",
      "Group 91, Epoch 3: G loss: 0.1285451203584671 vs. C loss: 0.2022905945777893\n",
      "Group 91, Epoch 4: G loss: 0.067418579437903 vs. C loss: 0.21763251887427434\n",
      "Group 91, Epoch 5: G loss: 0.05182868731873375 vs. C loss: 0.19905316912465623\n",
      "Group 91, Epoch 6: G loss: 0.051300786861351554 vs. C loss: 0.19102340771092308\n",
      "Group 92, Epoch 1: G loss: 0.34140464663505554 vs. C loss: 0.18547025488482582\n",
      "Group 92, Epoch 2: G loss: 0.2359415360859462 vs. C loss: 0.16317909873194167\n",
      "Group 92, Epoch 3: G loss: 0.3564268244164331 vs. C loss: 0.1625297251674864\n",
      "Group 92, Epoch 4: G loss: 0.3196347279208047 vs. C loss: 0.23744902842574647\n",
      "Group 92, Epoch 5: G loss: 0.07231277536068643 vs. C loss: 0.21058190531200835\n",
      "Group 92, Epoch 6: G loss: 0.04168772676161358 vs. C loss: 0.20466606401734885\n",
      "Group 93, Epoch 1: G loss: 0.12413953776870457 vs. C loss: 0.22055406785673562\n",
      "Group 93, Epoch 2: G loss: 0.13196482935122084 vs. C loss: 0.15907786041498184\n",
      "Group 93, Epoch 3: G loss: 0.2289339936205319 vs. C loss: 0.19745804535018074\n",
      "Group 93, Epoch 4: G loss: 0.09752440920897891 vs. C loss: 0.2111128866672516\n",
      "Group 93, Epoch 5: G loss: 0.018994219627763542 vs. C loss: 0.204939233760039\n",
      "Group 93, Epoch 6: G loss: 0.013365182120885168 vs. C loss: 0.20398665136761132\n",
      "Group 94, Epoch 1: G loss: 0.23180339421544757 vs. C loss: 0.19172477887736428\n",
      "Group 94, Epoch 2: G loss: 0.3097691110202244 vs. C loss: 0.17940206825733185\n",
      "Group 94, Epoch 3: G loss: 0.33518740449632917 vs. C loss: 0.20148689713742998\n",
      "Group 94, Epoch 4: G loss: 0.11135000288486484 vs. C loss: 0.2016882449388504\n",
      "Group 94, Epoch 5: G loss: 0.03756189921072551 vs. C loss: 0.20373204681608412\n",
      "Group 94, Epoch 6: G loss: 0.03163111321628093 vs. C loss: 0.19625615411334565\n",
      "Group 95, Epoch 1: G loss: 0.47547561100551056 vs. C loss: 0.22973539266321394\n",
      "Group 95, Epoch 2: G loss: 0.29270483766283306 vs. C loss: 0.1773044748438729\n",
      "Group 95, Epoch 3: G loss: 0.2522516476256507 vs. C loss: 0.15575819048616624\n",
      "Group 95, Epoch 4: G loss: 0.24202050545385909 vs. C loss: 0.20963622629642487\n",
      "Group 95, Epoch 5: G loss: 0.05579944221036775 vs. C loss: 0.19783204131656226\n",
      "Group 95, Epoch 6: G loss: 0.04897434530513628 vs. C loss: 0.18623389138115776\n",
      "Group 96, Epoch 1: G loss: 0.35819467306137087 vs. C loss: 0.1311481926176283\n",
      "Group 96, Epoch 2: G loss: 0.5581273913383483 vs. C loss: 0.06917548634939724\n",
      "Group 96, Epoch 3: G loss: 0.3640188212905611 vs. C loss: 0.21630144450399613\n",
      "Group 96, Epoch 4: G loss: 0.037510511598416736 vs. C loss: 0.2072556697660022\n",
      "Group 96, Epoch 5: G loss: 0.019766154007187915 vs. C loss: 0.20793340976039568\n",
      "Group 96, Epoch 6: G loss: 0.008676122900630745 vs. C loss: 0.20784281359778511\n",
      "Group 97, Epoch 1: G loss: 0.24884784689971381 vs. C loss: 0.19167960352367827\n",
      "Group 97, Epoch 2: G loss: 0.44922556451388773 vs. C loss: 0.1340335069431199\n",
      "Group 97, Epoch 3: G loss: 0.3102227568626405 vs. C loss: 0.2097731133302053\n",
      "Group 97, Epoch 4: G loss: 0.021037483374987332 vs. C loss: 0.20445885426468316\n",
      "Group 97, Epoch 5: G loss: 0.013385105026619775 vs. C loss: 0.20248207284344566\n",
      "Group 97, Epoch 6: G loss: 0.024762819600956783 vs. C loss: 0.19653265840477416\n",
      "Group 98, Epoch 1: G loss: 0.2857927250010626 vs. C loss: 0.2988653017414941\n",
      "Group 98, Epoch 2: G loss: 0.33963056973048616 vs. C loss: 0.1820342375172509\n",
      "Group 98, Epoch 3: G loss: 0.1649012667792184 vs. C loss: 0.18061842396855354\n",
      "Group 98, Epoch 4: G loss: 0.14296975114515847 vs. C loss: 0.19300750891367593\n",
      "Group 98, Epoch 5: G loss: 0.08361610450914927 vs. C loss: 0.1947916564014223\n",
      "Group 98, Epoch 6: G loss: 0.0868220953004701 vs. C loss: 0.18080067965719437\n",
      "Group 99, Epoch 1: G loss: 0.38286709615162445 vs. C loss: 0.2183453804916806\n",
      "Group 99, Epoch 2: G loss: 0.2731436001402991 vs. C loss: 0.16504041684998408\n",
      "Group 99, Epoch 3: G loss: 0.2651420657123838 vs. C loss: 0.1396624876393212\n",
      "Group 99, Epoch 4: G loss: 0.3715294152498245 vs. C loss: 0.21619504855738747\n",
      "Group 99, Epoch 5: G loss: 0.0853792852056878 vs. C loss: 0.2108659810490078\n",
      "Group 99, Epoch 6: G loss: 0.018968507248376097 vs. C loss: 0.20523700449201798\n",
      "Group 100, Epoch 1: G loss: 0.35289023518562324 vs. C loss: 0.2229778178864055\n",
      "Group 100, Epoch 2: G loss: 0.3196812987327576 vs. C loss: 0.16476148780849245\n",
      "Group 100, Epoch 3: G loss: 0.19980018053736007 vs. C loss: 0.13364047308762866\n",
      "Group 100, Epoch 4: G loss: 0.37385103787694657 vs. C loss: 0.16373143841822943\n",
      "Group 100, Epoch 5: G loss: 0.27814113370009835 vs. C loss: 0.21031741301218668\n",
      "Group 100, Epoch 6: G loss: 0.060335084795951836 vs. C loss: 0.19916591379377577\n",
      "Group 101, Epoch 1: G loss: 0.3401814869471959 vs. C loss: 0.21511710352367827\n",
      "Group 101, Epoch 2: G loss: 0.2829338048185621 vs. C loss: 0.14990816430913076\n",
      "Group 101, Epoch 3: G loss: 0.48638449353831154 vs. C loss: 0.1461424297756619\n",
      "Group 101, Epoch 4: G loss: 0.25259148840393336 vs. C loss: 0.22282227956586417\n",
      "Group 101, Epoch 5: G loss: 0.03620750520910536 vs. C loss: 0.2065570114387406\n",
      "Group 101, Epoch 6: G loss: 0.02647769110543387 vs. C loss: 0.20722661001814738\n",
      "Group 102, Epoch 1: G loss: 0.1726262424673353 vs. C loss: 0.16035724886589578\n",
      "Group 102, Epoch 2: G loss: 0.3152298684631075 vs. C loss: 0.16333733830187055\n",
      "Group 102, Epoch 3: G loss: 0.5626086754458292 vs. C loss: 0.11179753765463829\n",
      "Group 102, Epoch 4: G loss: 0.4017363467386791 vs. C loss: 0.18950075407822928\n",
      "Group 102, Epoch 5: G loss: 0.10215893185564451 vs. C loss: 0.20477908642755613\n",
      "Group 102, Epoch 6: G loss: 0.05723364810858455 vs. C loss: 0.19510980240172812\n",
      "Group 103, Epoch 1: G loss: 0.18947477191686632 vs. C loss: 0.20215723249647352\n",
      "Group 103, Epoch 2: G loss: 0.17042495565755025 vs. C loss: 0.19262247118684983\n",
      "Group 103, Epoch 3: G loss: 0.2454577718462263 vs. C loss: 0.10305871607528792\n",
      "Group 103, Epoch 4: G loss: 0.41994455329009467 vs. C loss: 0.20370019475618997\n",
      "Group 103, Epoch 5: G loss: 0.04290593553866658 vs. C loss: 0.20544674464811882\n",
      "Group 103, Epoch 6: G loss: 0.032436832359858926 vs. C loss: 0.2000027671456337\n",
      "Group 104, Epoch 1: G loss: 0.39420860580035616 vs. C loss: 0.23830293864011765\n",
      "Group 104, Epoch 2: G loss: 0.35110086883817404 vs. C loss: 0.195387065410614\n",
      "Group 104, Epoch 3: G loss: 0.11426178131784712 vs. C loss: 0.18321319917837778\n",
      "Group 104, Epoch 4: G loss: 0.0815484509936401 vs. C loss: 0.19433756503793928\n",
      "Group 104, Epoch 5: G loss: 0.11027350425720216 vs. C loss: 0.18508263015084794\n",
      "Group 104, Epoch 6: G loss: 0.15083218812942503 vs. C loss: 0.17249436055620512\n",
      "Group 105, Epoch 1: G loss: 0.39677939159529557 vs. C loss: 0.21522424949540034\n",
      "Group 105, Epoch 2: G loss: 0.29103207886219024 vs. C loss: 0.13909190975957444\n",
      "Group 105, Epoch 3: G loss: 0.42934283358710157 vs. C loss: 0.11492200940847397\n",
      "Group 105, Epoch 4: G loss: 0.43690521972520013 vs. C loss: 0.188479440079795\n",
      "Group 105, Epoch 5: G loss: 0.2351361985717501 vs. C loss: 0.2046785205602646\n",
      "Group 105, Epoch 6: G loss: 0.08611040136643819 vs. C loss: 0.21947227749559614\n",
      "Group 106, Epoch 1: G loss: 0.27907848613602776 vs. C loss: 0.1637591587172614\n",
      "Group 106, Epoch 2: G loss: 0.3003025961773736 vs. C loss: 0.16795845495329964\n",
      "Group 106, Epoch 3: G loss: 0.23915524567876542 vs. C loss: 0.15945746666855282\n",
      "Group 106, Epoch 4: G loss: 0.23601901318345747 vs. C loss: 0.22712646755907273\n",
      "Group 106, Epoch 5: G loss: 0.02668633402458259 vs. C loss: 0.20827419062455496\n",
      "Group 106, Epoch 6: G loss: 0.012709003128111362 vs. C loss: 0.20574299742778143\n",
      "Group 107, Epoch 1: G loss: 0.2284470174993787 vs. C loss: 0.1734005825387107\n",
      "Group 107, Epoch 2: G loss: 0.3641828622136797 vs. C loss: 0.18367032210032144\n",
      "Group 107, Epoch 3: G loss: 0.15304894894361495 vs. C loss: 0.20745808051692116\n",
      "Group 107, Epoch 4: G loss: 0.04549514321344239 vs. C loss: 0.19517690936724344\n",
      "Group 107, Epoch 5: G loss: 0.06476546472736767 vs. C loss: 0.1836263289054235\n",
      "Group 107, Epoch 6: G loss: 0.190976011965956 vs. C loss: 0.17746507128079733\n",
      "Group 108, Epoch 1: G loss: 0.5639339974948337 vs. C loss: 0.2722204162014855\n",
      "Group 108, Epoch 2: G loss: 0.40810230885233195 vs. C loss: 0.16644194722175598\n",
      "Group 108, Epoch 3: G loss: 0.29253425768443514 vs. C loss: 0.20698999365170798\n",
      "Group 108, Epoch 4: G loss: 0.07805701898677007 vs. C loss: 0.19591482811503938\n",
      "Group 108, Epoch 5: G loss: 0.05416989954454558 vs. C loss: 0.1920469237698449\n",
      "Group 108, Epoch 6: G loss: 0.0688895281936441 vs. C loss: 0.18373640543884703\n",
      "Group 109, Epoch 1: G loss: 0.3067260269607816 vs. C loss: 0.22670818699730766\n",
      "Group 109, Epoch 2: G loss: 0.31706381908484865 vs. C loss: 0.15942825211419\n",
      "Group 109, Epoch 3: G loss: 0.2794838224138532 vs. C loss: 0.1422202852037218\n",
      "Group 109, Epoch 4: G loss: 0.4464092352560588 vs. C loss: 0.2511581629514694\n",
      "Group 109, Epoch 5: G loss: 0.0636539957353047 vs. C loss: 0.20720408292901185\n",
      "Group 109, Epoch 6: G loss: 0.02241709509066173 vs. C loss: 0.2052885302238994\n",
      "Group 110, Epoch 1: G loss: 0.4551955154963902 vs. C loss: 0.2536173810561498\n",
      "Group 110, Epoch 2: G loss: 0.33006262608936854 vs. C loss: 0.15314796566963196\n",
      "Group 110, Epoch 3: G loss: 0.2102532024894442 vs. C loss: 0.16736317674318948\n",
      "Group 110, Epoch 4: G loss: 0.3143340545041221 vs. C loss: 0.22028728905651304\n",
      "Group 110, Epoch 5: G loss: 0.04974013715982438 vs. C loss: 0.20437253680494097\n",
      "Group 110, Epoch 6: G loss: 0.026506363813366213 vs. C loss: 0.20164693478080964\n",
      "Group 111, Epoch 1: G loss: 0.2995962194034032 vs. C loss: 0.2009773544139332\n",
      "Group 111, Epoch 2: G loss: 0.19048936920506612 vs. C loss: 0.16519593199094137\n",
      "Group 111, Epoch 3: G loss: 0.3471132912806102 vs. C loss: 0.19114359385437438\n",
      "Group 111, Epoch 4: G loss: 0.1295100645295211 vs. C loss: 0.2044846730099784\n",
      "Group 111, Epoch 5: G loss: 0.037343995485986985 vs. C loss: 0.19807960672510994\n",
      "Group 111, Epoch 6: G loss: 0.04923098949449404 vs. C loss: 0.19208778109815386\n",
      "Group 112, Epoch 1: G loss: 0.13089812014784133 vs. C loss: 0.17882071104314592\n",
      "Group 112, Epoch 2: G loss: 0.19458433602537425 vs. C loss: 0.1719590193695492\n",
      "Group 112, Epoch 3: G loss: 0.24935281127691272 vs. C loss: 0.2156813641389211\n",
      "Group 112, Epoch 4: G loss: 0.05319893871034894 vs. C loss: 0.20370457983679244\n",
      "Group 112, Epoch 5: G loss: 0.03137859255075455 vs. C loss: 0.19665195047855377\n",
      "Group 112, Epoch 6: G loss: 0.08593441247940063 vs. C loss: 0.15118506716357336\n",
      "Group 113, Epoch 1: G loss: 0.31213496497699195 vs. C loss: 0.20187711384561327\n",
      "Group 113, Epoch 2: G loss: 0.27268767867769517 vs. C loss: 0.1495883365472158\n",
      "Group 113, Epoch 3: G loss: 0.32406610505921496 vs. C loss: 0.14024709744585886\n",
      "Group 113, Epoch 4: G loss: 0.3651665091514587 vs. C loss: 0.2061152681708336\n",
      "Group 113, Epoch 5: G loss: 0.056062841202531535 vs. C loss: 0.20372686949041155\n",
      "Group 113, Epoch 6: G loss: 0.05249987585203988 vs. C loss: 0.19046783364481398\n",
      "Group 114, Epoch 1: G loss: 0.20423009182725638 vs. C loss: 0.1747317798435688\n",
      "Group 114, Epoch 2: G loss: 0.23386271979127612 vs. C loss: 0.16536368098523882\n",
      "Group 114, Epoch 3: G loss: 0.3351275656904492 vs. C loss: 0.14283908324109185\n",
      "Group 114, Epoch 4: G loss: 0.2661009301032339 vs. C loss: 0.23134079741107094\n",
      "Group 114, Epoch 5: G loss: 0.04241780415177345 vs. C loss: 0.1949293845229679\n",
      "Group 114, Epoch 6: G loss: 0.08772507948534829 vs. C loss: 0.15064946396483317\n",
      "Group 115, Epoch 1: G loss: 0.29756193033286504 vs. C loss: 0.23245719075202942\n",
      "Group 115, Epoch 2: G loss: 0.24006764250142232 vs. C loss: 0.16822259417838523\n",
      "Group 115, Epoch 3: G loss: 0.3673342709030424 vs. C loss: 0.14874864286846584\n",
      "Group 115, Epoch 4: G loss: 0.2848001033067703 vs. C loss: 0.2190090301964018\n",
      "Group 115, Epoch 5: G loss: 0.01901794597506523 vs. C loss: 0.2065950632095337\n",
      "Group 115, Epoch 6: G loss: 0.005447231233119965 vs. C loss: 0.20654012428389656\n"
     ]
    }
   ],
   "source": [
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer, add the start pos\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.002)\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "class PrintCross(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('x', end='')\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, GAN_PARAMS[\"g_input_size\"]));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    \n",
    "    classifier_model = build_classifier_model()\n",
    "    extvar[\"classifier_model\"] = classifier_model;\n",
    "    \n",
    "    def loss_function_for_generative_model(y_true, y_pred):\n",
    "        return construct_map_and_calc_loss(y_pred, extvar);\n",
    "    \n",
    "#     classifier_true_set_group = special_train_data[np.random.randint(0, special_train_data.shape[0], (500,))];\n",
    "    \n",
    "#     classifier_model.summary()\n",
    "#     gmodel.summary()\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    max_epoch = GAN_PARAMS[\"max_epoch\"];\n",
    "    good_epoch = GAN_PARAMS[\"good_epoch\"] - 1;\n",
    "    g_multiplier = GAN_PARAMS[\"g_epochs\"];\n",
    "    c_multiplier = GAN_PARAMS[\"c_epochs\"];\n",
    "    g_batch = GAN_PARAMS[\"g_batch\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    c_true_batch = GAN_PARAMS[\"c_true_batch\"]\n",
    "    c_false_batch = GAN_PARAMS[\"c_false_batch\"]\n",
    "    \n",
    "    gmodel = generative_model(g_input_size, note_group_size * 4, loss_function_for_generative_model);\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        gnoise = np.random.random((g_batch, g_input_size));\n",
    "        glabel = np.ones((g_batch,))\n",
    "\n",
    "        history = gmodel.fit(gnoise, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintDot()\n",
    "        \n",
    "        predicted_maps_data = gmodel.predict(np.random.random((c_false_batch, g_input_size)));\n",
    "        new_false_maps = construct_map_with_sliders(tf.convert_to_tensor(predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "        new_false_labels = np.zeros(c_false_batch);\n",
    "        \n",
    "#         not_predicted_maps_data = np.random.random((10, 40));\n",
    "    #     new_false_maps2 = construct_map_with_sliders(tf.convert_to_tensor(not_predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "    #     new_false_labels2 = np.zeros(10);\n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (c_true_batch,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "    #     actual_train_data = special_train_data[st:se];\n",
    "    #     actual_train_labels = special_train_labels[st:se];\n",
    "\n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintCross()\n",
    "\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        res = gmodel.predict(plot_noise);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        if i >= good_epoch:\n",
    "            current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # it's the same anyways\n",
    "            res = gmodel.predict(np.random.random((1, g_input_size)));\n",
    "#         print(construct_map_with_sliders(tf.convert_to_tensor(res), extvar=extvar).numpy().squeeze());\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "\n",
    "    onoise = np.random.random((1, g_input_size));\n",
    "    \n",
    "    return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    for i in range(timestamps.shape[0] // note_group_size):\n",
    "        z = generate_set(begin = i * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    generate_set(begin = 3 * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success! finished on: 2018-08-18 00:27:04.026041\n"
     ]
    }
   ],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@kotri_lv204 / ar3sgice, 2018/8/16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
