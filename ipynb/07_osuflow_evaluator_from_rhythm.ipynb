{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* ~~sliderData x 1~~\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Final edit: 2018/8/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"//127.0.0.1/ll/surface/SophieBG.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "GAN is, kind of, pretty hard to train; and I personally felt the pain when I downloaded 10+ github repos where no one worked. While all of those code are based on MNIST, they either do not train any image that looks good, or simply fail to run.\n",
    "\n",
    "As a consequence, I coded this notebook using tf.contrib.eager and tf.keras - took a while to find out how to modify that loss function. The biggest obstacle was that all the tensors are one dimension higher than the data, which was the batch (first dimension).\n",
    "\n",
    "tf.contrib.eager is also said to be a pretty new feature in Tensorflow. ~~Idk how to write the other style of code using sessions without getting error anyways,~~ so make sure Tensorflow has the right version. My env is tensorflow v1.9.0. On Win10, no cuda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "3LXMVuV0VhDr"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this line below can only run once in the notebook! otherwise it will cause errors\n",
    "tf.enable_eager_execution();\n",
    "tfe = tf.contrib.eager;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, sv = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + 5: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "\n",
    "# IMPORTANT!! we need this data!!!\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // 4;\n",
    "\n",
    "slider_types = np.randint(0, 3, is_slider.shape).astype(int); # needs to determine the slider types!! also is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]) * slider_length_base;\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] for k in slider_types]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)\n",
    "\n",
    "# ...tired hahaha do it later\n",
    "# use NOTE DENSITY to determine the critical region for IS_NOTE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "timestamps_after = timestamps_plus_1 - timestamps;\n",
    "timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "\n",
    "note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib.text as mtext\n",
    "\n",
    "\n",
    "class MyLine(lines.Line2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # we'll update the position when the line data is set\n",
    "        self.text = mtext.Text(0, 0, '')\n",
    "        lines.Line2D.__init__(self, *args, **kwargs)\n",
    "\n",
    "        # we can't access the label attr until *after* the line is\n",
    "        # inited\n",
    "        self.text.set_text(self.get_label())\n",
    "\n",
    "    def set_figure(self, figure):\n",
    "        self.text.set_figure(figure)\n",
    "        lines.Line2D.set_figure(self, figure)\n",
    "\n",
    "    def set_axes(self, axes):\n",
    "        self.text.set_axes(axes)\n",
    "        lines.Line2D.set_axes(self, axes)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        # 2 pixel offset\n",
    "        texttrans = transform + mtransforms.Affine2D().translate(2, 2)\n",
    "        self.text.set_transform(texttrans)\n",
    "        lines.Line2D.set_transform(self, transform)\n",
    "\n",
    "    def set_data(self, x, y):\n",
    "        if len(x):\n",
    "            self.text.set_position((x[-1], y[-1]))\n",
    "\n",
    "        lines.Line2D.set_data(self, x, y)\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        # draw my label at the end of the line with 2 pixel offset\n",
    "        lines.Line2D.draw(self, renderer)\n",
    "        self.text.draw(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import numpy as np;\n",
    "\n",
    "root = \".\\\\mapdata\";\n",
    "\n",
    "chunk_size = 10;\n",
    "step_size = 5;\n",
    "divisor = 4;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# def construct_random_map():\n",
    "#     var_tensor = np.random.random(size=(20,));\n",
    "#     out = [];\n",
    "#     cp = np.array([0.5, 0.5, 0, 0]);\n",
    "#     l = 0.3;\n",
    "#     sl = 0.1;\n",
    "#     phase = 0.6;\n",
    "#     cos_list = np.cos(var_tensor * 6.283);\n",
    "#     sin_list = np.sin(var_tensor * 6.283);\n",
    "#     cos_list2 = np.cos(var_tensor * 6.283 + phase);\n",
    "#     sin_list2 = np.sin(var_tensor * 6.283 + phase);\n",
    "#     half_tensor = var_tensor.shape[0]//2;\n",
    "#     _px = 0.5;\n",
    "#     _py = 0.5;\n",
    "#     for k in range(half_tensor):\n",
    "#         _x = _px + l*cos_list[k];\n",
    "#         _y = _py + l*sin_list[k];\n",
    "#         _a = sl*cos_list2[k + half_tensor];\n",
    "#         _b = sl*sin_list2[k + half_tensor];\n",
    "#         _x += _a;\n",
    "#         _y += _b\n",
    "#         cp = np.array([_x, _y, _a, _b]);\n",
    "#         out.append(cp);\n",
    "#         _px = _x;\n",
    "#         _py = _y;\n",
    "#     return np.array(out);\n",
    "\n",
    "# def construct_random_map_n(n):\n",
    "#     out = [];\n",
    "#     for i in range(n):\n",
    "#         out.append(construct_random_map());\n",
    "#     return np.array(out);\n",
    "\n",
    "maps = read_maps();\n",
    "labels = np.ones(maps.shape[0]);\n",
    "# false_maps = construct_random_map_n(maps.shape[0] * 5);\n",
    "# false_labels = np.zeros(maps.shape[0] * 5);\n",
    "\n",
    "# Mix true and false maps\n",
    "# train_data = np.concatenate((maps, false_maps), axis=0);\n",
    "# train_labels = np.concatenate((labels, false_labels), axis=0);\n",
    "\n",
    "# Shuffle the training set\n",
    "# order = np.argsort(np.random.random(train_labels.shape[0]));\n",
    "# train_data = train_data[order];\n",
    "# train_labels = train_labels[order];\n",
    "# test_data = construct_random_map_n(maps.shape[0]);\n",
    "# test_labels = np.zeros(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define the classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define some sort of plotting functions! so we can preview the map - with followlines only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Limitless]')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# actual_train_data = np.concatenate((special_train_data[0:1], special_false_data[0:1]), axis=0);\n",
    "# actual_train_labels = np.concatenate((special_train_labels[0:1], special_false_labels[0:1]), axis=0);\n",
    "\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "eMAWbDJFVmMk"
   },
   "source": [
    "## Variables\n",
    "\n",
    "Tensors in TensorFlow are immutable stateless objects. Machine learning models, however, need to have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!). To represent this state which needs to change over the course of your computation, you can choose to rely on the fact that Python is a stateful programming language:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "VkJwtLS_Jbn8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.47145066  0.2319202   0.88674736  0.77331001  0.28124741  0.9694159\n",
      "    0.36926863  0.77961349]\n",
      "  [ 0.38939032  0.99535179  0.53037614  0.39196023  0.95319253  0.04592647\n",
      "    0.51032066  0.87815326]\n",
      "  [ 0.90134883  0.77904886  0.39886186  0.36319393  0.80995166  0.04369746\n",
      "    0.44056538  0.83942652]\n",
      "  [ 0.26945361  0.65242809  0.81714237  0.01456681  0.82101679  0.91126525\n",
      "    0.3431488   0.48718137]\n",
      "  [ 0.96114779  0.0420151   0.92596442  0.59074914  0.17783387  0.86402893\n",
      "    0.00597308  0.01951672]\n",
      "  [ 0.76530033  0.23419939  0.98889595  0.38912109  0.70416331  0.02727577\n",
      "    0.75117743  0.09735746]\n",
      "  [ 0.53189266  0.87493044  0.36968052  0.82487971  0.14956352  0.9699108\n",
      "    0.05209237  0.25745264]\n",
      "  [ 0.58195287  0.90211433  0.856489    0.46655422  0.07549961  0.32761398\n",
      "    0.08938725  0.86323071]\n",
      "  [ 0.17696761  0.53798211  0.22035123  0.35556069  0.20982566  0.27248114\n",
      "    0.53860217  0.5757488 ]\n",
      "  [ 0.82129955  0.43099105  0.22317803  0.57468587  0.34904191  0.16391018\n",
      "    0.40298077  0.05825501]]], shape=(1, 10, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np;\n",
    "\n",
    "# Using python state\n",
    "x = tf.convert_to_tensor(np.random.rand(1, 10, 8), dtype='float32')\n",
    "#x += 2  # This is equivalent to x = x + 2, which does not mutate the original\n",
    "        # value of x\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "wfneTXy7JcUz"
   },
   "source": [
    "TensorFlow, however, has stateful operations built in, and these are often more pleasant to use than low-level Python representations of your state. To represent weights in a model, for example, it's often convenient and efficient to use TensorFlow variables.\n",
    "\n",
    "A Variable is an object which stores a value and, when used in a TensorFlow computation, will implicitly read from this stored value. There are operations (`tf.assign_sub`, `tf.scatter_update`, etc) which manipulate the value stored in a TensorFlow variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "itxmrMil6DQi"
   },
   "outputs": [],
   "source": [
    "v = tfe.Variable(1.0)\n",
    "assert v.numpy() == 1.0\n",
    "\n",
    "# Re-assign the value\n",
    "v.assign(3.0)\n",
    "assert v.numpy() == 3.0\n",
    "\n",
    "# Use `v` in a TensorFlow operation like tf.square() and reassign\n",
    "v.assign(tf.square(v))\n",
    "assert v.numpy() == 9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-paSaeq1JzwC"
   },
   "source": [
    "Computations using Variables are automatically traced when computing gradients. For Variables representing embeddings TensorFlow will do sparse updates by default, which are more computation and memory efficient.\n",
    "\n",
    "Using Variables is also a way to quickly let a reader of your code know that this piece of state is mutable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "BMiFcDzE7Qu3"
   },
   "source": [
    "## Example: Fitting a linear model\n",
    "\n",
    "Let's now put the few concepts we have so far ---`Tensor`, `GradientTape`, `Variable` --- to build and train a simple model. This typically involves a few steps:\n",
    "\n",
    "1. Define the model.\n",
    "2. Define a loss function.\n",
    "3. Obtain training data.\n",
    "4. Run through the training data and use an \"optimizer\" to adjust the variables to fit the data.\n",
    "\n",
    "In this tutorial, we'll walk through a trivial example of a simple linear model: `f(x) = x * W + b`, which has two variables - `W` and `b`. Furthermore, we'll synthesize data such that a well trained model would have `W = 3.0` and `b = 2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "gFzH64Jn9PIm"
   },
   "source": [
    "### Define the model\n",
    "\n",
    "Let's define a simple class to encapsulate the variables and the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "_WRu7Pze7wk8"
   },
   "outputs": [],
   "source": [
    "# class Model(object):\n",
    "#     def __init__(self):\n",
    "#         # Initialize variable to (5.0, 0.0)\n",
    "#         # In practice, these should be initialized to random values.\n",
    "#         self.W = tfe.Variable(5.0)\n",
    "#         self.b = tfe.Variable(0.0)\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         return self.W * x + self.b\n",
    "\n",
    "# model = Model()\n",
    "\n",
    "#assert model(3.0).eval() == 15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "xa6j_yXa-j79"
   },
   "source": [
    "### Define a loss function\n",
    "\n",
    "A loss function measures how well the output of a model for a given input matches the desired output. Let's use the standard L2 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Y0ysUFGY924U"
   },
   "outputs": [],
   "source": [
    "# def loss(predicted_y, desired_y):\n",
    "#       return tf.reduce_mean(tf.square(predicted_y - desired_y)) + 16 * tf.reduce_mean(tf.to_float(tf.greater(predicted_y, 1)) * tf.to_float(tf.less(predicted_y, 3)) *(1 - tf.square(predicted_y - 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "### Obtain training data\n",
    "\n",
    "Let's synthesize the training data with some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "# TRUE_W = 3.0\n",
    "# TRUE_b = 2.0\n",
    "# NUM_EXAMPLES = 1000\n",
    "\n",
    "# inputs  = tf.random_normal(shape=[NUM_EXAMPLES])\n",
    "# noise   = tf.random_normal(shape=[NUM_EXAMPLES])\n",
    "# outputs = inputs * TRUE_W + TRUE_b + noise\n",
    "\n",
    "def inblock_loss(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0.2)) * tf.square(0.3 - vg);\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 0.8)) * tf.square(vg - 0.7);\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0));\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 1));\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map(var_tensor):\n",
    "    out = [];\n",
    "    cp = tf.constant([0.5, 0.5]);\n",
    "    l = 0.3;\n",
    "    cos_list = l* tf.cos(var_tensor * 6.283);\n",
    "    sin_list = l* tf.sin(var_tensor * 6.283);\n",
    "    for k, _ in enumerate(cos_list):\n",
    "        cp = tf.add(cp, tf.stack([cos_list[k], sin_list[k]]));\n",
    "        out.append(cp);\n",
    "    return tf.stack(out, axis=0);\n",
    "\n",
    "### now editing this!!\n",
    "def construct_map_without_sliders(var_tensor, extvar={}):\n",
    "    \n",
    "    var_tensor = tf.to_float(var_tensor);\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_tensor.shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    batch_size = var_tensor.shape[0];\n",
    "    note_distances_now = length_multiplier * np.tile(np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "    note_angles_now = np.tile(np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "#     ntensor_list = ((var_tensor - 0.5)) * 2 * 6.283;\n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "#     cos_list2 = tf.cos(var_tensor * 6.283 + phase);\n",
    "#     sin_list2 = tf.sin(var_tensor * 6.283 + phase);\n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.to_float(tf.greater(l, y_max / 2));\n",
    "    not_rerand = tf.to_float(tf.less_equal(l, y_max / 2));\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _px, _py = extvar[\"start_pos\"];\n",
    "    else:\n",
    "        _px = 256;\n",
    "        _py = 192;\n",
    "    _pa = 0;\n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = 256;\n",
    "    _y = 192;\n",
    "    for k in range(half_tensor):\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        wall_value_l = tf.to_float(tf.less(_px, wall_l[:, k]));\n",
    "        wall_value_r = tf.to_float(tf.greater(_px, wall_r[:, k]));\n",
    "        wall_value_xmid = tf.to_float(tf.greater(_px, wall_l[:, k])) * tf.to_float(tf.less(_px, wall_r[:, k]));\n",
    "        wall_value_t = tf.to_float(tf.less(_py, wall_t[:, k]));\n",
    "        wall_value_b = tf.to_float(tf.greater(_py, wall_b[:, k]));\n",
    "        wall_value_ymid = tf.to_float(tf.greater(_py, wall_t[:, k])) * tf.to_float(tf.less(_py, wall_b[:, k]));\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "        _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "        cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b]));\n",
    "        out.append(cp);\n",
    "        _px = _x;\n",
    "        _py = _y;\n",
    "        _pa = tf.mod(_pa + 0, 6.283);\n",
    "    return tf.transpose(tf.stack(out, axis=0), [1, 0, 2]);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "\n",
    "    var_tensor = tf.to_float(var_tensor);\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_tensor.shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    batch_size = var_tensor.shape[0];\n",
    "    note_distances_now = length_multiplier * np.tile(np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "    note_angles_now = np.tile(np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "#     ntensor_list = ((var_tensor - 0.5)) * 2 * 6.283;\n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "#     cos_list2 = tf.cos(var_tensor * 6.283 + phase);\n",
    "#     sin_list2 = tf.sin(var_tensor * 6.283 + phase);\n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.to_float(tf.greater(l, y_max / 2));\n",
    "    not_rerand = tf.to_float(tf.less_equal(l, y_max / 2));\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _px, _py = extvar[\"start_pos\"];\n",
    "    else:\n",
    "        _px = 256;\n",
    "        _py = 192;\n",
    "    _pa = 0;\n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = 256;\n",
    "    _y = 192;\n",
    "    for k in range(half_tensor):\n",
    "        note_index = begin_offset + k;\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        wall_value_l = tf.to_float(tf.less(_px, wall_l[:, k]));\n",
    "        wall_value_r = tf.to_float(tf.greater(_px, wall_r[:, k]));\n",
    "        wall_value_xmid = tf.to_float(tf.greater(_px, wall_l[:, k])) * tf.to_float(tf.less(_px, wall_r[:, k]));\n",
    "        wall_value_t = tf.to_float(tf.less(_py, wall_t[:, k]));\n",
    "        wall_value_b = tf.to_float(tf.greater(_py, wall_b[:, k]));\n",
    "        wall_value_ymid = tf.to_float(tf.greater(_py, wall_t[:, k])) * tf.to_float(tf.less(_py, wall_b[:, k]));\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        if is_slider[note_index]:\n",
    "            sln = slider_lengths[note_index];\n",
    "            slider_type = slider_types[note_index];\n",
    "            scos = slider_cos[slider_type];\n",
    "            ssin = slider_sin[slider_type];\n",
    "            _a = cos_list[:, k + half_tensor];\n",
    "            _b = sin_list[:, k + half_tensor];\n",
    "            # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "            # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "            _oa = _a * scos - _b * ssin;\n",
    "            _ob = _a * ssin + _b * scos;\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x + _a * sln;\n",
    "            _py = _y + _b * sln;\n",
    "        else:\n",
    "            _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "            _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x;\n",
    "            _py = _y;\n",
    "        _pa = tf.mod(_pa + 0, 6.283);\n",
    "    return tf.transpose(tf.stack(out, axis=0), [1, 0, 2]);\n",
    "\n",
    "def stack_loss(tensor):\n",
    "    complex_list = tf.complex(tensor[:, :, 0] * 512, tensor[:, :, 1] * 384);\n",
    "    stack_limit = 30;\n",
    "    precise_limit = 1;\n",
    "    a = [];\n",
    "    for k in range(tensor.shape[1]):\n",
    "        w = tf.tile(tf.expand_dims(complex_list[:, k], axis=1), [1, tensor.shape[1]]);\n",
    "        r = tf.abs(w - complex_list);\n",
    "        rless = tf.to_float(tf.less(r, stack_limit)) * tf.to_float(tf.greater(r, precise_limit));\n",
    "        rmean = tf.reduce_mean(rless * (stack_limit - r) / stack_limit);\n",
    "        a.append(rmean);\n",
    "    b = tf.reduce_sum(a);\n",
    "#         print(tf.tile(w, [1, tensor.shape[1]]));\n",
    "#         print(complex_list);\n",
    "    return b;\n",
    "\n",
    "def polygon_loss(tensor):\n",
    "    tensor_this = tensor[:, :, 0:2];\n",
    "    tensor_next = tf.concat([tensor[:, 1:, 0:2], tensor[:, 0:1, 0:2]], axis=1);\n",
    "    sa = (tensor_this[:, :, 0] + tensor_next[:, :, 0]) * (tensor_next[:, :, 1] - tensor_this[:, :, 0]);\n",
    "    surface = tf.abs(tf.reduce_sum(sa, axis=1))/2;\n",
    "    return surface;\n",
    "\n",
    "def construct_map_and_calc_loss(var_tensor, extvar):\n",
    "    ## edited!!!\n",
    "    classifier_model = extvar[\"classifier_model\"]\n",
    "    out = construct_map_with_sliders(var_tensor, extvar=extvar);\n",
    "    cm = classifier_model(out);\n",
    "    predmean = 1 - tf.reduce_mean(cm, axis=1);\n",
    "#    regulator = tf.reduce_mean(tf.reduce_mean(- 0.1 * tf.square(out[:, :, 1:2] - 0.5), axis=2), axis=1); # * tf.square(out[:, :, 1:2] - 2)\n",
    "    box_loss = inblock_loss(out[:, :, 0:2]);\n",
    "    box_loss2 = inblock_loss(out[:, :, 4:6]);\n",
    "#     polygon = polygon_loss(out);\n",
    "    # print(out.shape); shape is (10, X, 4)\n",
    "    #return predmean + box_loss*100;\n",
    "    return predmean + box_loss + box_loss2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "OXH_MYdvj_7u"
   },
   "source": [
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "z1wuX09njf9r"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# vvm = tfe.Variable([6,6,6])\n",
    "# print(np.array(list(range(vvm.shape[0],0,-1)))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-50nq-wPBsAW"
   },
   "source": [
    "Before we train the model let's visualize where the model stands right now. We'll plot the model's predictions in red and the training data in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "_eb83LtrB4nt"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.scatter(inputs, outputs, c='b')\n",
    "#plt.scatter(inputs, model(inputs), c='r')\n",
    "#plt.show()\n",
    "\n",
    "# print('Current loss: 9,999,999,999')\n",
    "# #print(construct_map_and_calc_loss(inputs, []).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sSDP-yeq_4jE"
   },
   "source": [
    "### Define a training loop\n",
    "\n",
    "We now have our network and our training data. Let's train it, i.e., use the training data to update the model's variables (`W` and `b`) so that the loss goes down using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer` implementations. We'd highly recommend using those implementations, but in the spirit of building from first principles, in this particular example we will implement the basic math ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MBIACgdnA55X"
   },
   "outputs": [],
   "source": [
    "# def train(model, inputs, outputs, learning_rate):\n",
    "#     with tf.GradientTape() as t:\n",
    "#         current_loss = construct_map_and_calc_loss(model(inputs), outputs)\n",
    "#     dW, db = t.gradient(current_loss, [model.W, model.b])\n",
    "#     model.W.assign_sub(learning_rate * dW)\n",
    "#     model.b.assign_sub(learning_rate * db)\n",
    "\n",
    "# def train2(inputs, learning_rate, extvar=[]):\n",
    "#     with tf.GradientTape() as t:\n",
    "#         current_loss = construct_map_and_calc_loss(inputs, extvar);\n",
    "#     d_inputs = t.gradient(current_loss, inputs)\n",
    "#     inputs.assign_sub(learning_rate * d_inputs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Finally, let's repeatedly run through the training data and see how `W` and `b` evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "XdfkR223D9dW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0, Epoch 1: G loss: 0.3957063470567976 vs. C loss: 0.1812899129258262\n",
      "Group 0, Epoch 2: G loss: 0.2764297183070864 vs. C loss: 0.19444422092702654\n",
      "Group 0, Epoch 3: G loss: 0.05311429037579469 vs. C loss: 0.20195519675811133\n",
      "Group 0, Epoch 4: G loss: 0.03425081856548786 vs. C loss: 0.2065418759981791\n",
      "Group 0, Epoch 5: G loss: 0.021835142374038694 vs. C loss: 0.203477766778734\n",
      "Group 0, Epoch 6: G loss: 0.02753207747425352 vs. C loss: 0.19521075735489526\n",
      "Group 1, Epoch 1: G loss: 0.37420977268900185 vs. C loss: 0.228778815931744\n",
      "Group 1, Epoch 2: G loss: 0.3768880324704306 vs. C loss: 0.15178736547629038\n",
      "Group 1, Epoch 3: G loss: 0.18394438411508288 vs. C loss: 0.20248516069518194\n",
      "Group 1, Epoch 4: G loss: 0.06743252399776663 vs. C loss: 0.21550565295749244\n",
      "Group 1, Epoch 5: G loss: 0.011828235842819725 vs. C loss: 0.20953805247942606\n",
      "Group 1, Epoch 6: G loss: 0.003426745028368064 vs. C loss: 0.20824277069833544\n",
      "Group 2, Epoch 1: G loss: 0.348720520734787 vs. C loss: 0.21378118627601198\n",
      "Group 2, Epoch 2: G loss: 0.2510065768446241 vs. C loss: 0.19445573290189108\n",
      "Group 2, Epoch 3: G loss: 0.0659804942352431 vs. C loss: 0.2012507120768229\n",
      "Group 2, Epoch 4: G loss: 0.034392320045403074 vs. C loss: 0.20083547797467974\n",
      "Group 2, Epoch 5: G loss: 0.043938032324825016 vs. C loss: 0.19758478966024187\n",
      "Group 2, Epoch 6: G loss: 0.06149859737072673 vs. C loss: 0.18557085593541464\n",
      "Group 3, Epoch 1: G loss: 0.26192346640995573 vs. C loss: 0.26125713934501016\n",
      "Group 3, Epoch 2: G loss: 0.29949564210006174 vs. C loss: 0.1679613855150011\n",
      "Group 3, Epoch 3: G loss: 0.15361943244934081 vs. C loss: 0.20735979245768651\n",
      "Group 3, Epoch 4: G loss: 0.04590369643909591 vs. C loss: 0.2042287124527825\n",
      "Group 3, Epoch 5: G loss: 0.04319562284009797 vs. C loss: 0.19364194985893038\n",
      "Group 3, Epoch 6: G loss: 0.08687462136149406 vs. C loss: 0.19439702563815645\n",
      "Group 4, Epoch 1: G loss: 0.31781353013856073 vs. C loss: 0.21865076985624102\n",
      "Group 4, Epoch 2: G loss: 0.4847761034965515 vs. C loss: 0.16286328352159923\n",
      "Group 4, Epoch 3: G loss: 0.2567169461931501 vs. C loss: 0.21127374139097\n",
      "Group 4, Epoch 4: G loss: 0.09545089964355742 vs. C loss: 0.20338544249534607\n",
      "Group 4, Epoch 5: G loss: 0.08101190935288158 vs. C loss: 0.1914911957250701\n",
      "Group 4, Epoch 6: G loss: 0.15962103243385045 vs. C loss: 0.16434083630641302\n",
      "Group 5, Epoch 1: G loss: 0.27148731308324 vs. C loss: 0.22432003749741447\n",
      "Group 5, Epoch 2: G loss: 0.19893202504941396 vs. C loss: 0.19580661257108053\n",
      "Group 5, Epoch 3: G loss: 0.09897885194846563 vs. C loss: 0.1992953833606508\n",
      "Group 5, Epoch 4: G loss: 0.05762137132031577 vs. C loss: 0.19904697189728418\n",
      "Group 5, Epoch 5: G loss: 0.06651956737041473 vs. C loss: 0.19291925761434767\n",
      "Group 5, Epoch 6: G loss: 0.11482744302068439 vs. C loss: 0.1748414950238334\n",
      "Group 6, Epoch 1: G loss: 0.15046356426818028 vs. C loss: 0.17080160975456238\n",
      "Group 6, Epoch 2: G loss: 0.3758909280811037 vs. C loss: 0.19371993839740753\n",
      "Group 6, Epoch 3: G loss: 0.19036058325852667 vs. C loss: 0.24510477897193694\n",
      "Group 6, Epoch 4: G loss: 0.025568801457328455 vs. C loss: 0.20819396442837187\n",
      "Group 6, Epoch 5: G loss: 0.0031184714287519457 vs. C loss: 0.20755489667256674\n",
      "Group 6, Epoch 6: G loss: 0.0019957924993442636 vs. C loss: 0.20755942248635825\n",
      "Group 7, Epoch 1: G loss: 0.5262688594205038 vs. C loss: 0.2510009383161863\n",
      "Group 7, Epoch 2: G loss: 0.3616099493844169 vs. C loss: 0.18458140558666655\n",
      "Group 7, Epoch 3: G loss: 0.14605283843619482 vs. C loss: 0.18379054301314887\n",
      "Group 7, Epoch 4: G loss: 0.18151106217077798 vs. C loss: 0.21990044746134016\n",
      "Group 7, Epoch 5: G loss: 0.04418812032256808 vs. C loss: 0.20191494292683074\n",
      "Group 7, Epoch 6: G loss: 0.019319921465856687 vs. C loss: 0.20314257509178588\n",
      "Group 8, Epoch 1: G loss: 0.46071498649460935 vs. C loss: 0.23665887117385861\n",
      "Group 8, Epoch 2: G loss: 0.2553016113383429 vs. C loss: 0.1973695009946823\n",
      "Group 8, Epoch 3: G loss: 0.08017990376268114 vs. C loss: 0.18937993049621582\n",
      "Group 8, Epoch 4: G loss: 0.11911156283957616 vs. C loss: 0.14963407152228883\n",
      "Group 8, Epoch 5: G loss: 0.30595412318195614 vs. C loss: 0.2175637169016732\n",
      "Group 8, Epoch 6: G loss: 0.045142531820705954 vs. C loss: 0.20388922095298767\n",
      "Group 9, Epoch 1: G loss: 0.30112283059528894 vs. C loss: 0.19475084046522775\n",
      "Group 9, Epoch 2: G loss: 0.2082772795643125 vs. C loss: 0.15665806664360893\n",
      "Group 9, Epoch 3: G loss: 0.40380804623876293 vs. C loss: 0.24476827846633062\n",
      "Group 9, Epoch 4: G loss: 0.08252689295581409 vs. C loss: 0.20496886306338843\n",
      "Group 9, Epoch 5: G loss: 0.022703787950532777 vs. C loss: 0.2035465662678083\n",
      "Group 9, Epoch 6: G loss: 0.021006987509982925 vs. C loss: 0.2012368705537584\n",
      "126,96,1876,1,0,0:0:0\n",
      "103,113,2337,1,0,0:0:0\n",
      "140,90,2799,1,0,0:0:0\n",
      "385,108,3260,1,0,0:0:0\n",
      "174,206,3722,2,0,L|105:279,1,100,0:0:0\n",
      "154,287,4645,1,0,0:0:0\n",
      "386,185,5106,1,0,0:0:0\n",
      "257,106,5568,1,0,0:0:0\n",
      "395,94,6029,1,0,0:0:0\n",
      "312,303,6491,1,0,0:0:0\n",
      "321,86,6952,1,0,0:0:0\n",
      "167,257,7414,2,0,L|190:160,1,100,0:0:0\n",
      "234,93,8337,1,0,0:0:0\n",
      "115,115,8799,1,0,0:0:0\n",
      "407,158,9260,2,0,L|347:79,1,100,0:0:0\n",
      "363,269,10183,1,0,0:0:0\n",
      "213,294,10645,2,0,L|143:224,1,100,0:0:0\n",
      "259,152,11568,2,0,L|282:250,1,100,0:0:0\n",
      "391,259,12491,1,0,0:0:0\n",
      "133,86,12952,1,0,0:0:0\n",
      "100,99,13183,1,0,0:0:0\n",
      "196,195,13414,1,0,0:0:0\n",
      "210,103,13876,1,0,0:0:0\n",
      "189,237,14337,1,0,0:0:0\n",
      "181,193,14799,1,0,0:0:0\n",
      "194,228,15260,2,0,L|127:302,1,100,0:0:0\n",
      "217,87,16183,2,0,L|292:154,1,100,0:0:0\n",
      "112,296,17568,2,0,L|212:288,1,100,0:0:0\n",
      "117,82,18491,1,0,0:0:0\n",
      "359,75,18952,1,0,0:0:0\n",
      "260,309,19414,2,0,L|166:277,1,100,0:0:0\n",
      "353,113,20337,2,0,L|279:179,1,100,0:0:0\n",
      "165,286,21260,1,0,0:0:0\n",
      "106,218,21722,2,0,L|174:293,1,100,0:0:0\n",
      "400,206,22645,2,0,L|333:133,1,100,0:0:0\n",
      "281,77,23568,2,0,L|186:106,1,100,0:0:0\n",
      "201,276,24952,2,0,L|135:203,1,100,0:0:0\n",
      "174,287,25876,2,0,L|245:217,1,100,0:0:0\n",
      "128,295,26799,1,0,0:0:0\n",
      "340,284,27260,1,0,0:0:0\n",
      "209,55,28183,1,0,0:0:0\n",
      "247,144,28645,2,0,L|341:179,1,100,0:0:0\n",
      "263,79,29337,1,0,0:0:0\n",
      "352,238,29568,1,0,0:0:0\n",
      "376,258,30029,1,0,0:0:0\n",
      "355,144,30491,1,0,0:0:0\n",
      "359,110,30952,2,0,L|288:181,1,100,0:0:0\n",
      "284,283,31876,2,0,L|320:248,1,50,0:0:0\n",
      "36,283,32337,2,0,L|-34:212,1,100,0:0:0\n",
      "12,93,33029,1,0,0:0:0\n",
      "370,79,33260,1,0,0:0:0\n",
      "189,168,33722,1,0,0:0:0\n",
      "238,262,34183,1,0,0:0:0\n",
      "332,118,34645,1,0,0:0:0\n",
      "246,106,35106,1,0,0:0:0\n",
      "116,231,35568,2,0,L|118:282,1,50,0:0:0\n",
      "180,150,36491,2,0,L|110:80,1,100,0:0:0\n",
      "414,189,37414,2,0,L|351:113,1,100,0:0:0\n",
      "374,126,38337,2,0,L|448:195,1,100,0:0:0\n",
      "93,112,39260,1,0,0:0:0\n",
      "113,266,39722,2,0,L|213:254,1,100,0:0:0\n",
      "233,252,40414,1,0,0:0:0\n",
      "383,133,40645,1,0,0:0:0\n",
      "245,201,41106,2,0,L|171:269,1,100,0:0:0\n",
      "259,119,42029,1,0,0:0:0\n",
      "181,147,42491,2,0,L|269:99,1,100,0:0:0\n",
      "325,130,43414,2,0,L|278:218,1,100,0:0:0\n",
      "400,212,44337,1,0,0:0:0\n",
      "231,116,44799,1,0,0:0:0\n",
      "244,261,45260,1,0,0:0:0\n",
      "270,231,45722,2,0,L|332:310,1,100,0:0:0\n",
      "128,102,46645,1,0,0:0:0\n",
      "157,301,47106,1,0,0:0:0\n",
      "145,164,47568,2,0,L|238:126,1,100,0:0:0\n",
      "382,133,48491,1,0,0:0:0\n",
      "129,73,48952,1,0,0:0:0\n",
      "387,88,49414,2,0,L|318:160,1,100,0:0:0\n",
      "181,127,50337,1,0,0:0:0\n",
      "305,252,50568,1,0,0:0:0\n",
      "390,252,50799,1,0,0:0:0\n",
      "284,136,51029,1,0,0:0:0\n",
      "234,204,51260,2,0,L|192:230,1,50,0:0:0\n",
      "403,6,52183,1,0,0:0:0\n",
      "400,186,52414,1,0,0:0:0\n",
      "302,282,52645,1,0,0:0:0\n",
      "176,154,52876,1,0,0:0:0\n",
      "283,257,53106,2,0,L|357:191,1,100,0:0:0\n",
      "180,171,53799,1,0,0:0:0\n",
      "281,285,54029,2,0,L|206:351,1,100,0:0:0\n",
      "380,87,54952,1,0,0:0:0\n",
      "385,126,55414,1,0,0:0:0\n",
      "123,95,55876,1,0,0:0:0\n",
      "114,256,56337,2,0,L|203:209,1,100,0:0:0\n",
      "326,88,57029,1,0,0:0:0\n",
      "204,235,57260,2,0,L|136:163,1,100,0:0:0\n",
      "286,224,58183,2,0,L|208:163,1,100,0:0:0\n",
      "380,226,59568,1,0,0:0:0\n",
      "197,187,60029,1,0,0:0:0\n",
      "323,306,60260,1,0,0:0:0\n",
      "345,175,60491,2,0,L|310:211,1,50,0:0:0\n"
     ]
    }
   ],
   "source": [
    "inputs  = tfe.Variable(tf.random_normal(shape=[40]))\n",
    "\n",
    "# Collect the history of W-values and b-values to plot later\n",
    "Cs = []\n",
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.002)\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "class PrintCross(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('x', end='')\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, 50));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    \n",
    "    classifier_model = build_classifier_model()\n",
    "    extvar[\"classifier_model\"] = classifier_model;\n",
    "    \n",
    "    def loss_function_for_generative_model(y_true, y_pred):\n",
    "        #print(y_pred.shape); #(?, 20)\n",
    "        return construct_map_and_calc_loss(y_pred, extvar);\n",
    "    \n",
    "#     classifier_true_set_group = special_train_data[np.random.randint(0, special_train_data.shape[0], (500,))];\n",
    "    \n",
    "#     classifier_model.summary()\n",
    "    gmodel = generative_model(50, 40, loss_function_for_generative_model);\n",
    "#     gmodel.summary()\n",
    "    max_epoch = 30\n",
    "    good_epoch = 5\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        g_multiplier = 7;\n",
    "        c_multiplier = 3;\n",
    "        gnoise = np.random.random((50, 50));\n",
    "        glabel = np.ones((50,))\n",
    "\n",
    "        history = gmodel.fit(gnoise, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintDot()\n",
    "        \n",
    "        predicted_maps_data = gmodel.predict(np.random.random((10, 50)));\n",
    "        new_false_maps = construct_map_with_sliders(tf.convert_to_tensor(predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "        new_false_labels = np.zeros(10);\n",
    "        \n",
    "#         not_predicted_maps_data = np.random.random((10, 40));\n",
    "    #     new_false_maps2 = construct_map_without_sliders(tf.convert_to_tensor(not_predicted_maps_data, dtype=\"float32\")).numpy();\n",
    "    #     new_false_labels2 = np.zeros(10);\n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (50,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "    #     actual_train_data = special_train_data[st:se];\n",
    "    #     actual_train_labels = special_train_labels[st:se];\n",
    "\n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintCross()\n",
    "\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        res = gmodel.predict(plot_noise);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        if i >= good_epoch:\n",
    "            current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0:\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    for i in range(3): # it's the same anyways\n",
    "        res = gmodel.predict(np.random.random((1, 50)));\n",
    "#         print(construct_map_with_sliders(tf.convert_to_tensor(res), extvar=extvar).numpy().squeeze());\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "\n",
    "    onoise = np.random.random((1, 50));\n",
    "    \n",
    "    return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    pos = [256, 192];\n",
    "    for i in range(10):#range(timestamps.shape[0] // 10):\n",
    "        z = generate_set(begin = i * 10, start_pos = pos, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    generate_set(begin = 30, start_pos = pos, length_multiplier = 0.5, group_id = 3, plot_map=True);\n",
    "\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "print_osu_text(osu_a);\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Output to node:\n",
    "# timingData\n",
    "# music file link (yes)\n",
    "# generated map/vector data\n",
    "# rhythm data\n",
    "# - ticks, timestamps, is_slider, is_spinner, slider_types, slider_ticks, slider_lengths\n",
    "# あと少し！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "In this tutorial we covered `Variable`s and built and trained a simple linear model using the TensorFlow primitives discussed so far.\n",
    "\n",
    "In theory, this is pretty much all you need to use TensorFlow for your machine learning research.\n",
    "In practice, particularly for neural networks, the higher level APIs like `tf.keras` will be much more convenient since it provides higher level building blocks (called \"layers\"), utilities to save and restore state, a suite of loss functions, a suite of optimization strategies etc. \n",
    "\n",
    "The [next tutorial](TODO) will cover these higher level APIs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "custom_training_002",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1RStBySCoGq8pIA0ya3mOFaYufAvyqvs8",
     "timestamp": 1533259533996
    },
    {
     "file_id": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/notebooks/custom_training.ipynb",
     "timestamp": 1533259167668
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
