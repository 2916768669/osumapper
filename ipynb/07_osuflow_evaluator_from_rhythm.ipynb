{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2018/8/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "#### First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://ar3.moe/files/sophie.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "GAN is, kind of, hard to train; and I personally felt the pain when I downloaded 10+ github repos where no one worked. While all of those code are based on MNIST, they either do not train any image that looks good, or simply fail to run.\n",
    "\n",
    "As a consequence, I coded this notebook myself using tf.contrib.eager and tf.keras - took a while to find out how to modify that loss function.\n",
    "\n",
    "tf.contrib.eager is also said to be a pretty new feature in Tensorflow. ~~Idk how to write the other style of code using sessions without getting error anyways,~~ so make sure Tensorflow has the right version. My env is tensorflow v1.9.0 - 1.10.0. On Win10, python3.5, no cuda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "3LXMVuV0VhDr"
   },
   "source": [
    "Import the wheels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# this line below can only run once in the notebook! otherwise it will cause errors\n",
    "try:\n",
    "    tf.enable_eager_execution();\n",
    "except:\n",
    "    pass\n",
    "tfe = tf.contrib.eager;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some parameters related to GAN training.\n",
    "\n",
    "May or may not affect the result; haven't experimented a lot about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GAN_PARAMS = {\n",
    "    \"good_epoch\" : 6,\n",
    "    \"max_epoch\" : 25,\n",
    "    \"note_group_size\" : 10,\n",
    "    \"g_epochs\" : 7,\n",
    "    \"c_epochs\" : 3,\n",
    "    \"g_batch\" : 50,\n",
    "    \"g_input_size\" : 50,\n",
    "    \"c_true_batch\" : 50,\n",
    "    \"c_false_batch\" : 10\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "#### Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, unfiltered_sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "first_step_sv =          unfiltered_sv[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + 5: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(4);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "sv =          first_step_sv[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // 4;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 3, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "timestamps_after = timestamps_plus_1 - timestamps;\n",
    "timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "\n",
    "note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plotting functions found from stackoverflow. Probably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib.text as mtext\n",
    "\n",
    "\n",
    "class MyLine(lines.Line2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # we'll update the position when the line data is set\n",
    "        self.text = mtext.Text(0, 0, '')\n",
    "        lines.Line2D.__init__(self, *args, **kwargs)\n",
    "\n",
    "        # we can't access the label attr until *after* the line is\n",
    "        # inited\n",
    "        self.text.set_text(self.get_label())\n",
    "\n",
    "    def set_figure(self, figure):\n",
    "        self.text.set_figure(figure)\n",
    "        lines.Line2D.set_figure(self, figure)\n",
    "\n",
    "    def set_axes(self, axes):\n",
    "        self.text.set_axes(axes)\n",
    "        lines.Line2D.set_axes(self, axes)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        # 2 pixel offset\n",
    "        texttrans = transform + mtransforms.Affine2D().translate(2, 2)\n",
    "        self.text.set_transform(texttrans)\n",
    "        lines.Line2D.set_transform(self, transform)\n",
    "\n",
    "    def set_data(self, x, y):\n",
    "        if len(x):\n",
    "            self.text.set_position((x[-1], y[-1]))\n",
    "\n",
    "        lines.Line2D.set_data(self, x, y)\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        # draw my label at the end of the line with 2 pixel offset\n",
    "        lines.Line2D.draw(self, renderer)\n",
    "        self.text.draw(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import numpy as np;\n",
    "\n",
    "root = \"mapdata/\";\n",
    "\n",
    "chunk_size = GAN_PARAMS[\"note_group_size\"];\n",
    "step_size = 5;\n",
    "divisor = 4;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define some sort of plotting functions to show the generator and discriminator losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Limitless]')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# actual_train_data = np.concatenate((special_train_data[0:1], special_false_data[0:1]), axis=0);\n",
    "# actual_train_labels = np.concatenate((special_train_labels[0:1], special_false_labels[0:1]), axis=0);\n",
    "\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0.2)) * tf.square(0.3 - vg);\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 0.8)) * tf.square(vg - 0.7);\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0));\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 1));\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "    var_tensor = tf.to_float(var_tensor);\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_tensor.shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    batch_size = var_tensor.shape[0];\n",
    "    note_distances_now = length_multiplier * np.tile(np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "    note_angles_now = np.tile(np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "    \n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "    \n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.to_float(tf.greater(l, y_max / 2));\n",
    "    not_rerand = tf.to_float(tf.less_equal(l, y_max / 2));\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _px, _py = extvar[\"start_pos\"];\n",
    "    else:\n",
    "        _px = 256;\n",
    "        _py = 192;\n",
    "    _pa = 0;\n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = 256;\n",
    "    _y = 192;\n",
    "    for k in range(half_tensor):\n",
    "        note_index = begin_offset + k;\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l = tf.to_float(tf.less(_px, wall_l[:, k]));\n",
    "        wall_value_r = tf.to_float(tf.greater(_px, wall_r[:, k]));\n",
    "        wall_value_xmid = tf.to_float(tf.greater(_px, wall_l[:, k])) * tf.to_float(tf.less(_px, wall_r[:, k]));\n",
    "        wall_value_t = tf.to_float(tf.less(_py, wall_t[:, k]));\n",
    "        wall_value_b = tf.to_float(tf.greater(_py, wall_b[:, k]));\n",
    "        wall_value_ymid = tf.to_float(tf.greater(_py, wall_t[:, k])) * tf.to_float(tf.less(_py, wall_b[:, k]));\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        if is_slider[note_index]:\n",
    "            sln = slider_lengths[note_index];\n",
    "            slider_type = slider_types[note_index];\n",
    "            scos = slider_cos[slider_type];\n",
    "            ssin = slider_sin[slider_type];\n",
    "            _a = cos_list[:, k + half_tensor];\n",
    "            _b = sin_list[:, k + half_tensor];\n",
    "            # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "            # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "            _oa = _a * scos - _b * ssin;\n",
    "            _ob = _a * ssin + _b * scos;\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x + _a * sln;\n",
    "            _py = _y + _b * sln;\n",
    "        else:\n",
    "            _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "            _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x;\n",
    "            _py = _y;\n",
    "        _pa = tf.mod(_pa + 0, 6.283);\n",
    "    return tf.transpose(tf.stack(out, axis=0), [1, 0, 2]);\n",
    "\n",
    "def stack_loss(tensor):\n",
    "    complex_list = tf.complex(tensor[:, :, 0] * 512, tensor[:, :, 1] * 384);\n",
    "    stack_limit = 30;\n",
    "    precise_limit = 1;\n",
    "    a = [];\n",
    "    for k in range(tensor.shape[1]):\n",
    "        w = tf.tile(tf.expand_dims(complex_list[:, k], axis=1), [1, tensor.shape[1]]);\n",
    "        r = tf.abs(w - complex_list);\n",
    "        rless = tf.to_float(tf.less(r, stack_limit)) * tf.to_float(tf.greater(r, precise_limit));\n",
    "        rmean = tf.reduce_mean(rless * (stack_limit - r) / stack_limit);\n",
    "        a.append(rmean);\n",
    "    b = tf.reduce_sum(a);\n",
    "#         print(tf.tile(w, [1, tensor.shape[1]]));\n",
    "#         print(complex_list);\n",
    "    return b;\n",
    "\n",
    "# This polygon loss was an attempt to make the map less likely to overlap each other.\n",
    "# The idea is: calculate the area of polygon formed from the note positions;\n",
    "# If it is big, then it is good - they form a convex shape, no overlap.\n",
    "# ... of course it totally doesn't work like that.\n",
    "def polygon_loss(tensor):\n",
    "    tensor_this = tensor[:, :, 0:2];\n",
    "    tensor_next = tf.concat([tensor[:, 1:, 0:2], tensor[:, 0:1, 0:2]], axis=1);\n",
    "    sa = (tensor_this[:, :, 0] + tensor_next[:, :, 0]) * (tensor_next[:, :, 1] - tensor_this[:, :, 0]);\n",
    "    surface = tf.abs(tf.reduce_sum(sa, axis=1))/2;\n",
    "    return surface;\n",
    "\n",
    "def construct_map_and_calc_loss(var_tensor, extvar):\n",
    "    # first make a map from the outputs of generator, then ask the classifier (discriminator) to classify it\n",
    "    classifier_model = extvar[\"classifier_model\"]\n",
    "    out = construct_map_with_sliders(var_tensor, extvar=extvar);\n",
    "    cm = classifier_model(out);\n",
    "    predmean = 1 - tf.reduce_mean(cm, axis=1);\n",
    "#    regulator = tf.reduce_mean(tf.reduce_mean(- 0.1 * tf.square(out[:, :, 1:2] - 0.5), axis=2), axis=1); # * tf.square(out[:, :, 1:2] - 2)\n",
    "    box_loss = inblock_loss(out[:, :, 0:2]);\n",
    "    box_loss2 = inblock_loss(out[:, :, 4:6]);\n",
    "#     polygon = polygon_loss(out);\n",
    "    # print(out.shape); shape is (10, X, 4)\n",
    "    #return predmean + box_loss*100; <-- this was too harsh, in fact breaking SGD and creating waveforms on the borderline\n",
    "    return predmean + box_loss + box_loss2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "This will take some time. It splits map into groups of 10 (10 by default), and trains GAN to play with the flow of maps in the dataset. It will train for floor(note_count / 10) groups, for example, if you have 820 then it needs to train 82 groups.\n",
    "\n",
    "Some notes are converted to sliders, so it has less than the predicted count in #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "XdfkR223D9dW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0, Epoch 1: G loss: 0.4734493357794626 vs. C loss: 0.18857702944013807\n",
      "Group 0, Epoch 2: G loss: 0.26859331727027896 vs. C loss: 0.1966180685493681\n",
      "Group 0, Epoch 3: G loss: 0.05122294489826474 vs. C loss: 0.19534353415171304\n",
      "Group 0, Epoch 4: G loss: 0.05723814378891672 vs. C loss: 0.19325820770528582\n",
      "Group 0, Epoch 5: G loss: 0.09524989042963303 vs. C loss: 0.19556780242257651\n",
      "Group 0, Epoch 6: G loss: 0.11174806015832082 vs. C loss: 0.1708115835984548\n",
      "Group 1, Epoch 1: G loss: 0.26465032867022925 vs. C loss: 0.21206908424695334\n",
      "Group 1, Epoch 2: G loss: 0.22780211525303978 vs. C loss: 0.19491309424241385\n",
      "Group 1, Epoch 3: G loss: 0.1160343789628574 vs. C loss: 0.1590233329269621\n",
      "Group 1, Epoch 4: G loss: 0.17933556448136057 vs. C loss: 0.22024519410398272\n",
      "Group 1, Epoch 5: G loss: 0.07388651349714825 vs. C loss: 0.18962108426623872\n",
      "Group 1, Epoch 6: G loss: 0.16680072290556772 vs. C loss: 0.1340167700416512\n",
      "Group 2, Epoch 1: G loss: 0.24269565301282067 vs. C loss: 0.22593964139620462\n",
      "Group 2, Epoch 2: G loss: 0.1656359470316342 vs. C loss: 0.15814963314268324\n",
      "Group 2, Epoch 3: G loss: 0.4198508603232248 vs. C loss: 0.15224668466382554\n",
      "Group 2, Epoch 4: G loss: 0.3751103299004691 vs. C loss: 0.1473832701643308\n",
      "Group 2, Epoch 5: G loss: 0.4501034940992083 vs. C loss: 0.15232086678345996\n",
      "Group 2, Epoch 6: G loss: 0.3084359943866729 vs. C loss: 0.21515090929137337\n",
      "Group 2, Epoch 7: G loss: 0.12160644839916908 vs. C loss: 0.20711378365780952\n",
      "Group 2, Epoch 8: G loss: 0.03612384455544608 vs. C loss: 0.20654812786314222\n",
      "Group 3, Epoch 1: G loss: 0.24270711328302114 vs. C loss: 0.17821405165725282\n",
      "Group 3, Epoch 2: G loss: 0.37764532523495814 vs. C loss: 0.13191331923007968\n",
      "Group 3, Epoch 3: G loss: 0.5475832679441995 vs. C loss: 0.19303994377454123\n",
      "Group 3, Epoch 4: G loss: 0.14736484209341663 vs. C loss: 0.21391292413075766\n",
      "Group 3, Epoch 5: G loss: 0.014143506836678298 vs. C loss: 0.20787506633334693\n",
      "Group 3, Epoch 6: G loss: 0.004444243625870773 vs. C loss: 0.20734301209449768\n",
      "Group 4, Epoch 1: G loss: 0.3299132934638432 vs. C loss: 0.19051636507113776\n",
      "Group 4, Epoch 2: G loss: 0.3052519172430039 vs. C loss: 0.1913831283648809\n",
      "Group 4, Epoch 3: G loss: 0.13022165106875555 vs. C loss: 0.19207977420753905\n",
      "Group 4, Epoch 4: G loss: 0.12938995297466008 vs. C loss: 0.20197881427076125\n",
      "Group 4, Epoch 5: G loss: 0.08240606476153646 vs. C loss: 0.19360669867859948\n",
      "Group 4, Epoch 6: G loss: 0.14091243680034365 vs. C loss: 0.15982578694820404\n",
      "Group 5, Epoch 1: G loss: 0.28767749752317157 vs. C loss: 0.23956590104434225\n",
      "Group 5, Epoch 2: G loss: 0.24083143004349303 vs. C loss: 0.16517014718718\n",
      "Group 5, Epoch 3: G loss: 0.3405872123582023 vs. C loss: 0.10887749824259015\n",
      "Group 5, Epoch 4: G loss: 0.5632855721882412 vs. C loss: 0.09950992258058654\n",
      "Group 5, Epoch 5: G loss: 0.5450787305831908 vs. C loss: 0.2282111214266883\n",
      "Group 5, Epoch 6: G loss: 0.038406940549612044 vs. C loss: 0.20443965908553866\n",
      "Group 5, Epoch 7: G loss: 0.026740905695727898 vs. C loss: 0.20438201228777567\n",
      "Group 5, Epoch 8: G loss: 0.01688650237130267 vs. C loss: 0.2043642583820555\n",
      "Group 6, Epoch 1: G loss: 0.2900186381169728 vs. C loss: 0.15536428822411433\n",
      "Group 6, Epoch 2: G loss: 0.2885517367294857 vs. C loss: 0.15532311962710485\n",
      "Group 6, Epoch 3: G loss: 0.5557735979557037 vs. C loss: 0.11340488327874078\n",
      "Group 6, Epoch 4: G loss: 0.39501148291996546 vs. C loss: 0.2106273737218645\n",
      "Group 6, Epoch 5: G loss: 0.037205044073717934 vs. C loss: 0.20577646295229593\n",
      "Group 6, Epoch 6: G loss: 0.0190733331654753 vs. C loss: 0.20601224568155077\n",
      "Group 7, Epoch 1: G loss: 0.4482235610485077 vs. C loss: 0.22198295096556345\n",
      "Group 7, Epoch 2: G loss: 0.34616748009409226 vs. C loss: 0.16208512004878783\n",
      "Group 7, Epoch 3: G loss: 0.2529448845556804 vs. C loss: 0.16775653097364637\n",
      "Group 7, Epoch 4: G loss: 0.3504107615777424 vs. C loss: 0.20536981191900042\n",
      "Group 7, Epoch 5: G loss: 0.0982025370001793 vs. C loss: 0.16676041815016007\n",
      "Group 7, Epoch 6: G loss: 0.4524620422295162 vs. C loss: 0.0835439811150233\n"
     ]
    }
   ],
   "source": [
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer, add the start pos\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.002)\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "class PrintCross(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('x', end='')\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, GAN_PARAMS[\"g_input_size\"]));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    \n",
    "    classifier_model = build_classifier_model()\n",
    "    extvar[\"classifier_model\"] = classifier_model;\n",
    "    \n",
    "    def loss_function_for_generative_model(y_true, y_pred):\n",
    "        return construct_map_and_calc_loss(y_pred, extvar);\n",
    "    \n",
    "#     classifier_true_set_group = special_train_data[np.random.randint(0, special_train_data.shape[0], (500,))];\n",
    "    \n",
    "#     classifier_model.summary()\n",
    "#     gmodel.summary()\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    max_epoch = GAN_PARAMS[\"max_epoch\"];\n",
    "    good_epoch = GAN_PARAMS[\"good_epoch\"] - 1;\n",
    "    g_multiplier = GAN_PARAMS[\"g_epochs\"];\n",
    "    c_multiplier = GAN_PARAMS[\"c_epochs\"];\n",
    "    g_batch = GAN_PARAMS[\"g_batch\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    c_true_batch = GAN_PARAMS[\"c_true_batch\"]\n",
    "    c_false_batch = GAN_PARAMS[\"c_false_batch\"]\n",
    "    \n",
    "    gmodel = generative_model(g_input_size, note_group_size * 4, loss_function_for_generative_model);\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        gnoise = np.random.random((g_batch, g_input_size));\n",
    "        glabel = np.ones((g_batch,))\n",
    "\n",
    "        history = gmodel.fit(gnoise, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintDot()\n",
    "        \n",
    "        predicted_maps_data = gmodel.predict(np.random.random((c_false_batch, g_input_size)));\n",
    "        new_false_maps = construct_map_with_sliders(tf.convert_to_tensor(predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "        new_false_labels = np.zeros(c_false_batch);\n",
    "        \n",
    "#         not_predicted_maps_data = np.random.random((10, 40));\n",
    "    #     new_false_maps2 = construct_map_with_sliders(tf.convert_to_tensor(not_predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "    #     new_false_labels2 = np.zeros(10);\n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (c_true_batch,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "    #     actual_train_data = special_train_data[st:se];\n",
    "    #     actual_train_labels = special_train_labels[st:se];\n",
    "\n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintCross()\n",
    "\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        res = gmodel.predict(plot_noise);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        if i >= good_epoch:\n",
    "            current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # it's the same anyways\n",
    "            res = gmodel.predict(np.random.random((1, g_input_size)));\n",
    "#         print(construct_map_with_sliders(tf.convert_to_tensor(res), extvar=extvar).numpy().squeeze());\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "\n",
    "    onoise = np.random.random((1, g_input_size));\n",
    "    \n",
    "    return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    for i in range(timestamps.shape[0] // note_group_size):\n",
    "        z = generate_set(begin = i * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    generate_set(begin = 3 * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success! finished on: 2018-08-19 00:17:21.503352\n"
     ]
    }
   ],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@2018/8/19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
