{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### osu!nn #2: rhythm estimator\n",
    "\n",
    "Builds up a rhythm model to estimate the rhythm using music and timing data.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* rhythmData x 1\n",
    "* (Audio) x 3\n",
    "* (Classifier) x 1\n",
    "\n",
    "Synthesis Time: ~5 mins\n",
    "\n",
    "Final edit: 2018/8/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First of all, Import the wheels.\n",
    "\n",
    "\"root\" points to the folder that stores all the .npz map data, where all files in .npz extension are read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\asus\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\Pyth3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m           \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudart_dll_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\asus\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\Pyth3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] 找不到指定的模块。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-80db7cc7d862>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\asus\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\Pyth3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\asus\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\Pyth3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\asus\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\Pyth3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Perform pre-load sanity checks in order to produce a more actionable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# than we get from an error during SWIG import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mself_check\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\asus\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\Pyth3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m               \u001b[1;34m\"environment variable. Download and install CUDA %s from \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m               \u001b[1;34m\"this URL: https://developer.nvidia.com/cuda-90-download-archive\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m               % (build_info.cudart_dll_name, build_info.cuda_version_number))\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m       if hasattr(build_info, \"cudnn_dll_name\") and hasattr(\n",
      "\u001b[1;31mImportError\u001b[0m: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "root = \"mapdata/\";\n",
    "\n",
    "divisor = 4;\n",
    "\n",
    "# this is a global variable!\n",
    "time_interval = 16;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# lst file, [TICK, TIME, NOTE, IS_CIRCLE, IS_SLIDER, IS_SPINNER, IS_SLIDER_END, IS_SPINNER_END, \n",
    "#               0,    1,    2,         3,         4,          5,             6,              7,\n",
    "#            SLIDING, SPINNING, MOMENTUM, ANGULAR_MOMENTUM, EX1, EX2, EX3], length MAPTICKS\n",
    "#                  8,        9,       10,               11,  12,  13,  14,\n",
    "# wav file, [len(snapsize), MAPTICKS, 2, fft_size//4]\n",
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        wav_data = data[\"wav\"];\n",
    "        wav_data = np.swapaxes(wav_data, 2, 3);\n",
    "        train_data = wav_data;\n",
    "        div_source = data[\"lst\"][:, 0];\n",
    "        div_source2 = data[\"lst\"][:, 12:15];\n",
    "        div_data = np.concatenate([np.array([[int(k%4==0), int(k%4==1), int(k%4==2), int(k%4==3)] for k in div_source]), div_source2], axis=1);\n",
    "        lst_data = data[\"lst\"][:, 2:10];\n",
    "        # Change the 0/1 data to -1/1 to use tanh instead of softmax in the NN.\n",
    "        # Somehow tanh works much better than softmax, even if it is a linear combination. Maybe because it is alchemy!\n",
    "        lst_data = 2 * lst_data - 1;\n",
    "        train_labels = lst_data;\n",
    "    return train_data, div_data, train_labels;\n",
    "\n",
    "def read_npz_list():\n",
    "    npz_list = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            npz_list.append(os.path.join(root, file));\n",
    "    # reutnr npz_lsit;\n",
    "    return npz_list;\n",
    "\n",
    "def prefilter_data(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered):\n",
    "    # Filter out slider ends from the training set, since we cannot reliably decide if a slider end is on a note.\n",
    "    # Another way is to set 0.5 for is_note value, but that will break the validation algorithm.\n",
    "    # Also remove the IS_SLIDER_END, IS_SPINNER_END columns which are left to be zeros.\n",
    "\n",
    "    # Before: NOTE, IS_CIRCLE, IS_SLIDER, IS_SPINNER, IS_SLIDER_END, IS_SPINNER_END, SLIDING, SPINNING\n",
    "    #            0,         1,         2,          3,             4,              5,       6,        7\n",
    "    # After:  NOTE, IS_CIRCLE, IS_SLIDER, IS_SPINNER, SLIDING, SPINNING\n",
    "    #            0,         1,         2,          3,       4,        5\n",
    "\n",
    "    non_object_end_indices = [i for i,k in enumerate(train_labels_unfiltered) if k[4] == -1 and k[5] == -1];\n",
    "    train_data = train_data_unfiltered[non_object_end_indices];\n",
    "    div_data = div_data_unfiltered[non_object_end_indices];\n",
    "    train_labels = train_labels_unfiltered[non_object_end_indices][:, [0, 1, 2, 3, 6, 7]];\n",
    "    \n",
    "    # should be (X, 7, 32, 2) and (X, 6) in default sampling settings\n",
    "    # (X, fft_window_type, freq_point, magnitude/phase)\n",
    "    return train_data, div_data, train_labels;\n",
    "\n",
    "def preprocess_npzs(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered):\n",
    "    train_data, div_data, train_labels = prefilter_data(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered);\n",
    "    # In this version, the train data is already normalized, no need to do it again here\n",
    "#     mean = train_data.mean(axisprefilter_=0)\n",
    "#     std = train_data.std(axis=0)\n",
    "#     train_data = (train_data - np.tile(mean, (train_data.shape[0], 1,1,1))) / np.tile(std, (train_data.shape[0], 1,1,1))\n",
    "    \n",
    "    # Make time intervals from training data\n",
    "    if train_data.shape[0]%time_interval > 0:\n",
    "        train_data = train_data[:-(train_data.shape[0]%time_interval)];\n",
    "        div_data = div_data[:-(div_data.shape[0]%time_interval)];\n",
    "        train_labels = train_labels[:-(train_labels.shape[0]%time_interval)];\n",
    "    train_data2 = np.reshape(train_data, (-1, time_interval, train_data.shape[1], train_data.shape[2], train_data.shape[3]))\n",
    "    div_data2 = np.reshape(div_data, (-1, time_interval, div_data.shape[1]))\n",
    "    train_labels2 = np.reshape(train_labels, (-1, time_interval, train_labels.shape[1]))\n",
    "    return train_data2, div_data2, train_labels2;\n",
    "\n",
    "def get_data_shape():\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered = read_npz(os.path.join(root, file));\n",
    "            train_data, div_data, train_labels = prefilter_data(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered);\n",
    "            # should be (X, 7, 32, 2) and (X, 6) in default sampling settings\n",
    "            # (X, fft_window_type, freq_point, magnitude/phase)\n",
    "            # X = 76255\n",
    "            # print(train_data.shape, train_labels.shape);\n",
    "            return train_data.shape, div_data.shape, train_labels.shape;\n",
    "\n",
    "def read_some_npzs_and_preprocess(npz_list):\n",
    "    td_list = [];\n",
    "    dd_list = [];\n",
    "    tl_list = [];\n",
    "    for fp in npz_list:\n",
    "        if fp.endswith(\".npz\"):\n",
    "            _td, _dd, _tl = read_npz(fp);\n",
    "            td_list.append(_td);\n",
    "            dd_list.append(_dd);\n",
    "            tl_list.append(_tl);\n",
    "    train_data_unfiltered = np.concatenate(td_list);\n",
    "    div_data_unfiltered = np.concatenate(dd_list);\n",
    "    train_labels_unfiltered = np.concatenate(tl_list);\n",
    "    \n",
    "    train_data2, div_data2, train_labels2 = preprocess_npzs(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered);\n",
    "    return train_data2, div_data2, train_labels2;\n",
    "\n",
    "def train_test_split(train_data2, div_data2, train_labels2, test_split_count=233):\n",
    "    new_train_data = train_data2[:-test_split_count];\n",
    "    new_div_data = div_data2[:-test_split_count];\n",
    "    new_train_labels = train_labels2[:-test_split_count];\n",
    "    test_data = train_data2[-test_split_count:];\n",
    "    test_div_data = div_data2[-test_split_count:];\n",
    "    test_labels = train_labels2[-test_split_count:];\n",
    "    return (new_train_data, new_div_data, new_train_labels), (test_data, test_div_data, test_labels);\n",
    "\n",
    "# (train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered) = read_all_npzs();\n",
    "\n",
    "train_file_list = read_npz_list();\n",
    "\n",
    "train_shape, div_shape, label_shape = get_data_shape();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model;\n",
    "\n",
    "def build_model():\n",
    "    model1 = keras.Sequential([\n",
    "        keras.layers.TimeDistributed(keras.layers.Conv2D(16, (2, 2),\n",
    "                           data_format='channels_last'),\n",
    "                           input_shape=(time_interval, train_shape[1], train_shape[2], train_shape[3])),\n",
    "        keras.layers.TimeDistributed(keras.layers.MaxPool2D((1, 2),\n",
    "                           data_format='channels_last')),\n",
    "        keras.layers.TimeDistributed(keras.layers.Activation(activation=tf.nn.relu)),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dropout(0.3)),\n",
    "        keras.layers.TimeDistributed(keras.layers.Conv2D(16, (2, 3),\n",
    "                           data_format='channels_last')),\n",
    "        keras.layers.TimeDistributed(keras.layers.MaxPool2D((1, 2),\n",
    "                           data_format='channels_last')),\n",
    "        keras.layers.TimeDistributed(keras.layers.Activation(activation=tf.nn.relu)),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dropout(0.3)),\n",
    "        keras.layers.TimeDistributed(keras.layers.Flatten()),\n",
    "        keras.layers.LSTM(64, activation=tf.nn.tanh, return_sequences=True)\n",
    "    ])\n",
    "    \n",
    "    input2 = keras.layers.InputLayer(input_shape=(time_interval, div_shape[1]));\n",
    "    \n",
    "    conc = keras.layers.concatenate([model1.output, input2.output]);\n",
    "    dense1 = keras.layers.Dense(71, activation=tf.nn.tanh)(conc);\n",
    "    dense2 = keras.layers.Dense(71, activation=tf.nn.relu)(dense1);\n",
    "    dense3 = keras.layers.Dense(label_shape[1], activation=tf.nn.tanh)(dense2);\n",
    "    \n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001);\n",
    "\n",
    "    \n",
    "    final_model = Model(inputs=[model1.input, input2.input], outputs=dense3);\n",
    "    final_model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return final_model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Limitless]')\n",
    "    plt.plot(history.epoch, np.array(history.history['mean_absolute_error']), \n",
    "           label='Train MAE')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
    "           label = 'Val MAE')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Display training progress by printing a single dot for each completed epoch.\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ぐるぐる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Don't worry, it will successfully overfit after those 16 epochs.\n",
    "EPOCHS = 16\n",
    "\n",
    "# since each map npz is about 6mb, this amounts to around 1200mb of RAM.\n",
    "too_many_maps_threshold = 200\n",
    "data_split_count = 160\n",
    "\n",
    "# if there is too much data, reduce epoch count (hmm)\n",
    "if len(train_file_list) >= too_many_maps_threshold:\n",
    "    EPOCHS = 6\n",
    "\n",
    "if len(train_file_list) < too_many_maps_threshold:\n",
    "    train_data2, div_data2, train_labels2 = read_some_npzs_and_preprocess(train_file_list);\n",
    "\n",
    "    # Split some test data out\n",
    "    (new_train_data, new_div_data, new_train_labels), (test_data, test_div_data, test_labels) = train_test_split(train_data2, div_data2, train_labels2);\n",
    "\n",
    "    # Store training stats\n",
    "    history = model.fit([new_train_data, new_div_data], new_train_labels, epochs=EPOCHS,\n",
    "                        validation_split=0.2, verbose=0, #batch_size=477,\n",
    "                        callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "    # For development! may cause bug in some environment.\n",
    "#     plot_history(history)\n",
    "else: # too much data! read it every turn.\n",
    "    for epoch in range(EPOCHS):\n",
    "        for map_batch in range(np.ceil(len(train_file_list) / data_split_count).astype(int)): # hmmmmm\n",
    "            if map_batch == 0:\n",
    "                train_data2, div_data2, train_labels2 = read_some_npzs_and_preprocess(train_file_list[map_batch * data_split_count : (map_batch+1) * data_split_count]);\n",
    "                (new_train_data, new_div_data, new_train_labels), (test_data, test_div_data, test_labels) = train_test_split(train_data2, div_data2, train_labels2);\n",
    "            else:\n",
    "                new_train_data, new_div_data, new_train_labels = read_some_npzs_and_preprocess(train_file_list[map_batch * data_split_count : (map_batch+1) * data_split_count]);\n",
    "            \n",
    "            history = model.fit([new_train_data, new_div_data], new_train_labels, epochs=1,\n",
    "                                validation_split=0.2, verbose=0, #batch_size=477,\n",
    "                                callbacks=[])\n",
    "            # Manually print the dot\n",
    "            print('.', end='');\n",
    "        print('');\n",
    "\n",
    "[loss, mae] = model.evaluate([test_data, test_div_data], test_labels, verbose=0)\n",
    "\n",
    "print(\"\\nTesting set Mean Abs Error: {}\".format(mae))\n",
    "\n",
    "\n",
    "# print(test_predictions)\n",
    "# print(test_labels)\n",
    "# print(test_predictions - list(test_labels))\n",
    "# print(\"Mean Abs Error: \"+str(np.mean(np.abs(test_predictions - test_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Print the testing accuracy of the model (using F1-score), and compare with the accuracy of a random result, then in addition print the accuracy of individual columns.\n",
    "\n",
    "For the Sota dataset, it should get around 0.6 overall score, 0.77 for is_note, and something much smaller for is_circle, is_slider and is_spinner. It may also throw a warning because there is no spinner predicted/actually present.\n",
    "\n",
    "This is not a very high accuracy - but it is not really a problem; map rhythm does not fully correlate to the music itself. There are overmaps, innovative rhythms... and we can also learn from some of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "test_predictions = model.predict([test_data, test_div_data]).reshape((-1, time_interval, label_shape[1]))\n",
    "\n",
    "flat_test_preds = test_predictions.reshape(-1, label_shape[1]);\n",
    "flat_test_labels = test_labels.reshape(-1, label_shape[1]);\n",
    "\n",
    "pred_result = (np.sign(flat_test_preds) + 1) / 2\n",
    "actual_result = (flat_test_labels + 1) / 2\n",
    "\n",
    "random_result = (1 + np.sign(-1 + 2 * np.random.random(size=pred_result.shape))) / 2;\n",
    "\n",
    "is_obj_pred = (1 + np.sign(flat_test_preds[:, 0:1])) / 2;\n",
    "obj_type_pred = np.sign(flat_test_preds[:, 1:4] - np.tile(np.expand_dims(np.max(flat_test_preds[:, 1:4], axis=1), 1), (1, 3))) + 1;\n",
    "others_pred = (1 + np.sign(flat_test_preds[:, 4:label_shape[1]] + 0.5)) / 2;\n",
    "# Only predict obj_type when there is an object!\n",
    "another_pred_result = np.concatenate([is_obj_pred, is_obj_pred * obj_type_pred, others_pred], axis=1);\n",
    "print(f1_score(actual_result.flatten(), pred_result.flatten()));\n",
    "print(f1_score(actual_result.flatten(), another_pred_result.flatten()));\n",
    "print(f1_score(actual_result.flatten(), random_result.flatten()));\n",
    "\n",
    "# Individual column predictions\n",
    "column_names = [\"is_note\", \"is_circle\", \"is_slider\", \"is_spinner\", \"is_sliding\", \"is_spinning\"];\n",
    "for i, k in enumerate(column_names):\n",
    "    print(\"{} f1_score: {} from {}\".format(k, f1_score(another_pred_result[:, i], actual_result[:, i]), f1_score(random_result[:, i], actual_result[:, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# emmmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "出来上がり☆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    \"saved_rhythm_model\",\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ");\n",
    "\n",
    "# WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer\n",
    "# state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will\n",
    "# have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
