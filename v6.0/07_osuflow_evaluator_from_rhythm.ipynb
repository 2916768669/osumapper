{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2018/8/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "#### First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://i.imgur.com/Ko2wogO.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "Note that this GAN is irrelevant to the music, not the time interval, only the coordinates of notes themselves.\n",
    "\n",
    "Probably could get some slider coordinates inbetween? this way it may learn something about slider shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# this line below can only run once in the notebook! otherwise it will cause errors\n",
    "try:\n",
    "    tf.enable_eager_execution();\n",
    "except:\n",
    "    pass\n",
    "tfe = tf.contrib.eager;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some parameters related to GAN training.\n",
    "\n",
    "May or may not affect the result; haven't experimented a lot about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GAN_PARAMS = {\n",
    "    \"divisor\" : 4,\n",
    "    \"good_epoch\" : 6,\n",
    "    \"max_epoch\" : 25,\n",
    "    \"note_group_size\" : 10,\n",
    "    \"g_epochs\" : 7,\n",
    "    \"c_epochs\" : 3,\n",
    "    \"g_batch\" : 50,\n",
    "    \"g_input_size\" : 50,\n",
    "    \"c_true_batch\" : 50,\n",
    "    \"c_false_batch\" : 10,\n",
    "    \"slider_max_ticks\" : 8,\n",
    "    \"next_from_slider_end\" : False\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "#### Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, unfiltered_sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "first_step_sv =          unfiltered_sv[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "slider_max_ticks = GAN_PARAMS[\"slider_max_ticks\"];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + slider_max_ticks + 1: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "sv =          first_step_sv[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# get divisor from GAN_PARAMS\n",
    "divisor = GAN_PARAMS[\"divisor\"];\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // divisor;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 5, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For distances starting from slider ends\n",
    "tick_lengths_pre = (timestamps[1:] - timestamps[:-1]) / (ticks[1:] - ticks[:-1]);\n",
    "tick_lengths = np.concatenate([tick_lengths_pre, [tick_lengths_pre[-1]]]);\n",
    "timestamps_note_end = timestamps + slider_ticks * tick_lengths;\n",
    "\n",
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "\n",
    "if GAN_PARAMS[\"next_from_slider_end\"]:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps_note_end;\n",
    "    timestamps_before = np.concatenate([[6662], timestamps_after[:-1]]); # why 6662????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "else:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps;\n",
    "    timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plotting functions found from stackoverflow. Probably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib.text as mtext\n",
    "\n",
    "\n",
    "class MyLine(lines.Line2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # we'll update the position when the line data is set\n",
    "        self.text = mtext.Text(0, 0, '')\n",
    "        lines.Line2D.__init__(self, *args, **kwargs)\n",
    "\n",
    "        # we can't access the label attr until *after* the line is\n",
    "        # inited\n",
    "        self.text.set_text(self.get_label())\n",
    "\n",
    "    def set_figure(self, figure):\n",
    "        self.text.set_figure(figure)\n",
    "        lines.Line2D.set_figure(self, figure)\n",
    "\n",
    "    def set_axes(self, axes):\n",
    "        self.text.set_axes(axes)\n",
    "        lines.Line2D.set_axes(self, axes)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        # 2 pixel offset\n",
    "        texttrans = transform + mtransforms.Affine2D().translate(2, 2)\n",
    "        self.text.set_transform(texttrans)\n",
    "        lines.Line2D.set_transform(self, transform)\n",
    "\n",
    "    def set_data(self, x, y):\n",
    "        if len(x):\n",
    "            self.text.set_position((x[-1], y[-1]))\n",
    "\n",
    "        lines.Line2D.set_data(self, x, y)\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        # draw my label at the end of the line with 2 pixel offset\n",
    "        lines.Line2D.draw(self, renderer)\n",
    "        self.text.draw(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import numpy as np;\n",
    "\n",
    "root = \"mapdata/\";\n",
    "\n",
    "chunk_size = GAN_PARAMS[\"note_group_size\"];\n",
    "step_size = 5;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define some sort of plotting functions to show the generator and discriminator losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Limitless]')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# actual_train_data = np.concatenate((special_train_data[0:1], special_false_data[0:1]), axis=0);\n",
    "# actual_train_labels = np.concatenate((special_train_labels[0:1], special_false_labels[0:1]), axis=0);\n",
    "\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0.2)) * tf.square(0.3 - vg);\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 0.8)) * tf.square(vg - 0.7);\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    #priority = tf.constant(list(k**2 for k in range(vg.shape[0], 0, -1)))\n",
    "    wall_var_l = tf.to_float(tf.less(vg, 0));\n",
    "    wall_var_r = tf.to_float(tf.greater(vg, 1));\n",
    "#     wall_var_ins = -1 * tf.to_float(tf.greater(vg, 0.2)) * tf.to_float(tf.less(vg, 0.8)) * tf.square(vg - 0.5);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "    var_tensor = tf.to_float(var_tensor);\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_tensor.shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    batch_size = var_tensor.shape[0];\n",
    "    note_distances_now = length_multiplier * np.tile(np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "    note_angles_now = np.tile(np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0), (batch_size, 1));\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "    \n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "    \n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.to_float(tf.greater(l, y_max / 2));\n",
    "    not_rerand = tf.to_float(tf.less_equal(l, y_max / 2));\n",
    "    \n",
    "    next_from_slider_end = extvar[\"next_from_slider_end\"];\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _px, _py = extvar[\"start_pos\"];\n",
    "    else:\n",
    "        _px = 256;\n",
    "        _py = 192;\n",
    "    _pa = 0;\n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = 256;\n",
    "    _y = 192;\n",
    "    for k in range(half_tensor):\n",
    "        note_index = begin_offset + k;\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l = tf.to_float(tf.less(_px, wall_l[:, k]));\n",
    "        wall_value_r = tf.to_float(tf.greater(_px, wall_r[:, k]));\n",
    "        wall_value_xmid = tf.to_float(tf.greater(_px, wall_l[:, k])) * tf.to_float(tf.less(_px, wall_r[:, k]));\n",
    "        wall_value_t = tf.to_float(tf.less(_py, wall_t[:, k]));\n",
    "        wall_value_b = tf.to_float(tf.greater(_py, wall_b[:, k]));\n",
    "        wall_value_ymid = tf.to_float(tf.greater(_py, wall_t[:, k])) * tf.to_float(tf.less(_py, wall_b[:, k]));\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        if is_slider[note_index]:\n",
    "            sln = slider_lengths[note_index];\n",
    "            slider_type = slider_types[note_index];\n",
    "            scos = slider_cos[slider_type];\n",
    "            ssin = slider_sin[slider_type];\n",
    "            _a = cos_list[:, k + half_tensor];\n",
    "            _b = sin_list[:, k + half_tensor];\n",
    "            # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "            # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "            _oa = _a * scos - _b * ssin;\n",
    "            _ob = _a * ssin + _b * scos;\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "            out.append(cp);\n",
    "            if next_from_slider_end:\n",
    "                _px = _x + _a * sln;\n",
    "                _py = _y + _b * sln;\n",
    "            else:\n",
    "                _px = _x;\n",
    "                _py = _y;\n",
    "        else:\n",
    "            _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "            _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "            cp = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "            out.append(cp);\n",
    "            _px = _x;\n",
    "            _py = _y;\n",
    "        _pa = tf.mod(_pa + 0, 6.283);\n",
    "    return tf.transpose(tf.stack(out, axis=0), [1, 0, 2]);\n",
    "\n",
    "def stack_loss(tensor):\n",
    "    complex_list = tf.complex(tensor[:, :, 0] * 512, tensor[:, :, 1] * 384);\n",
    "    stack_limit = 30;\n",
    "    precise_limit = 1;\n",
    "    a = [];\n",
    "    for k in range(tensor.shape[1]):\n",
    "        w = tf.tile(tf.expand_dims(complex_list[:, k], axis=1), [1, tensor.shape[1]]);\n",
    "        r = tf.abs(w - complex_list);\n",
    "        rless = tf.to_float(tf.less(r, stack_limit)) * tf.to_float(tf.greater(r, precise_limit));\n",
    "        rmean = tf.reduce_mean(rless * (stack_limit - r) / stack_limit);\n",
    "        a.append(rmean);\n",
    "    b = tf.reduce_sum(a);\n",
    "#         print(tf.tile(w, [1, tensor.shape[1]]));\n",
    "#         print(complex_list);\n",
    "    return b;\n",
    "\n",
    "# This polygon loss was an attempt to make the map less likely to overlap each other.\n",
    "# The idea is: calculate the area of polygon formed from the note positions;\n",
    "# If it is big, then it is good - they form a convex shape, no overlap.\n",
    "# ... of course it totally doesn't work like that.\n",
    "def polygon_loss(tensor):\n",
    "    tensor_this = tensor[:, :, 0:2];\n",
    "    tensor_next = tf.concat([tensor[:, 1:, 0:2], tensor[:, 0:1, 0:2]], axis=1);\n",
    "    sa = (tensor_this[:, :, 0] + tensor_next[:, :, 0]) * (tensor_next[:, :, 1] - tensor_this[:, :, 0]);\n",
    "    surface = tf.abs(tf.reduce_sum(sa, axis=1))/2;\n",
    "    return surface;\n",
    "\n",
    "def construct_map_and_calc_loss(var_tensor, extvar):\n",
    "    # first make a map from the outputs of generator, then ask the classifier (discriminator) to classify it\n",
    "    classifier_model = extvar[\"classifier_model\"]\n",
    "    out = construct_map_with_sliders(var_tensor, extvar=extvar);\n",
    "    cm = classifier_model(out);\n",
    "    predmean = 1 - tf.reduce_mean(cm, axis=1);\n",
    "#    regulator = tf.reduce_mean(tf.reduce_mean(- 0.1 * tf.square(out[:, :, 1:2] - 0.5), axis=2), axis=1); # * tf.square(out[:, :, 1:2] - 2)\n",
    "    box_loss = inblock_loss(out[:, :, 0:2]);\n",
    "    box_loss2 = inblock_loss(out[:, :, 4:6]);\n",
    "#     polygon = polygon_loss(out);\n",
    "    # print(out.shape); shape is (10, X, 4)\n",
    "    #return predmean + box_loss*100; <-- this was too harsh, in fact breaking SGD and creating waveforms on the borderline\n",
    "    return predmean + box_loss + box_loss2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "This will take some time. It splits map into groups of 10 (10 by default), and trains GAN to play with the flow of maps in the dataset. It will train for floor(note_count / 10) groups, for example, if you have 820 then it needs to train 82 groups.\n",
    "\n",
    "Some notes are converted to sliders, so it has less than the predicted count in #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "XdfkR223D9dW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of groups: 75\n",
      "Group 0, Epoch 1: G loss: 0.42647273795945306 vs. C loss: 0.21550278034475112\n",
      "Group 0, Epoch 2: G loss: 0.35935096740722655 vs. C loss: 0.1420758921239111\n",
      "Group 0, Epoch 3: G loss: 0.5383697152137755 vs. C loss: 0.20748698380258349\n",
      "Group 0, Epoch 4: G loss: 0.10258631216628211 vs. C loss: 0.20467817617787257\n",
      "Group 0, Epoch 5: G loss: 0.0653033543910299 vs. C loss: 0.19386824137634703\n",
      "Group 0, Epoch 6: G loss: 0.13604655138083868 vs. C loss: 0.13956065641509163\n",
      "Group 1, Epoch 1: G loss: 0.24536054730415344 vs. C loss: 0.19670254985491434\n",
      "Group 1, Epoch 2: G loss: 0.24881456111158642 vs. C loss: 0.18321099297867882\n",
      "Group 1, Epoch 3: G loss: 0.19879445689065114 vs. C loss: 0.19254717727502188\n",
      "Group 1, Epoch 4: G loss: 0.08969145278845513 vs. C loss: 0.19473279184765288\n",
      "Group 1, Epoch 5: G loss: 0.08537816522376876 vs. C loss: 0.20438613494237265\n",
      "Group 1, Epoch 6: G loss: 0.05618708421077047 vs. C loss: 0.19601770242055258\n",
      "Group 2, Epoch 1: G loss: 0.2721729891640799 vs. C loss: 0.18589213656054604\n",
      "Group 2, Epoch 2: G loss: 0.4321210052285876 vs. C loss: 0.10215452023678356\n",
      "Group 2, Epoch 3: G loss: 0.5142014090503965 vs. C loss: 0.159699570801523\n",
      "Group 2, Epoch 4: G loss: 0.4394903036100524 vs. C loss: 0.2023676600721147\n",
      "Group 2, Epoch 5: G loss: 0.23132518913064684 vs. C loss: 0.13449577904409832\n",
      "Group 2, Epoch 6: G loss: 0.4738936999014446 vs. C loss: 0.1958871285120646\n",
      "Group 2, Epoch 7: G loss: 0.4192825768675123 vs. C loss: 0.07390528834528394\n",
      "Group 2, Epoch 8: G loss: 0.3987013467720577 vs. C loss: 0.23510712054040694\n",
      "Group 2, Epoch 9: G loss: 0.030117303133010868 vs. C loss: 0.20680642127990723\n",
      "Group 3, Epoch 1: G loss: 0.5164229478154864 vs. C loss: 0.2943685402472814\n",
      "Group 3, Epoch 2: G loss: 0.3845392857279096 vs. C loss: 0.17116216984060076\n",
      "Group 3, Epoch 3: G loss: 0.31655784462179454 vs. C loss: 0.18040731052557626\n",
      "Group 3, Epoch 4: G loss: 0.24782150856086185 vs. C loss: 0.19555879135926565\n",
      "Group 3, Epoch 5: G loss: 0.10185743144580296 vs. C loss: 0.1768668012486564\n",
      "Group 3, Epoch 6: G loss: 0.15994439486946374 vs. C loss: 0.18050358858373428\n",
      "Group 4, Epoch 1: G loss: 0.3006703823804856 vs. C loss: 0.15785867389705446\n",
      "Group 4, Epoch 2: G loss: 0.29440920012337823 vs. C loss: 0.16433021260632408\n",
      "Group 4, Epoch 3: G loss: 0.24271438079220906 vs. C loss: 0.1844724259442753\n",
      "Group 4, Epoch 4: G loss: 0.18286851112331662 vs. C loss: 0.20688951015472412\n",
      "Group 4, Epoch 5: G loss: 0.10743657095091684 vs. C loss: 0.18144964509540132\n",
      "Group 4, Epoch 6: G loss: 0.16918824953692302 vs. C loss: 0.159955196082592\n",
      "Group 5, Epoch 1: G loss: 0.3208961222852979 vs. C loss: 0.22729210389984977\n",
      "Group 5, Epoch 2: G loss: 0.3703481623104641 vs. C loss: 0.14468053976694745\n",
      "Group 5, Epoch 3: G loss: 0.3509340580020632 vs. C loss: 0.12864742759201261\n",
      "Group 5, Epoch 4: G loss: 0.3169069760612078 vs. C loss: 0.20369793640242684\n",
      "Group 5, Epoch 5: G loss: 0.08447669308100426 vs. C loss: 0.1911451667547226\n",
      "Group 5, Epoch 6: G loss: 0.22085278630256658 vs. C loss: 0.15196033153269026\n",
      "Group 5, Epoch 7: G loss: 0.48296398861067635 vs. C loss: 0.12267429961098565\n",
      "Group 6, Epoch 1: G loss: 0.24982651003769465 vs. C loss: 0.20018242299556732\n",
      "Group 6, Epoch 2: G loss: 0.2667782872915268 vs. C loss: 0.14821444948514303\n",
      "Group 6, Epoch 3: G loss: 0.5008178923811231 vs. C loss: 0.1054167209400071\n",
      "Group 6, Epoch 4: G loss: 0.3569593506199973 vs. C loss: 0.15335426562362245\n",
      "Group 6, Epoch 5: G loss: 0.5837051068033492 vs. C loss: 0.035345149123006396\n",
      "Group 6, Epoch 6: G loss: 0.9087825332369123 vs. C loss: 0.03354643088661962\n",
      "Group 6, Epoch 7: G loss: 0.47492762037685937 vs. C loss: 0.14356197044253352\n",
      "Group 6, Epoch 8: G loss: 0.7050110476357597 vs. C loss: 0.013123386953439979\n",
      "Group 6, Epoch 9: G loss: 0.7082247998033252 vs. C loss: 0.10841598071985774\n",
      "Group 6, Epoch 10: G loss: 0.5913758644035886 vs. C loss: 0.10019332931066553\n",
      "Group 7, Epoch 1: G loss: 0.4159998621259417 vs. C loss: 0.1706506535410881\n",
      "Group 7, Epoch 2: G loss: 0.23417677304574425 vs. C loss: 0.1974448031849331\n",
      "Group 7, Epoch 3: G loss: 0.09896832172359739 vs. C loss: 0.17581908570395577\n",
      "Group 7, Epoch 4: G loss: 0.19318584118570598 vs. C loss: 0.12929761740896437\n",
      "Group 7, Epoch 5: G loss: 0.31527112807546337 vs. C loss: 0.2320538428094652\n",
      "Group 7, Epoch 6: G loss: 0.049450707542044774 vs. C loss: 0.19702440169122484\n",
      "Group 8, Epoch 1: G loss: 0.29823567995003286 vs. C loss: 0.20120458967155883\n",
      "Group 8, Epoch 2: G loss: 0.2703839753355298 vs. C loss: 0.13632503400246304\n",
      "Group 8, Epoch 3: G loss: 0.4174154145377023 vs. C loss: 0.10325076679388683\n",
      "Group 8, Epoch 4: G loss: 0.4524760442120689 vs. C loss: 0.10557187596956889\n",
      "Group 8, Epoch 5: G loss: 0.7411000064441134 vs. C loss: 0.09689900361829334\n",
      "Group 8, Epoch 6: G loss: 0.4668056209172521 vs. C loss: 0.21322950472434363\n",
      "Group 8, Epoch 7: G loss: 0.044260182870285845 vs. C loss: 0.2078783611456553\n",
      "Group 9, Epoch 1: G loss: 0.24347257316112522 vs. C loss: 0.22013083265887368\n",
      "Group 9, Epoch 2: G loss: 0.21839845989431655 vs. C loss: 0.1841806603802575\n",
      "Group 9, Epoch 3: G loss: 0.17252502036946166 vs. C loss: 0.20087372263272604\n",
      "Group 9, Epoch 4: G loss: 0.07627369889191218 vs. C loss: 0.19991840918858847\n",
      "Group 9, Epoch 5: G loss: 0.04389188406722887 vs. C loss: 0.2002316473258866\n",
      "Group 9, Epoch 6: G loss: 0.03833870451365199 vs. C loss: 0.19940907259782156\n",
      "Group 10, Epoch 1: G loss: 0.29886995809418815 vs. C loss: 0.20112094779809317\n",
      "Group 10, Epoch 2: G loss: 0.29622462136404853 vs. C loss: 0.14919858715600437\n",
      "Group 10, Epoch 3: G loss: 0.29926123789378567 vs. C loss: 0.16832003328535294\n",
      "Group 10, Epoch 4: G loss: 0.31484379917383193 vs. C loss: 0.20122846961021423\n",
      "Group 10, Epoch 5: G loss: 0.18296149500778744 vs. C loss: 0.20724568764368692\n",
      "Group 10, Epoch 6: G loss: 0.05349519199558668 vs. C loss: 0.20146243108643425\n",
      "Group 11, Epoch 1: G loss: 0.4212881173406328 vs. C loss: 0.22624479979276657\n",
      "Group 11, Epoch 2: G loss: 0.28570011769022263 vs. C loss: 0.14580571817027196\n",
      "Group 11, Epoch 3: G loss: 0.34307381681033544 vs. C loss: 0.15046032269795737\n",
      "Group 11, Epoch 4: G loss: 0.4506497012717383 vs. C loss: 0.2227338320679135\n",
      "Group 11, Epoch 5: G loss: 0.07401896033968244 vs. C loss: 0.20940622025065955\n",
      "Group 11, Epoch 6: G loss: 0.0202896529010364 vs. C loss: 0.20438814494344923\n",
      "Group 12, Epoch 1: G loss: 0.39168557950428556 vs. C loss: 0.21025773551728988\n",
      "Group 12, Epoch 2: G loss: 0.2593073742730277 vs. C loss: 0.19192453080581293\n",
      "Group 12, Epoch 3: G loss: 0.10736120194196701 vs. C loss: 0.19204665223757425\n",
      "Group 12, Epoch 4: G loss: 0.09005903761301723 vs. C loss: 0.20510545372962952\n",
      "Group 12, Epoch 5: G loss: 0.05083531279649053 vs. C loss: 0.19862839217401215\n",
      "Group 12, Epoch 6: G loss: 0.10197809317282269 vs. C loss: 0.1644504815340042\n",
      "Group 13, Epoch 1: G loss: 0.5017701779093061 vs. C loss: 0.2391467640797297\n",
      "Group 13, Epoch 2: G loss: 0.365008978332792 vs. C loss: 0.132878122644292\n",
      "Group 13, Epoch 3: G loss: 0.517243641614914 vs. C loss: 0.14128713475333318\n",
      "Group 13, Epoch 4: G loss: 0.35439288786479406 vs. C loss: 0.21409089697731865\n",
      "Group 13, Epoch 5: G loss: 0.10321890456335885 vs. C loss: 0.20594030618667603\n",
      "Group 13, Epoch 6: G loss: 0.0275631099407162 vs. C loss: 0.203937033812205\n",
      "Group 14, Epoch 1: G loss: 0.22588395689214977 vs. C loss: 0.20537814994653067\n",
      "Group 14, Epoch 2: G loss: 0.199932248038905 vs. C loss: 0.17818470133675468\n",
      "Group 14, Epoch 3: G loss: 0.1801140391400882 vs. C loss: 0.1710242885682318\n",
      "Group 14, Epoch 4: G loss: 0.2118250131607056 vs. C loss: 0.1732837077644136\n",
      "Group 14, Epoch 5: G loss: 0.19026800159897123 vs. C loss: 0.20095891257127127\n",
      "Group 14, Epoch 6: G loss: 0.15610709020069666 vs. C loss: 0.16446734964847565\n",
      "Group 14, Epoch 7: G loss: 0.455183870451791 vs. C loss: 0.10097784962919022\n",
      "Group 14, Epoch 8: G loss: 0.5253599443605969 vs. C loss: 0.20809645288520393\n",
      "Group 15, Epoch 1: G loss: 0.37800818426268445 vs. C loss: 0.20621601823303437\n",
      "Group 15, Epoch 2: G loss: 0.3079188334090369 vs. C loss: 0.17281223667992485\n",
      "Group 15, Epoch 3: G loss: 0.25467434227466584 vs. C loss: 0.1341000207596355\n",
      "Group 15, Epoch 4: G loss: 0.7226823943001884 vs. C loss: 0.05953585439258152\n",
      "Group 15, Epoch 5: G loss: 0.6148186981678009 vs. C loss: 0.042184974791275136\n",
      "Group 15, Epoch 6: G loss: 0.627087285901819 vs. C loss: 0.21756397849983636\n",
      "Group 15, Epoch 7: G loss: 0.06726834986891066 vs. C loss: 0.20783125940296385\n",
      "Group 15, Epoch 8: G loss: 0.05054494099957602 vs. C loss: 0.2079473783572515\n",
      "Group 15, Epoch 9: G loss: 0.034001501064215386 vs. C loss: 0.20827943086624146\n",
      "Group 16, Epoch 1: G loss: 0.24380723067692348 vs. C loss: 0.1914849090907309\n",
      "Group 16, Epoch 2: G loss: 0.23914345375129153 vs. C loss: 0.15760007169511583\n",
      "Group 16, Epoch 3: G loss: 0.2435312962957791 vs. C loss: 0.20395453770955405\n",
      "Group 16, Epoch 4: G loss: 0.061760346112506734 vs. C loss: 0.21120291948318481\n",
      "Group 16, Epoch 5: G loss: 0.022264749237469263 vs. C loss: 0.20231206549538505\n",
      "Group 16, Epoch 6: G loss: 0.03188285886176995 vs. C loss: 0.19155990994638863\n",
      "Group 17, Epoch 1: G loss: 0.5240694752761296 vs. C loss: 0.22402094221777388\n",
      "Group 17, Epoch 2: G loss: 0.36664062568119593 vs. C loss: 0.15788588176170984\n",
      "Group 17, Epoch 3: G loss: 0.32266320075307575 vs. C loss: 0.1008447880546252\n",
      "Group 17, Epoch 4: G loss: 0.5461572059563228 vs. C loss: 0.08009023633268145\n",
      "Group 17, Epoch 5: G loss: 0.7358526791845049 vs. C loss: 0.1162615091436439\n",
      "Group 17, Epoch 6: G loss: 0.36885736520801277 vs. C loss: 0.22848779294225907\n",
      "Group 18, Epoch 1: G loss: 0.2991147288254329 vs. C loss: 0.15620984302626714\n",
      "Group 18, Epoch 2: G loss: 0.27198089680501397 vs. C loss: 0.18323602610164216\n",
      "Group 18, Epoch 3: G loss: 0.17087422375168118 vs. C loss: 0.13651759839720198\n",
      "Group 18, Epoch 4: G loss: 0.582567948102951 vs. C loss: 0.17509159942468008\n",
      "Group 18, Epoch 5: G loss: 0.24995447184358324 vs. C loss: 0.20726367996798623\n",
      "Group 18, Epoch 6: G loss: 0.054958689212799075 vs. C loss: 0.20585764282279548\n",
      "Group 18, Epoch 7: G loss: 0.03810659572482109 vs. C loss: 0.20284935666455162\n",
      "Group 18, Epoch 8: G loss: 0.05573064546499934 vs. C loss: 0.18513762123054925\n",
      "Group 18, Epoch 9: G loss: 0.20514969272272926 vs. C loss: 0.1277907970878813\n",
      "Group 18, Epoch 10: G loss: 0.45081651636532377 vs. C loss: 0.1004511271086004\n",
      "Group 18, Epoch 11: G loss: 0.5863303712436132 vs. C loss: 0.08715021444691552\n",
      "Group 18, Epoch 12: G loss: 0.18794235565832684 vs. C loss: 0.207715254690912\n",
      "Group 18, Epoch 13: G loss: 0.031102978384920526 vs. C loss: 0.2075748410489824\n",
      "Group 18, Epoch 14: G loss: 0.029064340304051122 vs. C loss: 0.20665500478612053\n",
      "Group 18, Epoch 15: G loss: 0.02888766319624015 vs. C loss: 0.20483297771877715\n",
      "Group 18, Epoch 16: G loss: 0.042887746436255314 vs. C loss: 0.17459439900186327\n",
      "Group 18, Epoch 17: G loss: 0.5366622652326312 vs. C loss: 0.12357310785187614\n",
      "Group 18, Epoch 18: G loss: 0.16626984104514123 vs. C loss: 0.21486252256565627\n",
      "Group 19, Epoch 1: G loss: 0.2883281260728836 vs. C loss: 0.21630391809675428\n",
      "Group 19, Epoch 2: G loss: 0.23532936232430596 vs. C loss: 0.13125302394231161\n",
      "Group 19, Epoch 3: G loss: 0.3879756599664689 vs. C loss: 0.21237152483728197\n",
      "Group 19, Epoch 4: G loss: 0.059133152770144595 vs. C loss: 0.20142042802439794\n",
      "Group 19, Epoch 5: G loss: 0.022614521586469243 vs. C loss: 0.2036162962516149\n",
      "Group 19, Epoch 6: G loss: 0.024373878645045415 vs. C loss: 0.19842150921208992\n",
      "Group 20, Epoch 1: G loss: 0.2703435242176056 vs. C loss: 0.18653676741653022\n",
      "Group 20, Epoch 2: G loss: 0.31353939771652223 vs. C loss: 0.14631428900692198\n",
      "Group 20, Epoch 3: G loss: 0.3633340452398572 vs. C loss: 0.1289104918638865\n",
      "Group 20, Epoch 4: G loss: 0.47178722577435633 vs. C loss: 0.2012587851948208\n",
      "Group 20, Epoch 5: G loss: 0.042343729894076074 vs. C loss: 0.2281254678964615\n",
      "Group 20, Epoch 6: G loss: 0.012719117011874916 vs. C loss: 0.2102078952723079\n",
      "Group 21, Epoch 1: G loss: 0.31363318519932876 vs. C loss: 0.21426708830727473\n",
      "Group 21, Epoch 2: G loss: 0.3996530336993081 vs. C loss: 0.12747100575102702\n",
      "Group 21, Epoch 3: G loss: 0.36914485437529426 vs. C loss: 0.1881746206846502\n",
      "Group 21, Epoch 4: G loss: 0.1247246137687138 vs. C loss: 0.1694664027955797\n",
      "Group 21, Epoch 5: G loss: 0.16850033051201274 vs. C loss: 0.20782368878523508\n",
      "Group 21, Epoch 6: G loss: 0.059815963410905434 vs. C loss: 0.20663470888717306\n",
      "Group 22, Epoch 1: G loss: 0.26326686612197336 vs. C loss: 0.1791123565700319\n",
      "Group 22, Epoch 2: G loss: 0.38835779769080025 vs. C loss: 0.09849422838952805\n",
      "Group 22, Epoch 3: G loss: 0.5305325627326967 vs. C loss: 0.03869582464297613\n",
      "Group 22, Epoch 4: G loss: 0.6410470802869116 vs. C loss: 0.11418476079901062\n",
      "Group 22, Epoch 5: G loss: 0.5762259150190012 vs. C loss: 0.22989947100480398\n",
      "Group 22, Epoch 6: G loss: 0.04321217834949493 vs. C loss: 0.20858130438460243\n",
      "Group 23, Epoch 1: G loss: 0.3513937439237322 vs. C loss: 0.23090313043859267\n",
      "Group 23, Epoch 2: G loss: 0.25477521078927173 vs. C loss: 0.14026659064822727\n",
      "Group 23, Epoch 3: G loss: 0.42672650899205894 vs. C loss: 0.10939519604047139\n",
      "Group 23, Epoch 4: G loss: 0.5868074655532837 vs. C loss: 0.05039172474708822\n",
      "Group 23, Epoch 5: G loss: 0.4349005926932608 vs. C loss: 0.19969748457272848\n",
      "Group 23, Epoch 6: G loss: 0.07144059773002352 vs. C loss: 0.2102244347333908\n",
      "Group 24, Epoch 1: G loss: 0.4912770850317819 vs. C loss: 0.20110395716296303\n",
      "Group 24, Epoch 2: G loss: 0.24888971958841596 vs. C loss: 0.1979051451716158\n",
      "Group 24, Epoch 3: G loss: 0.08071068365659032 vs. C loss: 0.18673943893777\n",
      "Group 24, Epoch 4: G loss: 0.10474579781293869 vs. C loss: 0.20603992707199517\n",
      "Group 24, Epoch 5: G loss: 0.08066505374653 vs. C loss: 0.20282272001107535\n",
      "Group 24, Epoch 6: G loss: 0.07723978119237082 vs. C loss: 0.17877275745073953\n",
      "Group 25, Epoch 1: G loss: 0.3858561532838004 vs. C loss: 0.1493768547144201\n",
      "Group 25, Epoch 2: G loss: 0.2575313006128584 vs. C loss: 0.16086464954747096\n",
      "Group 25, Epoch 3: G loss: 0.496543289082391 vs. C loss: 0.10073158476087783\n",
      "Group 25, Epoch 4: G loss: 0.3737815124647958 vs. C loss: 0.12895076721906662\n",
      "Group 25, Epoch 5: G loss: 0.46108116550104966 vs. C loss: 0.10045426442391342\n",
      "Group 25, Epoch 6: G loss: 0.6644479887826102 vs. C loss: 0.039731375459167696\n",
      "Group 26, Epoch 1: G loss: 0.4705077239445277 vs. C loss: 0.22209128489096955\n",
      "Group 26, Epoch 2: G loss: 0.37520754933357237 vs. C loss: 0.15834749986728033\n",
      "Group 26, Epoch 3: G loss: 0.3257442440305437 vs. C loss: 0.1450742764605416\n",
      "Group 26, Epoch 4: G loss: 0.4340484832014356 vs. C loss: 0.09359386273556286\n",
      "Group 26, Epoch 5: G loss: 0.532714934859957 vs. C loss: 0.1964902298318015\n",
      "Group 26, Epoch 6: G loss: 0.11205465969230448 vs. C loss: 0.21328555544217428\n",
      "Group 27, Epoch 1: G loss: 0.3143584055559976 vs. C loss: 0.1965741084681617\n",
      "Group 27, Epoch 2: G loss: 0.3529638971601214 vs. C loss: 0.15803091145224038\n",
      "Group 27, Epoch 3: G loss: 0.3036940370287214 vs. C loss: 0.13758830891715154\n",
      "Group 27, Epoch 4: G loss: 0.4055060118436814 vs. C loss: 0.23505263361665937\n",
      "Group 27, Epoch 5: G loss: 0.10989995300769806 vs. C loss: 0.1764299886094199\n",
      "Group 27, Epoch 6: G loss: 0.2858583590814045 vs. C loss: 0.11896972523795235\n",
      "Group 28, Epoch 1: G loss: 0.5000745875494822 vs. C loss: 0.21416494415866003\n",
      "Group 28, Epoch 2: G loss: 0.3573628655501775 vs. C loss: 0.17054173350334167\n",
      "Group 28, Epoch 3: G loss: 0.1679990677961282 vs. C loss: 0.21039032273822364\n",
      "Group 28, Epoch 4: G loss: 0.04838332608342171 vs. C loss: 0.20226459701855978\n",
      "Group 28, Epoch 5: G loss: 0.04181187248655728 vs. C loss: 0.19655994243092004\n",
      "Group 28, Epoch 6: G loss: 0.08521931384290969 vs. C loss: 0.15761538098255792\n",
      "Group 29, Epoch 1: G loss: 0.3266050530331475 vs. C loss: 0.21493435568279692\n",
      "Group 29, Epoch 2: G loss: 0.2991541304758617 vs. C loss: 0.1988322652048535\n",
      "Group 29, Epoch 3: G loss: 0.14540979564189913 vs. C loss: 0.13419461498657861\n",
      "Group 29, Epoch 4: G loss: 0.5244988015719823 vs. C loss: 0.0709321399529775\n",
      "Group 29, Epoch 5: G loss: 0.42229433826037815 vs. C loss: 0.16626023418373534\n",
      "Group 29, Epoch 6: G loss: 0.21537784444434302 vs. C loss: 0.16071234333018464\n",
      "Group 29, Epoch 7: G loss: 0.4476405284234456 vs. C loss: 0.16401133996744952\n",
      "Group 30, Epoch 1: G loss: 0.4031236776283809 vs. C loss: 0.18984508183267382\n",
      "Group 30, Epoch 2: G loss: 0.35517186607633316 vs. C loss: 0.13462930503818724\n",
      "Group 30, Epoch 3: G loss: 0.24762301189558847 vs. C loss: 0.22523469809028837\n",
      "Group 30, Epoch 4: G loss: 0.049745642766356464 vs. C loss: 0.20661858055326676\n",
      "Group 30, Epoch 5: G loss: 0.02168938155685152 vs. C loss: 0.20195714632670084\n",
      "Group 30, Epoch 6: G loss: 0.027168874761887955 vs. C loss: 0.19228859908050963\n",
      "Group 31, Epoch 1: G loss: 0.4077781813485282 vs. C loss: 0.22889939943949386\n",
      "Group 31, Epoch 2: G loss: 0.2742579502718789 vs. C loss: 0.14213288409842387\n",
      "Group 31, Epoch 3: G loss: 0.3757298478058407 vs. C loss: 0.09273393038246365\n",
      "Group 31, Epoch 4: G loss: 0.5969904184341431 vs. C loss: 0.0591128265692128\n",
      "Group 31, Epoch 5: G loss: 0.5321400693484716 vs. C loss: 0.12656833893722957\n",
      "Group 31, Epoch 6: G loss: 0.3179812925202506 vs. C loss: 0.21689104702737597\n",
      "Group 32, Epoch 1: G loss: 0.33443705780165534 vs. C loss: 0.1841575511627727\n",
      "Group 32, Epoch 2: G loss: 0.3542043941361564 vs. C loss: 0.19404102644572654\n",
      "Group 32, Epoch 3: G loss: 0.058616401574441354 vs. C loss: 0.20151013135910034\n",
      "Group 32, Epoch 4: G loss: 0.03630670830607414 vs. C loss: 0.1972938295867708\n",
      "Group 32, Epoch 5: G loss: 0.07626827231475286 vs. C loss: 0.16646264990170798\n",
      "Group 32, Epoch 6: G loss: 0.3172733315399715 vs. C loss: 0.15958461413780847\n",
      "Group 33, Epoch 1: G loss: 0.35212272575923376 vs. C loss: 0.20808549722035727\n",
      "Group 33, Epoch 2: G loss: 0.2523872234991619 vs. C loss: 0.12460783123970032\n",
      "Group 33, Epoch 3: G loss: 0.42888436487742826 vs. C loss: 0.19693486723634934\n",
      "Group 33, Epoch 4: G loss: 0.2413314517055239 vs. C loss: 0.13265794929530886\n",
      "Group 33, Epoch 5: G loss: 0.6377437055110932 vs. C loss: 0.15245895584424338\n",
      "Group 33, Epoch 6: G loss: 0.6739130292619978 vs. C loss: 0.13585494375891158\n",
      "Group 34, Epoch 1: G loss: 0.4433756564344678 vs. C loss: 0.17328390230735144\n",
      "Group 34, Epoch 2: G loss: 0.32514840619904656 vs. C loss: 0.16406949857870737\n",
      "Group 34, Epoch 3: G loss: 0.25604964537279945 vs. C loss: 0.13505025539133284\n",
      "Group 34, Epoch 4: G loss: 0.5302032334463936 vs. C loss: 0.21867721610599092\n",
      "Group 34, Epoch 5: G loss: 0.09518608248659542 vs. C loss: 0.2074184301826689\n",
      "Group 34, Epoch 6: G loss: 0.045656757163149965 vs. C loss: 0.19855207204818726\n",
      "Group 35, Epoch 1: G loss: 0.4045737998826163 vs. C loss: 0.23745614952511254\n",
      "Group 35, Epoch 2: G loss: 0.3196551646505083 vs. C loss: 0.1703021733297242\n",
      "Group 35, Epoch 3: G loss: 0.1835472828575543 vs. C loss: 0.19489414079321754\n",
      "Group 35, Epoch 4: G loss: 0.0929421770785536 vs. C loss: 0.20378461480140686\n",
      "Group 35, Epoch 5: G loss: 0.04647798538208008 vs. C loss: 0.18543172379334769\n",
      "Group 35, Epoch 6: G loss: 0.16436608050550733 vs. C loss: 0.14510312345292833\n",
      "Group 36, Epoch 1: G loss: 0.2547034868172237 vs. C loss: 0.2053653084569507\n",
      "Group 36, Epoch 2: G loss: 0.2806651881762913 vs. C loss: 0.16925331784619227\n",
      "Group 36, Epoch 3: G loss: 0.37627400585583276 vs. C loss: 0.11881348076793881\n",
      "Group 36, Epoch 4: G loss: 0.44052232333592006 vs. C loss: 0.21660037090380987\n",
      "Group 36, Epoch 5: G loss: 0.05691561124154499 vs. C loss: 0.20739129847950408\n",
      "Group 36, Epoch 6: G loss: 0.03835737151759011 vs. C loss: 0.20486957414282692\n",
      "Group 36, Epoch 7: G loss: 0.02744824777224234 vs. C loss: 0.20341616455051637\n",
      "Group 37, Epoch 1: G loss: 0.34303365434919086 vs. C loss: 0.20897188782691956\n",
      "Group 37, Epoch 2: G loss: 0.26191846174853184 vs. C loss: 0.15309392991993162\n",
      "Group 37, Epoch 3: G loss: 0.4913947318281446 vs. C loss: 0.07786100109418234\n",
      "Group 37, Epoch 4: G loss: 0.3671318175537245 vs. C loss: 0.195509499973721\n",
      "Group 37, Epoch 5: G loss: 0.047427498708878246 vs. C loss: 0.20592941674921247\n",
      "Group 37, Epoch 6: G loss: 0.016695793957582545 vs. C loss: 0.20742541882726884\n",
      "Group 38, Epoch 1: G loss: 0.23731291038649419 vs. C loss: 0.22248022589418623\n",
      "Group 38, Epoch 2: G loss: 0.150574249454907 vs. C loss: 0.1340970711575614\n",
      "Group 38, Epoch 3: G loss: 0.5599083491734096 vs. C loss: 0.08518132484621471\n",
      "Group 38, Epoch 4: G loss: 0.3643926882318088 vs. C loss: 0.17463730317023066\n",
      "Group 38, Epoch 5: G loss: 0.15931930009807857 vs. C loss: 0.1981839487950007\n",
      "Group 38, Epoch 6: G loss: 0.13126164142574584 vs. C loss: 0.16276923649840883\n",
      "Group 39, Epoch 1: G loss: 0.47767320871353147 vs. C loss: 0.25282095041539937\n",
      "Group 39, Epoch 2: G loss: 0.39793025851249697 vs. C loss: 0.13512873318460253\n",
      "Group 39, Epoch 3: G loss: 0.3919199960572379 vs. C loss: 0.10321817961004044\n",
      "Group 39, Epoch 4: G loss: 0.5106477226529803 vs. C loss: 0.09288468791378868\n",
      "Group 39, Epoch 5: G loss: 0.2476614174033914 vs. C loss: 0.21224778228335908\n",
      "Group 39, Epoch 6: G loss: 0.007283893280795642 vs. C loss: 0.20698909958203635\n",
      "Group 40, Epoch 1: G loss: 0.3369105551924024 vs. C loss: 0.17710273878441918\n",
      "Group 40, Epoch 2: G loss: 0.3896840040172849 vs. C loss: 0.11444643926289348\n",
      "Group 40, Epoch 3: G loss: 0.6787242327417646 vs. C loss: 0.15455285708109537\n",
      "Group 40, Epoch 4: G loss: 0.33884643678154264 vs. C loss: 0.20560298942857322\n",
      "Group 40, Epoch 5: G loss: 0.042827078753284044 vs. C loss: 0.21386555168363783\n",
      "Group 40, Epoch 6: G loss: 0.029488608400736534 vs. C loss: 0.20446707473860848\n",
      "Group 41, Epoch 1: G loss: 0.4476718613079616 vs. C loss: 0.2448600067032708\n",
      "Group 41, Epoch 2: G loss: 0.3738887097154345 vs. C loss: 0.12194416506422891\n",
      "Group 41, Epoch 3: G loss: 0.31682213055236 vs. C loss: 0.17590829067760042\n",
      "Group 41, Epoch 4: G loss: 0.333591799225126 vs. C loss: 0.1194866510728995\n",
      "Group 41, Epoch 5: G loss: 0.41745641486985346 vs. C loss: 0.10833621604575051\n",
      "Group 41, Epoch 6: G loss: 0.45825196163994925 vs. C loss: 0.2171609252691269\n",
      "Group 42, Epoch 1: G loss: 0.45451767785208563 vs. C loss: 0.25638922552267707\n",
      "Group 42, Epoch 2: G loss: 0.4457684057099479 vs. C loss: 0.11735774079958598\n",
      "Group 42, Epoch 3: G loss: 0.6698284557887486 vs. C loss: 0.058343885259495847\n",
      "Group 42, Epoch 4: G loss: 0.7402894599097116 vs. C loss: 0.02100714813503954\n",
      "Group 42, Epoch 5: G loss: 0.7485560025487628 vs. C loss: 0.04720970346695847\n",
      "Group 42, Epoch 6: G loss: 0.745033699274063 vs. C loss: 0.11333522448937096\n",
      "Group 42, Epoch 7: G loss: 0.7318489040647235 vs. C loss: 0.036780320314897426\n",
      "Group 42, Epoch 8: G loss: 0.7813428436006818 vs. C loss: 0.12744338562091193\n",
      "Group 42, Epoch 9: G loss: 0.6381186720516 vs. C loss: 0.2085492130782869\n",
      "Group 43, Epoch 1: G loss: 0.3415396562644414 vs. C loss: 0.19381939536995355\n",
      "Group 43, Epoch 2: G loss: 0.3642892258507864 vs. C loss: 0.09886896734436353\n",
      "Group 43, Epoch 3: G loss: 0.4971426576375962 vs. C loss: 0.165716296268834\n",
      "Group 43, Epoch 4: G loss: 0.39066492531980784 vs. C loss: 0.2317479600509008\n",
      "Group 43, Epoch 5: G loss: 0.05427413870181356 vs. C loss: 0.21739831235673693\n",
      "Group 43, Epoch 6: G loss: 0.06437688448599407 vs. C loss: 0.16461679836114249\n",
      "Group 44, Epoch 1: G loss: 0.3838727525302342 vs. C loss: 0.20200356013245055\n",
      "Group 44, Epoch 2: G loss: 0.45951489721025746 vs. C loss: 0.10272343042824005\n",
      "Group 44, Epoch 3: G loss: 0.5505709290504456 vs. C loss: 0.049910953682329916\n",
      "Group 44, Epoch 4: G loss: 0.8686523045812334 vs. C loss: 0.04761953237983915\n",
      "Group 44, Epoch 5: G loss: 0.23852002833570754 vs. C loss: 0.20997301075193617\n",
      "Group 44, Epoch 6: G loss: 0.027790311617510662 vs. C loss: 0.20828405684894982\n",
      "Group 45, Epoch 1: G loss: 0.4358600854873658 vs. C loss: 0.19833540750874412\n",
      "Group 45, Epoch 2: G loss: 0.2787876767771585 vs. C loss: 0.1660348011387719\n",
      "Group 45, Epoch 3: G loss: 0.20371010942118511 vs. C loss: 0.1440515418847402\n",
      "Group 45, Epoch 4: G loss: 0.3962458099637713 vs. C loss: 0.1334608710474438\n",
      "Group 45, Epoch 5: G loss: 0.24484028135027208 vs. C loss: 0.1802706370751063\n",
      "Group 45, Epoch 6: G loss: 0.1440095299056598 vs. C loss: 0.17658832317425144\n",
      "Group 46, Epoch 1: G loss: 0.4328327459948404 vs. C loss: 0.20783887637986076\n",
      "Group 46, Epoch 2: G loss: 0.26406281122139524 vs. C loss: 0.1398312151432037\n",
      "Group 46, Epoch 3: G loss: 0.5725963422230311 vs. C loss: 0.08464193095763524\n",
      "Group 46, Epoch 4: G loss: 0.4721349673611777 vs. C loss: 0.1862796495358149\n",
      "Group 46, Epoch 5: G loss: 0.22506823561021258 vs. C loss: 0.22661974694993758\n",
      "Group 46, Epoch 6: G loss: 0.05971708478672164 vs. C loss: 0.1866320114996698\n",
      "Group 47, Epoch 1: G loss: 0.2160776206425258 vs. C loss: 0.18138537142011854\n",
      "Group 47, Epoch 2: G loss: 0.21794724954026082 vs. C loss: 0.16900323331356049\n",
      "Group 47, Epoch 3: G loss: 0.29441590777465276 vs. C loss: 0.13794889797767004\n",
      "Group 47, Epoch 4: G loss: 0.28608435073069166 vs. C loss: 0.22212272716893092\n",
      "Group 47, Epoch 5: G loss: 0.05179313171122756 vs. C loss: 0.2074904532896148\n",
      "Group 47, Epoch 6: G loss: 0.032862070522138055 vs. C loss: 0.20468052559428743\n",
      "Group 48, Epoch 1: G loss: 0.4406917640141078 vs. C loss: 0.25406968924734324\n",
      "Group 48, Epoch 2: G loss: 0.37418924272060394 vs. C loss: 0.14480717480182648\n",
      "Group 48, Epoch 3: G loss: 0.2217414138572557 vs. C loss: 0.16305737114614913\n",
      "Group 48, Epoch 4: G loss: 0.2945540700639997 vs. C loss: 0.250225649939643\n",
      "Group 48, Epoch 5: G loss: 0.043808850007397794 vs. C loss: 0.20651495622264013\n",
      "Group 48, Epoch 6: G loss: 0.0319678608328104 vs. C loss: 0.20671693235635757\n",
      "Group 49, Epoch 1: G loss: 0.22120569476059507 vs. C loss: 0.16301623317930433\n",
      "Group 49, Epoch 2: G loss: 0.3762797594070434 vs. C loss: 0.10743504100375706\n",
      "Group 49, Epoch 3: G loss: 0.5544573187828064 vs. C loss: 0.13999957177374098\n",
      "Group 49, Epoch 4: G loss: 0.30603144743612837 vs. C loss: 0.2156310710642073\n",
      "Group 49, Epoch 5: G loss: 0.06104501006858689 vs. C loss: 0.20529428455564713\n",
      "Group 49, Epoch 6: G loss: 0.042530417495540214 vs. C loss: 0.20355861882368723\n",
      "Group 50, Epoch 1: G loss: 0.21799445535455433 vs. C loss: 0.18699375540018082\n",
      "Group 50, Epoch 2: G loss: 0.35014946588448115 vs. C loss: 0.13785120513704088\n",
      "Group 50, Epoch 3: G loss: 0.3761965811252595 vs. C loss: 0.17868234713872275\n",
      "Group 50, Epoch 4: G loss: 0.36745652939592083 vs. C loss: 0.12641539838578966\n",
      "Group 50, Epoch 5: G loss: 0.4246432962162153 vs. C loss: 0.15162931299871868\n",
      "Group 50, Epoch 6: G loss: 0.3083815001483474 vs. C loss: 0.2173773811923133\n",
      "Group 50, Epoch 7: G loss: 0.027331176400184627 vs. C loss: 0.20948626107484516\n",
      "Group 50, Epoch 8: G loss: 0.02140529256846224 vs. C loss: 0.20936431321832868\n",
      "Group 50, Epoch 9: G loss: 0.0189191745860236 vs. C loss: 0.20792985003855494\n",
      "Group 50, Epoch 10: G loss: 0.01853356563619205 vs. C loss: 0.2080501326256328\n",
      "Group 50, Epoch 11: G loss: 0.018445783055254395 vs. C loss: 0.20798719922701517\n",
      "Group 50, Epoch 12: G loss: 0.01789014248975686 vs. C loss: 0.2078867413931423\n",
      "Group 50, Epoch 13: G loss: 0.017459112618650706 vs. C loss: 0.2077511627640989\n",
      "Group 50, Epoch 14: G loss: 0.01798095905355045 vs. C loss: 0.20757700171735552\n",
      "Group 50, Epoch 15: G loss: 0.019135593676141326 vs. C loss: 0.20705105364322662\n",
      "Group 50, Epoch 16: G loss: 0.022814936456935747 vs. C loss: 0.20448290970590377\n",
      "Group 50, Epoch 17: G loss: 0.037847141389335905 vs. C loss: 0.1898260629839367\n",
      "Group 50, Epoch 18: G loss: 0.3039004879338401 vs. C loss: 0.12679736895693672\n",
      "Group 50, Epoch 19: G loss: 0.5888941185814993 vs. C loss: 0.05993882939219475\n",
      "Group 50, Epoch 20: G loss: 0.9202821799686978 vs. C loss: 0.025962340128090646\n",
      "Group 50, Epoch 21: G loss: 0.7904037935393198 vs. C loss: 0.008423989173024893\n",
      "Group 50, Epoch 22: G loss: 0.7993622388158526 vs. C loss: 0.028377226864298184\n",
      "Group 50, Epoch 23: G loss: 1.0157077959605625 vs. C loss: 0.009369083240421282\n",
      "Group 50, Epoch 24: G loss: 0.93224230493818 vs. C loss: 0.038141612660385564\n",
      "Group 50, Epoch 25: G loss: 1.0342705079487393 vs. C loss: 0.014663347780394057\n",
      "Group 51, Epoch 1: G loss: 0.4613740495273045 vs. C loss: 0.23176426854398513\n",
      "Group 51, Epoch 2: G loss: 0.3496589660644531 vs. C loss: 0.18770549860265517\n",
      "Group 51, Epoch 3: G loss: 0.1513497388788632 vs. C loss: 0.1777438728345765\n",
      "Group 51, Epoch 4: G loss: 0.19008485163961136 vs. C loss: 0.1462937162982093\n",
      "Group 51, Epoch 5: G loss: 0.4078396490642002 vs. C loss: 0.14824055963092378\n",
      "Group 51, Epoch 6: G loss: 0.3630312404462269 vs. C loss: 0.09936685032314724\n",
      "Group 52, Epoch 1: G loss: 0.441276160308293 vs. C loss: 0.235140860080719\n",
      "Group 52, Epoch 2: G loss: 0.4563217120511191 vs. C loss: 0.10328631434175704\n",
      "Group 52, Epoch 3: G loss: 0.603972202539444 vs. C loss: 0.06987279487980737\n",
      "Group 52, Epoch 4: G loss: 0.6279512916292463 vs. C loss: 0.19964676847060522\n",
      "Group 52, Epoch 5: G loss: 0.09668356914605412 vs. C loss: 0.2057377166218228\n",
      "Group 52, Epoch 6: G loss: 0.13995647387845173 vs. C loss: 0.15406019157833523\n",
      "Group 53, Epoch 1: G loss: 0.19457154486860545 vs. C loss: 0.17931900835699507\n",
      "Group 53, Epoch 2: G loss: 0.38136258551052643 vs. C loss: 0.1546574036280314\n",
      "Group 53, Epoch 3: G loss: 0.536055144241878 vs. C loss: 0.11301984472407235\n",
      "Group 53, Epoch 4: G loss: 0.47548549388136185 vs. C loss: 0.1562288204828898\n",
      "Group 53, Epoch 5: G loss: 0.16768432291490692 vs. C loss: 0.21397039708164003\n",
      "Group 53, Epoch 6: G loss: 0.015650670762572972 vs. C loss: 0.21099896480639777\n",
      "Group 54, Epoch 1: G loss: 0.38631335667201455 vs. C loss: 0.21369575957457224\n",
      "Group 54, Epoch 2: G loss: 0.28255711751324786 vs. C loss: 0.13921453555425006\n",
      "Group 54, Epoch 3: G loss: 0.3444691828319004 vs. C loss: 0.14765901615222296\n",
      "Group 54, Epoch 4: G loss: 0.3391199541943414 vs. C loss: 0.1568697897924317\n",
      "Group 54, Epoch 5: G loss: 0.309261201109205 vs. C loss: 0.17929391728507146\n",
      "Group 54, Epoch 6: G loss: 0.1837886072695256 vs. C loss: 0.19690527104669145\n",
      "Group 54, Epoch 7: G loss: 0.21262567554201398 vs. C loss: 0.17325258586141798\n",
      "Group 54, Epoch 8: G loss: 0.327963450551033 vs. C loss: 0.11369239538908005\n",
      "Group 54, Epoch 9: G loss: 0.4812001134668078 vs. C loss: 0.1489719140032927\n",
      "Group 54, Epoch 10: G loss: 0.4151740725551333 vs. C loss: 0.10271129715773795\n",
      "Group 54, Epoch 11: G loss: 0.7446921740259443 vs. C loss: 0.058731796426905535\n",
      "Group 54, Epoch 12: G loss: 0.5417703151702881 vs. C loss: 0.16835303770171273\n",
      "Group 55, Epoch 1: G loss: 0.15705207203115737 vs. C loss: 0.1774104005760617\n",
      "Group 55, Epoch 2: G loss: 0.37332206709044324 vs. C loss: 0.09384370926353665\n",
      "Group 55, Epoch 3: G loss: 0.6299481238637653 vs. C loss: 0.04642354365852144\n",
      "Group 55, Epoch 4: G loss: 0.6029236631734031 vs. C loss: 0.054583868218792804\n",
      "Group 55, Epoch 5: G loss: 0.6253504712666784 vs. C loss: 0.21899696191151938\n",
      "Group 55, Epoch 6: G loss: 0.02444465160369873 vs. C loss: 0.20814904322226843\n",
      "Group 56, Epoch 1: G loss: 0.19797233726297106 vs. C loss: 0.2447752041949166\n",
      "Group 56, Epoch 2: G loss: 0.17745582376207625 vs. C loss: 0.1741992798116472\n",
      "Group 56, Epoch 3: G loss: 0.18252240398100444 vs. C loss: 0.1656661348210441\n",
      "Group 56, Epoch 4: G loss: 0.21378210144383566 vs. C loss: 0.20318769911924997\n",
      "Group 56, Epoch 5: G loss: 0.07733454938445772 vs. C loss: 0.19909609854221344\n",
      "Group 56, Epoch 6: G loss: 0.08508343281490462 vs. C loss: 0.17979861464765337\n",
      "Group 57, Epoch 1: G loss: 0.21074856221675872 vs. C loss: 0.1888130745953984\n",
      "Group 57, Epoch 2: G loss: 0.2503057224409921 vs. C loss: 0.1335957580142551\n",
      "Group 57, Epoch 3: G loss: 0.3600670256784984 vs. C loss: 0.1781007564730114\n",
      "Group 57, Epoch 4: G loss: 0.13650974482297898 vs. C loss: 0.20618845522403717\n",
      "Group 57, Epoch 5: G loss: 0.07522432931831904 vs. C loss: 0.1667285379436281\n",
      "Group 57, Epoch 6: G loss: 0.3946261491094316 vs. C loss: 0.08588035735819073\n",
      "Group 58, Epoch 1: G loss: 0.24184499297823225 vs. C loss: 0.22226017713546753\n",
      "Group 58, Epoch 2: G loss: 0.26573738540921893 vs. C loss: 0.1488320661915673\n",
      "Group 58, Epoch 3: G loss: 0.5583660423755646 vs. C loss: 0.08049058707223998\n",
      "Group 58, Epoch 4: G loss: 0.47313519162791107 vs. C loss: 0.23312107721964517\n",
      "Group 58, Epoch 5: G loss: 0.0590318849576371 vs. C loss: 0.2026779842045572\n",
      "Group 58, Epoch 6: G loss: 0.026320125375475203 vs. C loss: 0.2030086583561367\n",
      "Group 59, Epoch 1: G loss: 0.3214095677648272 vs. C loss: 0.18241535458299848\n",
      "Group 59, Epoch 2: G loss: 0.26880676788943153 vs. C loss: 0.19790792299641505\n",
      "Group 59, Epoch 3: G loss: 0.06713873531137193 vs. C loss: 0.20276109418935248\n",
      "Group 59, Epoch 4: G loss: 0.039180709634508405 vs. C loss: 0.20018924110465577\n",
      "Group 59, Epoch 5: G loss: 0.043357413102473534 vs. C loss: 0.19593514170911577\n",
      "Group 59, Epoch 6: G loss: 0.11008796883480888 vs. C loss: 0.17133403238322997\n",
      "Group 60, Epoch 1: G loss: 0.2927017888852528 vs. C loss: 0.17246281438403663\n",
      "Group 60, Epoch 2: G loss: 0.4659725563866752 vs. C loss: 0.12557749905520016\n",
      "Group 60, Epoch 3: G loss: 0.4645942270755768 vs. C loss: 0.20308968383404946\n",
      "Group 60, Epoch 4: G loss: 0.09898991350616727 vs. C loss: 0.20319048232502404\n",
      "Group 60, Epoch 5: G loss: 0.1098107838204929 vs. C loss: 0.14782950199312633\n",
      "Group 60, Epoch 6: G loss: 0.26075026839971543 vs. C loss: 0.22232208318180505\n",
      "Group 61, Epoch 1: G loss: 0.507058243240629 vs. C loss: 0.20272174808714125\n",
      "Group 61, Epoch 2: G loss: 0.3443112075328827 vs. C loss: 0.11908670100900863\n",
      "Group 61, Epoch 3: G loss: 0.5980779153960093 vs. C loss: 0.09055208745929928\n",
      "Group 61, Epoch 4: G loss: 0.5800955844776972 vs. C loss: 0.1197364769048161\n",
      "Group 61, Epoch 5: G loss: 0.3174364403954574 vs. C loss: 0.21988848514027062\n",
      "Group 61, Epoch 6: G loss: 0.025732547736593652 vs. C loss: 0.2078700711329778\n",
      "Group 62, Epoch 1: G loss: 0.4613648772239685 vs. C loss: 0.23519267224603232\n",
      "Group 62, Epoch 2: G loss: 0.35695166971002307 vs. C loss: 0.1494729013906585\n",
      "Group 62, Epoch 3: G loss: 0.38049995303153994 vs. C loss: 0.17660810136132774\n",
      "Group 62, Epoch 4: G loss: 0.2881790689059666 vs. C loss: 0.22063759797149238\n",
      "Group 62, Epoch 5: G loss: 0.15795353289161412 vs. C loss: 0.17494831317000917\n",
      "Group 62, Epoch 6: G loss: 0.3829454098428999 vs. C loss: 0.11158074852493073\n",
      "Group 62, Epoch 7: G loss: 0.5450009073529924 vs. C loss: 0.09442285034391616\n",
      "Group 63, Epoch 1: G loss: 0.6806154319218226 vs. C loss: 0.33566340141826206\n",
      "Group 63, Epoch 2: G loss: 0.4355569916112082 vs. C loss: 0.169080278939671\n",
      "Group 63, Epoch 3: G loss: 0.27480171748570037 vs. C loss: 0.14879901541603938\n",
      "Group 63, Epoch 4: G loss: 0.3052111097744533 vs. C loss: 0.12991276797321108\n",
      "Group 63, Epoch 5: G loss: 0.4412309212344034 vs. C loss: 0.19799273709456125\n",
      "Group 63, Epoch 6: G loss: 0.1529859770621572 vs. C loss: 0.17787006083461973\n",
      "Group 63, Epoch 7: G loss: 0.2242501422762871 vs. C loss: 0.20525869064860872\n",
      "Group 63, Epoch 8: G loss: 0.14565235333783288 vs. C loss: 0.15059764186541238\n",
      "Group 63, Epoch 9: G loss: 0.5429387194769723 vs. C loss: 0.06715426945851909\n",
      "Group 63, Epoch 10: G loss: 0.44341113056455345 vs. C loss: 0.1704394519329071\n",
      "Group 63, Epoch 11: G loss: 0.2120300565447126 vs. C loss: 0.18543983002503714\n",
      "Group 64, Epoch 1: G loss: 0.2521618374756404 vs. C loss: 0.17826063889596197\n",
      "Group 64, Epoch 2: G loss: 0.2620060980319977 vs. C loss: 0.15066822369893393\n",
      "Group 64, Epoch 3: G loss: 0.5890810404505048 vs. C loss: 0.07520755856401391\n",
      "Group 64, Epoch 4: G loss: 0.7747145857129779 vs. C loss: 0.021043241437938478\n",
      "Group 64, Epoch 5: G loss: 0.4932863550526756 vs. C loss: 0.08407091163098812\n",
      "Group 64, Epoch 6: G loss: 0.43757645157831054 vs. C loss: 0.2173494514491823\n",
      "Group 65, Epoch 1: G loss: 0.2288467185837882 vs. C loss: 0.19490663541687858\n",
      "Group 65, Epoch 2: G loss: 0.19664850916181292 vs. C loss: 0.16800849967532686\n",
      "Group 65, Epoch 3: G loss: 0.27881376445293427 vs. C loss: 0.15065935171312755\n",
      "Group 65, Epoch 4: G loss: 0.33380883676665174 vs. C loss: 0.1721338116460376\n",
      "Group 65, Epoch 5: G loss: 0.20814001219613212 vs. C loss: 0.19782177110513052\n",
      "Group 65, Epoch 6: G loss: 0.08342369017856462 vs. C loss: 0.19399586816628775\n",
      "Group 66, Epoch 1: G loss: 0.3171706225190844 vs. C loss: 0.14796975751717886\n",
      "Group 66, Epoch 2: G loss: 0.2789627228464399 vs. C loss: 0.1518648349576526\n",
      "Group 66, Epoch 3: G loss: 0.42655532700674875 vs. C loss: 0.046947139625748\n",
      "Group 66, Epoch 4: G loss: 0.7528878739901951 vs. C loss: 0.022519452083441943\n",
      "Group 66, Epoch 5: G loss: 0.926307533468519 vs. C loss: 0.004517439442376296\n",
      "Group 66, Epoch 6: G loss: 0.7588439532688687 vs. C loss: 0.04034970546813889\n",
      "Group 67, Epoch 1: G loss: 0.27553618380001615 vs. C loss: 0.17732302016682097\n",
      "Group 67, Epoch 2: G loss: 0.3344218007155827 vs. C loss: 0.15018145905600655\n",
      "Group 67, Epoch 3: G loss: 0.3701726189681462 vs. C loss: 0.16672023468547395\n",
      "Group 67, Epoch 4: G loss: 0.33731934045042317 vs. C loss: 0.13432666576570937\n",
      "Group 67, Epoch 5: G loss: 0.30671950834138056 vs. C loss: 0.21865758299827576\n",
      "Group 67, Epoch 6: G loss: 0.0358517306723765 vs. C loss: 0.20777914590305754\n",
      "Group 68, Epoch 1: G loss: 0.13772786685398647 vs. C loss: 0.16098635726504856\n",
      "Group 68, Epoch 2: G loss: 0.2958881791148867 vs. C loss: 0.17640814599063662\n",
      "Group 68, Epoch 3: G loss: 0.3587739484650748 vs. C loss: 0.17568285349342558\n",
      "Group 68, Epoch 4: G loss: 0.256630962235587 vs. C loss: 0.18854903429746628\n",
      "Group 68, Epoch 5: G loss: 0.11978607614125523 vs. C loss: 0.19834781355328027\n",
      "Group 68, Epoch 6: G loss: 0.15970165176051004 vs. C loss: 0.13365324131316608\n",
      "Group 69, Epoch 1: G loss: 0.42080553770065304 vs. C loss: 0.27233390013376874\n",
      "Group 69, Epoch 2: G loss: 0.40449275374412536 vs. C loss: 0.13091553582085505\n",
      "Group 69, Epoch 3: G loss: 0.44874563472611567 vs. C loss: 0.15274472700224984\n",
      "Group 69, Epoch 4: G loss: 0.29052244978291647 vs. C loss: 0.2279923740360472\n",
      "Group 69, Epoch 5: G loss: 0.05371266049998147 vs. C loss: 0.2026263102889061\n",
      "Group 69, Epoch 6: G loss: 0.03399142404752119 vs. C loss: 0.19976074496905008\n",
      "Group 70, Epoch 1: G loss: 0.3552115568092891 vs. C loss: 0.20096523894204035\n",
      "Group 70, Epoch 2: G loss: 0.3175807718719755 vs. C loss: 0.12967955238289305\n",
      "Group 70, Epoch 3: G loss: 0.4078996215547834 vs. C loss: 0.09909679906235801\n",
      "Group 70, Epoch 4: G loss: 0.4300012682165417 vs. C loss: 0.20392986469798616\n",
      "Group 70, Epoch 5: G loss: 0.06184484836246285 vs. C loss: 0.19184120910035238\n",
      "Group 70, Epoch 6: G loss: 0.10382324872272354 vs. C loss: 0.167662070857154\n",
      "Group 71, Epoch 1: G loss: 0.3444185580526079 vs. C loss: 0.21746670537524757\n",
      "Group 71, Epoch 2: G loss: 0.3873337213482175 vs. C loss: 0.18250380290879145\n",
      "Group 71, Epoch 3: G loss: 0.15022647806576317 vs. C loss: 0.15742365519205728\n",
      "Group 71, Epoch 4: G loss: 0.2967440781848771 vs. C loss: 0.20759044422043693\n",
      "Group 71, Epoch 5: G loss: 0.10932572462729047 vs. C loss: 0.1992381794585122\n",
      "Group 71, Epoch 6: G loss: 0.09363228870289667 vs. C loss: 0.19944296528895697\n",
      "Group 71, Epoch 7: G loss: 0.08409137800335885 vs. C loss: 0.1856306261486477\n",
      "Group 72, Epoch 1: G loss: 0.12461381703615189 vs. C loss: 0.1413025702867243\n",
      "Group 72, Epoch 2: G loss: 0.35494396814278195 vs. C loss: 0.14637533740864858\n",
      "Group 72, Epoch 3: G loss: 0.39991541377135686 vs. C loss: 0.1502622597747379\n",
      "Group 72, Epoch 4: G loss: 0.43700544748987463 vs. C loss: 0.0930674175421397\n",
      "Group 72, Epoch 5: G loss: 0.6718347464288985 vs. C loss: 0.06345887192421489\n",
      "Group 72, Epoch 6: G loss: 0.6031434919152939 vs. C loss: 0.1916716992855072\n",
      "Group 73, Epoch 1: G loss: 0.3021283899034773 vs. C loss: 0.14129621618323854\n",
      "Group 73, Epoch 2: G loss: 0.4408157331602914 vs. C loss: 0.1298133937848939\n",
      "Group 73, Epoch 3: G loss: 0.575504366840635 vs. C loss: 0.1309777746597926\n",
      "Group 73, Epoch 4: G loss: 0.23593060331685206 vs. C loss: 0.22409031954076553\n",
      "Group 73, Epoch 5: G loss: 0.039402826449700765 vs. C loss: 0.20497217774391174\n",
      "Group 73, Epoch 6: G loss: 0.044167222295488626 vs. C loss: 0.20672158234649232\n",
      "Group 74, Epoch 1: G loss: 0.22427409504141127 vs. C loss: 0.18040322098467085\n",
      "Group 74, Epoch 2: G loss: 0.24639545721667153 vs. C loss: 0.1519839498731825\n",
      "Group 74, Epoch 3: G loss: 0.4466783540589469 vs. C loss: 0.13281275580326715\n",
      "Group 74, Epoch 4: G loss: 0.4054888048342296 vs. C loss: 0.19165695375866362\n",
      "Group 74, Epoch 5: G loss: 0.1365840100816318 vs. C loss: 0.13793352825774088\n",
      "Group 74, Epoch 6: G loss: 0.7031547120639257 vs. C loss: 0.10322160232398243\n",
      "Group 74, Epoch 7: G loss: 0.42781282365322115 vs. C loss: 0.2048232290479872\n",
      "Group 74, Epoch 8: G loss: 0.03335571124085358 vs. C loss: 0.20701622300677827\n"
     ]
    }
   ],
   "source": [
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer, add the start pos\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.002)\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "class PrintCross(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('x', end='')\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, GAN_PARAMS[\"g_input_size\"]));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    classifier_model = build_classifier_model()\n",
    "    extvar[\"classifier_model\"] = classifier_model;\n",
    "    \n",
    "    def loss_function_for_generative_model(y_true, y_pred):\n",
    "        return construct_map_and_calc_loss(y_pred, extvar);\n",
    "    \n",
    "#     classifier_true_set_group = special_train_data[np.random.randint(0, special_train_data.shape[0], (500,))];\n",
    "    \n",
    "#     classifier_model.summary()\n",
    "#     gmodel.summary()\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    max_epoch = GAN_PARAMS[\"max_epoch\"];\n",
    "    good_epoch = GAN_PARAMS[\"good_epoch\"] - 1;\n",
    "    g_multiplier = GAN_PARAMS[\"g_epochs\"];\n",
    "    c_multiplier = GAN_PARAMS[\"c_epochs\"];\n",
    "    g_batch = GAN_PARAMS[\"g_batch\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    c_true_batch = GAN_PARAMS[\"c_true_batch\"];\n",
    "    c_false_batch = GAN_PARAMS[\"c_false_batch\"];\n",
    "    \n",
    "    gmodel = generative_model(g_input_size, note_group_size * 4, loss_function_for_generative_model);\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        gnoise = np.random.random((g_batch, g_input_size));\n",
    "        glabel = np.ones((g_batch,))\n",
    "\n",
    "        history = gmodel.fit(gnoise, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintDot()\n",
    "        \n",
    "        predicted_maps_data = gmodel.predict(np.random.random((c_false_batch, g_input_size)));\n",
    "        new_false_maps = construct_map_with_sliders(tf.convert_to_tensor(predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "        new_false_labels = np.zeros(c_false_batch);\n",
    "        \n",
    "#         not_predicted_maps_data = np.random.random((10, 40));\n",
    "    #     new_false_maps2 = construct_map_with_sliders(tf.convert_to_tensor(not_predicted_maps_data, dtype=\"float32\"), extvar=extvar).numpy();\n",
    "    #     new_false_labels2 = np.zeros(10);\n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (c_true_batch,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "    #     actual_train_data = special_train_data[st:se];\n",
    "    #     actual_train_labels = special_train_labels[st:se];\n",
    "\n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[]) #PrintCross()\n",
    "\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        res = gmodel.predict(plot_noise);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        if i >= good_epoch:\n",
    "            current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # it's the same anyways\n",
    "            res = gmodel.predict(np.random.random((1, g_input_size)));\n",
    "#         print(construct_map_with_sliders(tf.convert_to_tensor(res), extvar=extvar).numpy().squeeze());\n",
    "            plot_current_map(tf.convert_to_tensor(res));\n",
    "\n",
    "    onoise = np.random.random((1, g_input_size));\n",
    "    \n",
    "    return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    print(\"# of groups: {}\".format(timestamps.shape[0] // note_group_size));\n",
    "    for i in range(timestamps.shape[0] // note_group_size):\n",
    "        z = generate_set(begin = i * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# yes! dist_multiplier in #6 is used here!!\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    generate_set(begin = 3 * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success! finished on: 2018-09-09 13:36:43.659064\n"
     ]
    }
   ],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@2018/8/19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
